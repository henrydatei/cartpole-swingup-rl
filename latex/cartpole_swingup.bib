
@misc{opencv_team_opencv-python_2023,
	title = {opencv-python: {Wrapper} package for {OpenCV} python bindings.},
	copyright = {Apache Software License},
	shorttitle = {opencv-python},
	url = {https://github.com/opencv/opencv-python},
	urldate = {2024-04-11},
	author = {OpenCV Team},
	month = dec,
	year = {2023},
	keywords = {Scientific/Engineering, Software Development, Scientific/Engineering - Image Recognition},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\ETF42V9M\\opencv-python.html:text/html},
}

@misc{huaman_opencv_nodate,
	title = {{OpenCV}: {Image} {Moments}},
	url = {https://docs.opencv.org/4.x/d0/d49/tutorial_moments.html},
	urldate = {2024-04-11},
	author = {Huamán, Ana},
	file = {OpenCV\: Image Moments:C\:\\Users\\Guest_\\Zotero\\storage\\QSFDTJEF\\tutorial_moments.html:text/html},
}

@misc{newton_shape_2023,
	title = {Shape {Based} {Object} {Tracking} with {OpenCV} on {Raspberry} {Pi}},
	url = {https://how2electronics.com/shape-based-object-tracking-with-opencv-on-raspberry-pi/},
	abstract = {Overview: This project is about Shape Based Object Detection \& Tracking with OpenCV on Raspberry Pi 4 Computer. In the world of computer vision,},
	language = {en-US},
	urldate = {2024-04-11},
	journal = {How To Electronics},
	author = {Newton, Alex},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\AR4MKA6E\\shape-based-object-tracking-with-opencv-on-raspberry-pi.html:text/html},
}

@misc{noauthor_hsl_2024,
	title = {{HSL} and {HSV}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=HSL_and_HSV&oldid=1218017971},
	abstract = {HSL and HSV are the two most common cylindrical-coordinate representations of points in an RGB color model.  The two representations rearrange the geometry of RGB in an attempt to be more intuitive and perceptually relevant than the cartesian (cube) representation.  Developed in the 1970s for computer graphics applications, HSL and HSV are used today in color pickers, in image editing software, and less commonly in image analysis and computer vision.
HSL stands for hue, saturation, and lightness, and is often also called HLS. HSV stands for hue, saturation, and value, and is also often called HSB (B for brightness). A third model, common in computer vision applications, is HSI, for hue, saturation, and intensity. However, while typically consistent, these definitions are not standardized, and any of these abbreviations might be used for any of these three or several other related cylindrical models. (For technical definitions of these terms, see below.)
In each cylinder, the angle around the central vertical axis corresponds to "hue", the distance from the axis corresponds to "saturation", and the distance along the axis corresponds to "lightness", "value" or "brightness". Note that while "hue" in HSL and HSV refers to the same attribute, their definitions of "saturation" differ dramatically. Because HSL and HSV are simple transformations of device-dependent RGB models, the physical colors they define depend on the colors of the red, green, and blue primaries of the device or of the particular RGB space, and on the gamma correction used to represent the amounts of those primaries. Each unique RGB device therefore has unique HSL and HSV spaces to accompany it, and numerical HSL or HSV values describe a different color for each basis RGB space.Both of these representations are used widely in computer graphics, and one or the other of them is often more convenient than RGB, but both are also criticized for not adequately separating color-making attributes, or for their lack of perceptual uniformity. Other more computationally intensive models, such as CIELAB or CIECAM02 are said to better achieve these goals.},
	language = {en},
	urldate = {2024-04-11},
	journal = {Wikipedia},
	month = apr,
	year = {2024},
	note = {Page Version ID: 1218017971},
}

@misc{arducam_picamera2_nodate,
	title = {Picamera2 - {Arducam} {Wiki}},
	url = {https://docs.arducam.com/Raspberry-Pi-Camera/Native-camera/PiCamera2-User-Guide/},
	urldate = {2024-04-11},
	author = {Arducam},
	file = {Picamera2 - Arducam Wiki:C\:\\Users\\Guest_\\Zotero\\storage\\QZA873KZ\\PiCamera2-User-Guide.html:text/html},
}

@misc{lotfi_serial_2017,
	type = {Forum},
	title = {Serial {Port} {Communication} over the {USB} in {Raspberry} {Pi} 3 - {Raspberry} {Pi} {Forums}},
	url = {https://forums.raspberrypi.com/viewtopic.php?t=195963},
	urldate = {2024-04-11},
	author = {Lotfi},
	month = oct,
	year = {2017},
}

@misc{the_robotics_back-end_raspberry_2019,
	title = {Raspberry {Pi} {Arduino} {Serial} {Communication} - {Everything} {You} {Need} {To} {Know}},
	url = {https://roboticsbackend.com/raspberry-pi-arduino-serial-communication/},
	abstract = {Raspberry Pi Arduino Serial communication - with complete Python code example. Learn how to connect your boards together, setup software, and write code.},
	language = {en-US},
	urldate = {2024-04-11},
	journal = {The Robotics Back-End},
	author = {The Robotics Back-End},
	month = nov,
	year = {2019},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\364QJ87E\\raspberry-pi-arduino-serial-communication.html:text/html},
}

@misc{openai_openaigym_2023,
	title = {openai/gym},
	copyright = {MIT License},
	url = {https://github.com/openai/gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms.},
	urldate = {2024-04-11},
	publisher = {OpenAI},
	author = {OpenAI},
	month = jan,
	year = {2023},
	note = {original-date: 2016-04-27T14:59:16Z},
}

@misc{andriy_answer_2011,
	title = {Answer to "fastest (low latency) method for {Inter} {Process} {Communication} between {Java} and {C}/{C}++"},
	url = {https://stackoverflow.com/a/6412333},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Andriy},
	month = jun,
	year = {2011},
}

@misc{vsekhar_answer_2011,
	title = {Answer to "{Interprocess} communication in {Python}"},
	url = {https://stackoverflow.com/a/6921402},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {vsekhar},
	month = aug,
	year = {2011},
}

@misc{kelling_answer_2011,
	title = {Answer to "{Interprocess} communication in {Python}"},
	url = {https://stackoverflow.com/a/6921340},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Kelling, Zach},
	month = aug,
	year = {2011},
}

@misc{basj_answer_2020,
	title = {Answer to "{Interprocess} communication in {Python}"},
	url = {https://stackoverflow.com/a/61771563},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Basj},
	month = may,
	year = {2020},
}

@misc{westphal_answer_2014,
	title = {Answer to "{Lazy} pub/sub in zeromq, only get last message"},
	url = {https://stackoverflow.com/a/26383697},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Westphal, Mathieu},
	month = oct,
	year = {2014},
}

@misc{rmunn_answer_2016,
	title = {Answer to "'{Last} message only' option in {ZMQ} {Subscribe} socket"},
	url = {https://stackoverflow.com/a/38257854},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {rmunn},
	month = jul,
	year = {2016},
}

@misc{pilcolearner_cart-pole_2011,
	title = {Cart-{Pole} {Swing}-up},
	url = {https://www.youtube.com/watch?v=XiigTGKZfks},
	abstract = {Learning to control a real cart-pole system from scratch in only 7 trials. The whole learning process is shown. The learning progress can easily be seen after each trial.},
	urldate = {2024-04-11},
	author = {{PilcoLearner}},
	month = may,
	year = {2011},
}

@article{deisenroth_pilco_2011,
	title = {{PILCO}: {A} {Model}-{Based} and {Data}-{Efficient} {Approach} to {Policy} {Search}},
	abstract = {In this paper, we introduce pilco, a practical, data-eﬃcient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning eﬃciency on challenging and high-dimensional control tasks.},
	language = {en},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	month = may,
	year = {2011},
	file = {Deisenroth und Rasmussen - PILCO A Model-Based and Data-Efficient Approach t.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\88IP78HP\\Deisenroth und Rasmussen - PILCO A Model-Based and Data-Efficient Approach t.pdf:application/pdf},
}

@article{gal_improving_2016,
	title = {Improving {PILCO} with {Bayesian} {Neural} {Network} {Dynamics} {Models}},
	abstract = {Model-based reinforcement learning (RL) allows an agent to discover good policies with a small number of trials by generalising observed transitions. Data efﬁciency can be further improved with a probabilistic model of the agent’s ignorance about the world, allowing it to choose actions under uncertainty. Bayesian modelling offers tools for this task, with PILCO [1] being a prominent example, achieving state-of-theart data efﬁciency on low dimensional RL benchmarks. But PILCO relies on Gaussian processes (GPs), which prohibits its applicability to problems that require a larger number of trials to be solved. Further, PILCO does not consider temporal correlation in model uncertainty between successive state transitions, which results in PILCO underestimating state uncertainty at future time steps [2]. In this paper we extend PILCO’s framework to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality. Using particle methods we sample dynamics function realisations, and obtain lower cumulative cost than PILCO. We give insights into the modelling assumptions made in PILCO, and show that moment matching is a crucial simplifying assumption made by the model. Our implementation can leverage GPU architectures, offering faster running time than PILCO, and will allow structured observation spaces to be modelled (images or higher dimensional inputs) in the future.},
	language = {en},
	author = {Gal, Yarin and McAllister, Rowan Thomas and Rasmussen, Carl Edward},
	month = jun,
	year = {2016},
	file = {Gal et al. - Improving PILCO with Bayesian Neural Network Dynam.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\TT7FCIN4\\Gal et al. - Improving PILCO with Bayesian Neural Network Dynam.pdf:application/pdf},
}

@misc{raffin_stable-baselines3_2024,
	title = {stable-baselines3: {Pytorch} version of {Stable} {Baselines}, implementations of reinforcement learning algorithms.},
	copyright = {MIT},
	shorttitle = {stable-baselines3},
	url = {https://github.com/DLR-RM/stable-baselines3},
	urldate = {2024-04-11},
	author = {Raffin, Antonin},
	month = mar,
	year = {2024},
	keywords = {baselines,, data-science, gym,, gymnasium,, machine-learning,, openai,, python,, reinforcement-learning-algorithms,, reinforcement-learning,, stable,, toolbox,},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\MCXY8GGU\\stable-baselines3.html:text/html},
}

@misc{nikhilaggarwal3_queue_2019,
	title = {Queue in {Python}},
	url = {https://www.geeksforgeeks.org/queue-in-python/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-US},
	urldate = {2024-04-16},
	journal = {GeeksforGeeks},
	author = {nikhilaggarwal3},
	month = oct,
	year = {2019},
	note = {Section: Python},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\K3HXUSM5\\queue-in-python.html:text/html},
}

@misc{tensorboard_team_tensorflowtensorboard_2024,
	title = {tensorflow/tensorboard},
	copyright = {Apache-2.0},
	url = {https://github.com/tensorflow/tensorboard},
	abstract = {TensorFlow's Visualization Toolkit},
	urldate = {2024-04-16},
	publisher = {tensorflow},
	author = {Tensorboard Team},
	month = apr,
	year = {2024},
	note = {original-date: 2017-05-15T20:08:07Z},
}

@article{manrique_escobar_parametric_2020,
	title = {A {Parametric} {Study} of a {Deep} {Reinforcement} {Learning} {Control} {System} {Applied} to the {Swing}-{Up} {Problem} of the {Cart}-{Pole}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/24/9013},
	doi = {10.3390/app10249013},
	abstract = {In this investigation, the nonlinear swing-up problem associated with the cart-pole system modeled as a multibody dynamical system is solved by developing a deep Reinforcement Learning (RL) controller. Furthermore, the sensitivity analysis of the deep RL controller applied to the cart-pole swing-up problem is carried out. To this end, the influence of modifying the physical properties of the system and the presence of dry friction forces are analyzed employing the cumulative reward during the task. Extreme limits for the modifications of the parameters are determined to prove that the neural network architecture employed in this work features enough learning capability to handle the task under modifications as high as 90\% on the pendulum mass, as well as a 100\% increment on the cart mass. As expected, the presence of dry friction greatly affects the performance of the controller. However, a post-training of the agent in the modified environment takes only thirty-nine episodes to find the optimal control policy, resulting in a promising path for further developments of robust controllers.},
	language = {en},
	number = {24},
	urldate = {2024-04-17},
	journal = {Applied Sciences},
	author = {Manrique Escobar, Camilo Andrés and Pappalardo, Carmine Maria and Guida, Domenico},
	month = jan,
	year = {2020},
	note = {Number: 24
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, artificial intelligence, cart-pole swing-up, deep reinforcement learning, dry friction, multibody system dynamics, nonlinear control, robustness},
	pages = {9013},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\6I7XCKQM\\Manrique Escobar et al. - 2020 - A Parametric Study of a Deep Reinforcement Learnin.pdf:application/pdf},
}

@article{doya_reinforcement_2000,
	title = {Reinforcement {Learning} in {Continuous} {Time} and {Space}},
	volume = {12},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976600300015961},
	doi = {10.1162/089976600300015961},
	abstract = {This article presents a reinforcement learning framework for continuous-time dynamical systems without a priori discretization of time, state, and action. Basedonthe Hamilton-Jacobi-Bellman (HJB) equation for infinite-horizon, discounted reward problems, we derive algorithms for estimating value functions and improving policies with the use of function approximators. The process of value function estimation is formulated as the minimization of a continuous-time form of the temporal difference (TD) error. Update methods based on backward Euler approximation and exponential eligibility traces are derived, and their correspondences with the conventional residual gradient, TD (0), and TD (λ) algorithms are shown. For policy improvement, two methods—a continuous actor-critic method and a value-gradient-based greedy policy—are formulated. As a special case of the latter, a nonlinear feedback control law using the value gradient and the model of the input gain is derived. The advantage updating, a model-free algorithm derived previously, is also formulated in the HJB-based framework.The performance of the proposed algorithms is first tested in a nonlinear control task of swinging a pendulum up with limited torque. It is shown in the simulations that (1) the task is accomplished by the continuous actor-critic method in a number of trials several times fewer than by the conventional discrete actor-critic method; (2) among the continuous policy update methods, the value-gradient-based policy with a known or learned dynamic model performs several times better than the actor-critic method; and (3) a value function update using exponential eligibility traces is more efficient and stable than that based on Euler approximation. The algorithms are then tested in a higher-dimensional task: cart-pole swing-up. This task is accomplished in several hundred trials using the value-gradient-based policy with a learned dynamic model.},
	number = {1},
	urldate = {2024-04-17},
	journal = {Neural Computation},
	author = {Doya, Kenji},
	month = jan,
	year = {2000},
	pages = {219--245},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\5DPK38RW\\Doya - 2000 - Reinforcement Learning in Continuous Time and Spac.pdf:application/pdf;Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\U2CTTD69\\Reinforcement-Learning-in-Continuous-Time-and.html:text/html},
}

@inproceedings{wawrzynski_model-free_2004,
	title = {Model-free off-policy reinforcement learning in continuous environment},
	volume = {2},
	url = {https://ieeexplore.ieee.org/abstract/document/1380086},
	doi = {10.1109/IJCNN.2004.1380086},
	abstract = {We introduce an algorithm of reinforcement learning in continuous state and action spaces. In order to construct a control policy, the algorithm utilizes the entire history of agent-environment interaction. The policy is a result of an estimation process based on all available information rather than the result of stochastic convergence as in classical reinforcement learning approaches. The policy is derived from the history directly, not through any kind of a model of the environment. We test our algorithm in the cart-pole swing-up simulated environment. The algorithm learns to control this plant in about 100 trials, which corresponds to 15 minutes of plant's real time. This is several times shorter than the one required by other algorithms.},
	urldate = {2024-04-17},
	booktitle = {2004 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks} ({IEEE} {Cat}. {No}.{04CH37541})},
	author = {Wawrzynski, P. and Pacut, A.},
	month = jul,
	year = {2004},
	note = {ISSN: 1098-7576},
	keywords = {Artificial intelligence, Control engineering computing, Convergence, Dynamic programming, History, Learning, Monte Carlo methods, Space technology, Stochastic processes, Testing},
	pages = {1091--1096 vol.2},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\D7CP4JH2\\1380086.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\9ZXKMPWR\\Wawrzynski und Pacut - 2004 - Model-free off-policy reinforcement learning in co.pdf:application/pdf},
}

@inproceedings{kimura_stochastic_1999,
	address = {Tokyo, Japan},
	title = {Stochastic real-valued reinforcement learning to solve a nonlinear control problem},
	volume = {5},
	isbn = {978-0-7803-5731-0},
	url = {http://ieeexplore.ieee.org/document/815604/},
	doi = {10.1109/ICSMC.1999.815604},
	abstract = {This paper presents a new approach to reinforcement learning (RL) to solve a non-linear control problem efficiently in which state and action spaces are continuous, Many DP-based reinforcement learning (RL) algorithms approximate the value function and give a greedy policy with respect to the learned value function. However, it is too expensive to fit highly accurate value functions, particularly in continuous state-action spaces. We provide a hierarchical RL algorithm composed of local linear controllers and TD-learning, which are both very simple. The continuous state space is discretized into an array of coarse boxes, and each box has its own local linear controller for choosing primitive continuous actions. The higher-level of the hierarchy accumulates state-values using tables with one entry for each box. Each linear controller improves the local control policy by using an actor-critic method. The algorithm was applied to a simulation of a cart-pole swing-up problem, and feasible solutions are found in less time than those of conventional discrete RL methods.},
	language = {en},
	urldate = {2024-04-17},
	booktitle = {{IEEE} {SMC}'99 {Conference} {Proceedings}. 1999 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({Cat}. {No}.{99CH37028})},
	publisher = {IEEE},
	author = {Kimura, H. and Kobayashi, S.},
	year = {1999},
	pages = {510--515},
	file = {Kimura und Kobayashi - 1999 - Stochastic real-valued reinforcement learning to s.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\66I7J378\\Kimura und Kobayashi - 1999 - Stochastic real-valued reinforcement learning to s.pdf:application/pdf},
}

@misc{igus_schrittmotoren_2014,
	title = {Schrittmotoren},
	url = {https://docs.rs-online.com/17c8/0900766b81643255.pdf},
	urldate = {2024-04-23},
	author = {igus},
	month = nov,
	year = {2014},
	file = {0900766b81643255.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\QSJQFLSK\\0900766b81643255.pdf:application/pdf},
}

@misc{stepperonline_nema_nodate,
	title = {Nema 17 {Bipolar} {59Ncm}(83.55oz.in) {2A} 42x48mm 4 {Wires} w/ 1m {Cable} \& {Connector} - {17HS19}-{2004S1}{\textbar}{STEPPERONLINE}},
	url = {https://www.omc-stepperonline.com/nema-17-bipolar-59ncm-84oz-in-2a-42x48mm-4-wires-w-1m-cable-connector-17hs19-2004s1},
	urldate = {2024-04-23},
	author = {stepperonline},
	file = {Nema 17 Bipolar 59Ncm(83.55oz.in) 2A 42x48mm 4 Wires w/ 1m Cable & Connector - 17HS19-2004S1|STEPPERONLINE:C\:\\Users\\Guest_\\Zotero\\storage\\SV9TW285\\nema-17-bipolar-59ncm-84oz-in-2a-42x48mm-4-wires-w-1m-cable-connector-17hs19-2004s1.html:text/html},
}

@misc{ma_eureka_2023,
	title = {Eureka: {Human}-{Level} {Reward} {Design} via {Coding} {Large} {Language} {Models}},
	shorttitle = {Eureka},
	url = {http://arxiv.org/abs/2310.12931},
	abstract = {Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex lowlevel manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present EUREKA, a human-level reward design algorithm powered by LLMs. EUREKA exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-theart LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, EUREKA generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, EUREKA outperforms human experts on 83\% of the tasks, leading to an average normalized improvement of 52\%. The generality of EUREKA also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using EUREKA rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.},
	language = {en},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12931 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Ma et al. - 2023 - Eureka Human-Level Reward Design via Coding Large.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\IWLWZP9A\\Ma et al. - 2023 - Eureka Human-Level Reward Design via Coding Large.pdf:application/pdf},
}

@inproceedings{mahmood_benchmarking_2018,
	title = {Benchmarking {Reinforcement} {Learning} {Algorithms} on {Real}-{World} {Robots}},
	url = {https://proceedings.mlr.press/v87/mahmood18a.html},
	abstract = {Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of {The} 2nd {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Mahmood, A. Rupam and Korenkevych, Dmytro and Vasan, Gautham and Ma, William and Bergstra, James},
	month = oct,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {561--591},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\7Z8MZRIR\\Mahmood et al. - 2018 - Benchmarking Reinforcement Learning Algorithms on .pdf:application/pdf},
}

@inproceedings{liu_swing-up_2023,
	address = {Singapore},
	title = {Swing-{Up} and {Balance} {Control} of {Cart}-{Pole} {Based} on {Reinforcement} {Learning} {DDPG}},
	isbn = {978-981-9915-49-1},
	doi = {10.1007/978-981-99-1549-1_33},
	abstract = {As a typical strong artificial intelligence method, reinforcement learning has been applied to real control tasks. Cart-pole system is an ideal controlled object, which is often used to verify the feasibility of control theory. In order to explore the effect of continuous reinforcement learning in the control of physical systems, this paper proposes a full control method for the swing up and stabilization of the cart-pole based on reinforcement learning DDPG. By interacting the cart-pole model with the main body of reinforcement learning, the Actor-Critic framework and the deterministic gradient algorithm DDPG are used to complete the learning of the cart-pole and realize the whole process of swing-up and balance control. Finally, the stability and effectiveness of reinforcement learning for the control of cart-pole are verified by simulation and experiment.},
	language = {en},
	booktitle = {Bio-{Inspired} {Computing}: {Theories} and {Applications}},
	publisher = {Springer Nature},
	author = {Liu, Jie and Zhuan, Xiangtao and Lu, Chuang},
	editor = {Pan, Linqiang and Zhao, Dongming and Li, Lianghao and Lin, Jianqing},
	year = {2023},
	keywords = {Cart-pole system, DDPG, Reinforcement learning},
	pages = {419--429},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\ZX5AVRQ9\\Liu et al. - 2023 - Swing-Up and Balance Control of Cart-Pole Based on.pdf:application/pdf},
}

@misc{pyzmq_team_zeromqpyzmq_2024,
	title = {zeromq/pyzmq},
	copyright = {BSD-3-Clause},
	url = {https://github.com/zeromq/pyzmq},
	abstract = {PyZMQ:  Python bindings for zeromq},
	urldate = {2024-05-09},
	publisher = {The ZeroMQ project},
	author = {pyzmq Team},
	month = may,
	year = {2024},
	note = {original-date: 2010-07-21T07:20:37Z},
	keywords = {cython, python, zeromq},
}

@misc{opencv_team_opencv_nodate,
	title = {{OpenCV}: {Hough} {Circle} {Transform}},
	url = {https://docs.opencv.org/3.4/d4/d70/tutorial_hough_circle.html},
	urldate = {2024-05-09},
	author = {OpenCV Team},
	file = {OpenCV\: Hough Circle Transform:C\:\\Users\\Guest_\\Zotero\\storage\\QHF7V96F\\tutorial_hough_circle.html:text/html},
}

@misc{opencv_team_opencv_nodate-1,
	title = {{OpenCV}: {Structural} {Analysis} and {Shape} {Descriptors}},
	url = {https://docs.opencv.org/4.x/d3/dc0/group__imgproc__shape.html#ga0012a5fdaea70b8a9970165d98722b4c},
	urldate = {2024-05-09},
	author = {OpenCV Team},
	file = {OpenCV\: Structural Analysis and Shape Descriptors:C\:\\Users\\Guest_\\Zotero\\storage\\DCGN5NRG\\group__imgproc__shape.html:text/html},
}

@misc{opencv_team_opencv_nodate-2,
	title = {{OpenCV}: cv::{Moments} {Class} {Reference}},
	url = {https://docs.opencv.org/4.x/d8/d23/classcv_1_1Moments.html},
	urldate = {2024-05-09},
	author = {OpenCV Team},
	file = {OpenCV\: cv\:\:Moments Class Reference:C\:\\Users\\Guest_\\Zotero\\storage\\4DMGJUNY\\classcv_1_1Moments.html:text/html},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}, second edition: {An} {Introduction}},
	isbn = {978-0-262-35270-3},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	note = {Google-Books-ID: uWV0DwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Data Science / Neural Networks, Computers / Programming / Algorithms},
}

@book{szepesvari_algorithms_2022,
	title = {Algorithms for {Reinforcement} {Learning}},
	isbn = {978-3-031-01551-9},
	abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations. Table of Contents: Markov Decision Processes / Value Prediction Problems / Control / For Further Exploration},
	language = {en},
	publisher = {Springer Nature},
	author = {Szepesvári, Csaba},
	month = may,
	year = {2022},
	note = {Google-Books-ID: g4RyEAAAQBAJ},
	keywords = {Computers / Information Technology, Computers / Artificial Intelligence / General, Mathematics / Applied},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	language = {en},
	number = {3},
	urldate = {2024-05-16},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\55YPL2DA\\Watkins und Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{rummery_-line_1994,
	title = {On-{Line} {Q}-{Learning} {Using} {Connectionist} {Systems}},
	abstract = {Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete finite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of different algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Difference algorithm (Sutton 1988), including a new algorithm (Modified Connectionist Q-Learning), and Q() (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each t...},
	journal = {Technical Report CUED/F-INFENG/TR 166},
	author = {Rummery, G. and Niranjan, Mahesan},
	month = nov,
	year = {1994},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\FN8QY9M3\\Rummery und Niranjan - 1994 - On-Line Q-Learning Using Connectionist Systems.pdf:application/pdf},
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement {Learning}: {A} {Survey}},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Reinforcement {Learning}},
	url = {https://www.jair.org/index.php/jair/article/view/10166},
	doi = {10.1613/jair.301},
	abstract = {This paper surveys the field of reinforcement learning from    a computer-science perspective. It is written to be accessible to    researchers familiar with machine learning.  Both the historical basis    of the field and a broad selection of current work are summarized.    Reinforcement learning is the problem faced by an agent that learns    behavior through trial-and-error interactions with a dynamic    environment.  The work described here has a resemblance to work in    psychology, but differs considerably in the details and in the use of    the word ``reinforcement.''  The paper discusses central issues of    reinforcement learning, including trading off exploration and    exploitation, establishing the foundations of the field via Markov    decision theory, learning from delayed reinforcement, constructing    empirical models to accelerate learning, making use of generalization    and hierarchy, and coping with hidden state.  It concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning.},
	language = {en},
	urldate = {2024-05-16},
	journal = {Journal of Artificial Intelligence Research},
	author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
	month = may,
	year = {1996},
	pages = {237--285},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\W47TKRT4\\Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf:application/pdf},
}

@article{bellman_dynamic_1957,
	title = {Dynamic programming and statistical communication theory},
	volume = {43},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.43.8.749},
	doi = {10.1073/pnas.43.8.749},
	number = {8},
	urldate = {2024-05-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bellman, Richard and Kalaba, Robert},
	month = aug,
	year = {1957},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {749--751},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\3RQ3XYER\\Bellman und Kalaba - 1957 - Dynamic programming and statistical communication .pdf:application/pdf},
}

@article{auer_finite-time_2002,
	title = {Finite-time {Analysis} of the {Multiarmed} {Bandit} {Problem}},
	volume = {47},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1013689704352},
	doi = {10.1023/A:1013689704352},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
	language = {en},
	number = {2},
	urldate = {2024-05-16},
	journal = {Machine Learning},
	author = {Auer, Peter and Cesa-Bianchi, Nicolò and Fischer, Paul},
	month = may,
	year = {2002},
	keywords = {adaptive allocation rules, bandit problems, finite horizon regret},
	pages = {235--256},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\3BIUPXTS\\Auer et al. - 2002 - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:application/pdf},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2024-05-16},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science},
	pages = {529--533},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\2GNQUNSH\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@inproceedings{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v48/mniha16.html},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1928--1937},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\KWLKAKC5\\Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\ZJNKPFH7\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\QE7Z7RIZ\\1707.html:text/html},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2024-05-16},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
	file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\CGYX95XH\\Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@article{kober_reinforcement_2013,
	title = {Reinforcement learning in robotics: {A} survey},
	volume = {32},
	issn = {0278-3649},
	shorttitle = {Reinforcement learning in robotics},
	url = {https://doi.org/10.1177/0278364913495721},
	doi = {10.1177/0278364913495721},
	abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
	language = {en},
	number = {11},
	urldate = {2024-05-16},
	journal = {The International Journal of Robotics Research},
	author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
	month = sep,
	year = {2013},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {1238--1274},
	file = {SAGE PDF Full Text:C\:\\Users\\Guest_\\Zotero\\storage\\W6PNMJRY\\Kober et al. - 2013 - Reinforcement learning in robotics A survey.pdf:application/pdf},
}

@article{deng_deep_2017,
	title = {Deep {Direct} {Reinforcement} {Learning} for {Financial} {Signal} {Representation} and {Trading}},
	volume = {28},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/7407387},
	doi = {10.1109/TNNLS.2016.2522401},
	abstract = {Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.},
	number = {3},
	urldate = {2024-05-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Deng, Yue and Bao, Feng and Kong, Youyong and Ren, Zhiquan and Dai, Qionghai},
	month = mar,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Feature extraction, Artificial neural networks, Deep learning (DL), financial signal processing, Learning (artificial intelligence), neural network (NN) for finance, Optimization, reinforcement learning (RL), Robustness, Signal representation, Training},
	pages = {653--664},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\FF6TABE4\\Deng et al. - 2017 - Deep Direct Reinforcement Learning for Financial S.pdf:application/pdf},
}

@misc{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2005.01643},
	doi = {10.48550/arXiv.2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01643 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\N7T965N3\\Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\SZGM2P3W\\2005.html:text/html},
}

@inproceedings{dai_sbeed_2018,
	title = {{SBEED}: {Convergent} {Reinforcement} {Learning} with {Nonlinear} {Function} {Approximation}},
	shorttitle = {{SBEED}},
	url = {https://proceedings.mlr.press/v80/dai18c.html},
	abstract = {When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov’s smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm’s sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1125--1134},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\GSN2L5IT\\Dai et al. - 2018 - SBEED Convergent Reinforcement Learning with Nonl.pdf:application/pdf},
}

@inproceedings{cobbe_quantifying_2019,
	title = {Quantifying {Generalization} in {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v97/cobbe19a.html},
	abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent’s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1282--1289},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\DPZXEUWB\\Cobbe et al. - 2019 - Quantifying Generalization in Reinforcement Learni.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Guest_\\Zotero\\storage\\52RRBFND\\Cobbe et al. - 2019 - Quantifying Generalization in Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{bellemare_distributional_2017,
	title = {A {Distributional} {Perspective} on {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v70/bellemare17a.html},
	abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {449--458},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\JUHUNYZV\\Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Guest_\\Zotero\\storage\\LMCIER3Z\\Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:application/pdf},
}

@misc{pan_policy_2018,
	title = {Policy {Optimization} with {Model}-based {Explorations}},
	url = {http://arxiv.org/abs/1811.07350},
	doi = {10.48550/arXiv.1811.07350},
	abstract = {Model-free reinforcement learning methods such as the Proximal Policy Optimization algorithm (PPO) have successfully applied in complex decision-making problems such as Atari games. However, these methods suffer from high variances and high sample complexity. On the other hand, model-based reinforcement learning methods that learn the transition dynamics are more sample efficient, but they often suffer from the bias of the transition estimation. How to make use of both model-based and model-free learning is a central problem in reinforcement learning. In this paper, we present a new technique to address the trade-off between exploration and exploitation, which regards the difference between model-free and model-based estimations as a measure of exploration value. We apply this new technique to the PPO algorithm and arrive at a new policy optimization method, named Policy Optimization with Model-based Explorations (POME). POME uses two components to predict the actions' target values: a model-free one estimated by Monte-Carlo sampling and a model-based one which learns a transition model and predicts the value of the next state. POME adds the error of these two target estimations as the additional exploration value for each state-action pair, i.e, encourages the algorithm to explore the states with larger target errors which are hard to estimate. We compare POME with PPO on Atari 2600 games, and it shows that POME outperforms PPO on 33 games out of 49 games.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Pan, Feiyang and Cai, Qingpeng and Zeng, An-Xiang and Pan, Chun-Xiang and Da, Qing and He, Hualin and He, Qing and Tang, Pingzhong},
	month = nov,
	year = {2018},
	note = {arXiv:1811.07350 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\E547VX42\\Pan et al. - 2018 - Policy Optimization with Model-based Explorations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\EYEXKNRA\\1811.html:text/html},
}

@misc{wouter_van_heeswijk_proximal_2023,
	title = {Proximal {Policy} {Optimization} ({PPO}) {Explained}},
	url = {https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b},
	abstract = {The journey from REINFORCE to the go-to algorithm in continuous control},
	language = {en},
	urldate = {2024-05-21},
	journal = {Medium},
	author = {Wouter van Heeswijk},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\2UKQZKMW\\proximal-policy-optimization-ppo-explained-abed1952457b.html:text/html},
}

@misc{openai_proximal_2017,
	title = {Proximal {Policy} {Optimization}},
	url = {https://openai.com/index/openai-baselines-ppo/},
	abstract = {We’re releasing a new class of reinforcement learning algorithms, Proximal Policy Optimization (PPO), which perform comparably or better than state-of-the-art approaches while being much simpler to implement and tune. PPO has become the default reinforcement learning algorithm at OpenAI because of its ease of use and good performance.},
	language = {en-US},
	urldate = {2024-05-21},
	author = {OpenAI},
	month = jul,
	year = {2017},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\XGR5IUPE\\openai-baselines-ppo.html:text/html},
}

@article{astrom_swinging_2000,
	title = {Swinging up a pendulum by energy control},
	volume = {36},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109899001405},
	doi = {10.1016/S0005-1098(99)00140-5},
	abstract = {Properties of simple strategies for swinging up an inverted pendulum are discussed. It is shown that the behavior critically depends on the ratio of the maximum acceleration of the pivot to the acceleration of gravity. A comparison of energy-based strategies with minimum time strategy gives interesting insights into the robustness of minimum time solutions.},
	number = {2},
	urldate = {2024-05-21},
	journal = {Automatica},
	author = {Åström, K. J. and Furuta, K.},
	month = feb,
	year = {2000},
	keywords = {Energy control, Inverted pendulum, Minimum time control, Swing-up},
	pages = {287--295},
	file = {Eingereichte Version:C\:\\Users\\Guest_\\Zotero\\storage\\CLVNJB6Z\\Åström und Furuta - 2000 - Swinging up a pendulum by energy control.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\KXL9A9BJ\\S0005109899001405.html:text/html},
}

@misc{kumar_balancing_2020,
	title = {Balancing a {CartPole} {System} with {Reinforcement} {Learning} -- {A} {Tutorial}},
	url = {http://arxiv.org/abs/2006.04938},
	doi = {10.48550/arXiv.2006.04938},
	abstract = {In this paper, we provide the details of implementing various reinforcement learning (RL) algorithms for controlling a Cart-Pole system. In particular, we describe various RL concepts such as Q-learning, Deep Q Networks (DQN), Double DQN, Dueling networks, (prioritized) experience replay and show their effect on the learning performance. In the process, the readers will be introduced to OpenAI/Gym and Keras utilities used for implementing the above concepts. It is observed that DQN with PER provides best performance among all other architectures being able to solve the problem within 150 episodes.},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Kumar, Swagat},
	month = jun,
	year = {2020},
	note = {arXiv:2006.04938 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\GJVIBK93\\Kumar - 2020 - Balancing a CartPole System with Reinforcement Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\Y54A2BVD\\2006.html:text/html},
}

@inproceedings{nagendra_comparison_2017,
	title = {Comparison of reinforcement learning algorithms applied to the cart-pole problem},
	url = {https://ieeexplore.ieee.org/abstract/document/8125811},
	doi = {10.1109/ICACCI.2017.8125811},
	abstract = {Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide an optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cart-pole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy-gradient actorcritic, and value-function approximation are compared in this context with the standard linear quadratic regulator solution. Further, we propose a novel approach for integrating RL and swing-up controllers.},
	urldate = {2024-05-21},
	booktitle = {2017 {International} {Conference} on {Advances} in {Computing}, {Communications} and {Informatics} ({ICACCI})},
	author = {Nagendra, Savinay and Podila, Nikhil and Ugarakhod, Rashmi and George, Koshy},
	month = sep,
	year = {2017},
	keywords = {Learning (artificial intelligence), Approximation algorithms, Benchmark testing, Heuristic algorithms, Intelligent systems, learning systems, nonlinear control systems, Optimal control, Regulators, Space exploration},
	pages = {26--32},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\LVVE35XG\\8125811.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\VTU9HS96\\Nagendra et al. - 2017 - Comparison of reinforcement learning algorithms ap.pdf:application/pdf},
}

@inproceedings{kumar_empirical_2024,
	title = {Empirical study of deep reinforcement learning algorithms for {CartPole} problem},
	url = {https://ieeexplore.ieee.org/abstract/document/10512107},
	doi = {10.1109/SPIN60856.2024.10512107},
	abstract = {CartPole problem is a classical control problem, which is important in many fields, including robotics and self-driving cars. Deep reinforcement learning is applied for controlling the CartPole balancing problem. This study compares the performance of four deep reinforcement learning algorithms: Deep Q-Network (DQN), Proximal Policy Optimization (PPO), QR (Quantile Regression) DQN, and Advantage Actor-Critic (A2C). The CartPole balance problem in the OpenAIGym environment is taken into consideration to execute the deep reinforcement learning algorithms. Many deep Reinforcement algorithms exist, and selecting the right one might be difficult depending on the problem’s characteristics. Our findings can help in the ranking of the four methods based on how well they fit the requirements of a given application. The performance in terms of training time, training timesteps, maximum value of episode-reward-mean, and average frames per second for each method are evaluated. The results demonstrate that the best approach for resolving this problem is to use PPO method with a set of prespecified hyperparameters.},
	urldate = {2024-05-21},
	booktitle = {2024 11th {International} {Conference} on {Signal} {Processing} and {Integrated} {Networks} ({SPIN})},
	author = {Kumar, Yogesh and Kumar, Pankaj},
	month = mar,
	year = {2024},
	note = {ISSN: 2688-769X},
	keywords = {Artificial neural networks, Training, Advantage Actor-Critic, Aerospace electronics, Autonomous automobiles, CartPole, Classical Control Problem, Deep Q-Network, Deep reinforcement learning, Proximal Policy Optimization, QR (Quantile Regression) DQN, Signal processing, Signal processing algorithms, Stable Baselines3},
	pages = {78--83},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\43DW5JJW\\10512107.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\TTGMCC47\\Kumar und Kumar - 2024 - Empirical study of deep reinforcement learning alg.pdf:application/pdf},
}

@article{florian_correct_2007,
	title = {Correct equations for the dynamics of the cart-pole system},
	abstract = {The problem of balancing a pole on a moving cart is a widely used benchmark problem for testing reinforcement learning algorithms. The classic papers that introduced this problem contain mistakes in the equations that govern the dynamics of the cart-pole system, and these mistakes propagated in other studies that used the same problem as a benchmark. Here we provide the equations that describe correctly the dynamics of the system.},
	language = {en},
	author = {Florian, Razvan V},
	month = feb,
	year = {2007},
	file = {Florian - Correct equations for the dynamics of the cart-pol.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\GYQS475Q\\Florian - Correct equations for the dynamics of the cart-pol.pdf:application/pdf},
}

@phdthesis{nayante_reinforcement_2021,
	address = {Gummersbach},
	type = {Bachelor {Thesis}},
	title = {Reinforcement {Learning} am realen und simulierten {Cart}-{Pole}-{Swing}-{Up} {Pendulum} im {Vergleich}},
	url = {https://www.gm.fh-koeln.de/~konen/research/PaperPDF/BA_Nayante2021_final.pdf},
	language = {de},
	urldate = {2024-05-21},
	school = {TH Köln},
	author = {Nayante, Yendoukon},
	month = sep,
	year = {2021},
	file = {BA_Nayante2021_final.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\344ZKJLT\\BA_Nayante2021_final.pdf:application/pdf},
}

@inproceedings{rasmussen_probabilistic_2008,
	address = {Berlin, Heidelberg},
	title = {Probabilistic {Inference} for {Fast} {Learning} in {Control}},
	isbn = {978-3-540-89722-4},
	doi = {10.1007/978-3-540-89722-4_18},
	abstract = {We provide a novel framework for very fast model-based reinforcement learning in continuous state and action spaces. The framework requires probabilistic models that explicitly characterize their levels of confidence. Within this framework, we use flexible, non-parametric models to describe the world based on previously collected experience. We demonstrate learning on the cart-pole problem in a setting where we provide very limited prior knowledge about the task. Learning progresses rapidly, and a good policy is found after only a hand-full of iterations.},
	language = {en},
	booktitle = {Recent {Advances} in {Reinforcement} {Learning}},
	publisher = {Springer},
	author = {Rasmussen, Carl Edward and Deisenroth, Marc Peter},
	editor = {Girgin, Sertan and Loth, Manuel and Munos, Rémi and Preux, Philippe and Ryabko, Daniil},
	year = {2008},
	keywords = {Gaussian Process, Goal State, Neural Information Processing System, Reinforcement Learning, Successor State},
	pages = {229--242},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\U2ZDU4NP\\Rasmussen und Deisenroth - 2008 - Probabilistic Inference for Fast Learning in Contr.pdf:application/pdf},
}

@misc{fattnerd_how-_2022,
	title = {[{HOW}-{TO}] camera stuck waiting for image · {Issue} \#336 · raspberrypi/picamera2},
	url = {https://github.com/raspberrypi/picamera2/issues/336},
	abstract = {Please only ask one question per issue! Describe what it is that you want to accomplish I use the camera in a python function to capture QRCODES. first time the function is called, it works fine an...},
	language = {en},
	urldate = {2024-05-24},
	journal = {GitHub},
	author = {FattNerd},
	month = oct,
	year = {2022},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\6VFIBK2J\\336.html:text/html},
}

@article{rowley_neural_1998,
	title = {Neural network-based face detection},
	volume = {20},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/655647},
	doi = {10.1109/34.655647},
	abstract = {We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.},
	number = {1},
	urldate = {2024-05-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rowley, H.A. and Baluja, S. and Kanade, T.},
	month = jan,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Machine learning, Artificial neural networks, Computer vision, Detectors, Face detection, Filters, Machine learning algorithms, Neural networks, Pattern recognition, Pixel},
	pages = {23--38},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\2X8DT64R\\655647.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\TGMSPV6I\\Rowley et al. - 1998 - Neural network-based face detection.pdf:application/pdf},
}

@inproceedings{deng_new_2013,
	title = {New types of deep neural network learning for speech recognition and related applications: an overview},
	shorttitle = {New types of deep neural network learning for speech recognition and related applications},
	url = {https://ieeexplore.ieee.org/abstract/document/6639344},
	doi = {10.1109/ICASSP.2013.6639344},
	abstract = {In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications,” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.},
	urldate = {2024-05-28},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
	month = may,
	year = {2013},
	note = {ISSN: 2379-190X},
	keywords = {Optimization, Training, Neural networks, Acoustics, convolutional neural network, deep neural network, Hidden Markov models, multilingual, multitask, music processing, optimization, recurrent neural network, spectrogram features, Speech, speech recognition, Speech recognition},
	pages = {8599--8603},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\558EJNDX\\6639344.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\F95Q3AXB\\Deng et al. - 2013 - New types of deep neural network learning for spee.pdf:application/pdf},
}

@article{zhang_deep_2015,
	title = {Deep {Neural} {Networks} in {Machine} {Translation}: {An} {Overview}},
	volume = {30},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1541-1672},
	shorttitle = {Deep {Neural} {Networks} in {Machine} {Translation}},
	url = {http://ieeexplore.ieee.org/document/7243232/},
	doi = {10.1109/MIS.2015.69},
	language = {en},
	number = {5},
	urldate = {2024-05-28},
	journal = {IEEE Intelligent Systems},
	author = {Zhang, Jiajun and Zong, Chengqing},
	month = sep,
	year = {2015},
	pages = {16--25},
	file = {Zhang und Zong - 2015 - Deep Neural Networks in Machine Translation An Ov.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\2FZ49LCD\\Zhang und Zong - 2015 - Deep Neural Networks in Machine Translation An Ov.pdf:application/pdf},
}

@incollection{bengio_practical_2012,
	address = {Berlin, Heidelberg},
	title = {Practical {Recommendations} for {Gradient}-{Based} {Training} of {Deep} {Architectures}},
	isbn = {978-3-642-35289-8},
	url = {https://doi.org/10.1007/978-3-642-35289-8_26},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	language = {en},
	urldate = {2024-05-29},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
	publisher = {Springer},
	author = {Bengio, Yoshua},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_26},
	pages = {437--478},
	file = {Eingereichte Version:C\:\\Users\\Guest_\\Zotero\\storage\\YDN2A7RB\\Bengio - 2012 - Practical Recommendations for Gradient-Based Train.pdf:application/pdf},
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	shorttitle = {Machine learning},
	url = {https://www.science.org/doi/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
	number = {6245},
	urldate = {2024-05-29},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	month = jul,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {255--260},
}

@misc{google_deepmind_alphago_2020,
	title = {{AlphaGo}},
	url = {https://deepmind.google/technologies/alphago/},
	abstract = {Novel AI system mastered the ancient game of Go, defeated a Go world champion, and inspired a new era of AI.},
	language = {en},
	urldate = {2024-05-29},
	journal = {Google DeepMind},
	author = {Google DeepMind},
	month = dec,
	year = {2020},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\EFKE3IU6\\alphago.html:text/html},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2024-05-29},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science, Reward},
	pages = {354--359},
	file = {Eingereichte Version:C\:\\Users\\Guest_\\Zotero\\storage\\QZSR9ZYW\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@misc{nicholas_renotte_reinforcement_2021,
	title = {Reinforcement {Learning} in 3 {Hours} {\textbar} {Full} {Course} using {Python}},
	url = {https://www.youtube.com/watch?v=Mut_u40Sqz4},
	abstract = {Want to get started with Reinforcement Learning?

This is the course for you!

This course will take you through all of the fundamentals required to get started with reinforcement learning with Python, OpenAI Gym and Stable Baselines. You'll be able to build deep learning powered agents to solve a varying number of RL problems including CartPole, Breakout and CarRacing as well as learning how to build your very own environment!

In this video you'll learn: 
1. All the basics to get up and started with Reinforcement Learning
2. How to build custom environments using OpenAI Gym
3. About working on custom projects for Reinforcement Learning

Get the code for this tutorial: https://github.com/nicknochnack/Reinf...

Links Mentioned
Stable Baselines 3: https://stable-baselines3.readthedocs...
OpenAI Gym: https://gym.openai.com/
PyTorch: https://pytorch.org/
Atarimania ROMs: http://www.atarimania.com/roms/Roms.rar
Swig: http://www.swig.org/Doc1.3/Windows.html

Chapters
0:00 - Start
0:23 - Introduction
1:15 - Gameplan
4:24 - RL in a Nutshell
13:30 - 1. Setup Stable Baselines
21:45 - 2. Environments
30:10 - Loading OpenAI Gym Environments
40:00 - Understanding OpenAI Gym Environments
42:58 - 3. Training
51:32 - Train a Reinforcement Learning Model
1:00:00 - Saving and Reloading Environments
1:04:23 - 4. Testing and Evaluation
1:06:35 - Evaluating RL Models
1:09:34 - Testing the Agent
1:15:56 - Viewing Logs in Tensorboard
1:24:50 - Performance Tuning
1:26:31 - 5. Callbacks, Alternate Algorithms, Neural Networks
1:27:39 - Adding Training Callbacks
1:34:44 - Changing Policies
1:38:27 - Changing Algorithms
1:40:29 - 6. Projects
1:41:31 - Project 1 Atari
1:41:51 - Importing Dependencies
1:44:16 - Applying GPU Acceleration with PyTorch
1:45:11 - Testing Atari Environments
1:51:35 - Vectorizing Environments
1:56:48 - Save and Reload Atari Model
1:57:45 - Evaluate and Test Atari RL Model
2:02:16 - Updated Performance
2:06:34 - Project 2 Autonomous Driving
2:06:56 - Installing Dependencies
2:09:27 - Test CarRacing-v0 Environment
2:12:23 - Train Autonomous Driving Agent
2:17:16 - Save and Reload Self Driving model
2:18:20 - Updated Self Driving Performance
2:28:56 - Project 3 Custom Open AI Gym Environments
2:29:35 - Import Dependencies for Custom Environment
2:32:00 - Types of OpenAI Gym Spaces
2:38:47 - Building a Custom Open AI Environment
2:51:49 - Testing a Custom Environment
2:52:49 - Train a RL Model for a Custom Environment
2:56:22 - Save a Custom Environment Model
2:58:49 - 7. Wrap Up

Oh, and don't forget to connect with me!
LinkedIn: https://bit.ly/324Epgo
Facebook: https://bit.ly/3mB1sZD
GitHub: https://bit.ly/3mDJllD
Patreon: https://bit.ly/2OCn3UW
Join the Discussion on Discord: https://bit.ly/3dQiZsV

Happy coding!
Nick

P.s. Let me know how you go and drop a comment if you need a hand!},
	urldate = {2024-05-29},
	author = {{Nicholas Renotte}},
	month = jun,
	year = {2021},
}

@misc{sentdex_q_2019,
	title = {Q {Learning} {Intro}/{Table} - {Reinforcement} {Learning} p.1},
	url = {https://www.youtube.com/watch?v=yMk_XtIEzH8},
	abstract = {Welcome to a reinforcement learning tutorial. In this part, we're going to focus on Q-Learning.

Q-Learning is a model-free form of machine learning, in the sense that the AI "agent" does not need to know or have a model of the environment that it will be in. The same algorithm can be used across a variety of environments.

For a given environment, everything is broken down into "states" and "actions." The states are observations and samplings that we pull from the environment, and the actions are the choices the agent has made based on the observation. For the purposes of the rest of this tutorial, we'll use the context of our environment to exemplify how this works.

Text-based tutorial and sample code: https://pythonprogramming.net/q-learn...

Channel membership:    / @sentdex  
Discord:   / discord  
Support the content: https://pythonprogramming.net/support...
Twitter:   / sentdex  
Instagram:   / sentdex  
Facebook:   / pythonprogramming.net  
Twitch:   / sentdex  

\#reinforcementlearning \#machinelearning \#python},
	urldate = {2024-05-29},
	author = {{sentdex}},
	month = may,
	year = {2019},
}

@misc{arxiv_insights_introduction_2018,
	title = {An introduction to {Policy} {Gradient} methods - {Deep} {Reinforcement} {Learning}},
	url = {https://www.youtube.com/watch?v=5P7I-xPq8u8},
	abstract = {In this episode I introduce Policy Gradient methods for Deep Reinforcement Learning.

After a general overview, I dive into Proximal Policy Optimization: an algorithm designed at OpenAI that tries to find a balance between sample efficiency and code complexity. PPO is the algorithm used to train the OpenAI Five system and is also used in a wide range of other challenges like Atari and robotic control tasks.

If you want to support this channel, here is my patreon link:
  / arxivinsights   --- You are amazing!! ;)

If you have questions you would like to discuss with me personally, you can book a 1-on-1 video call through Pensight: https://pensight.com/x/xander-steenbr... 

Links mentioned in the video:
⦁ PPO paper:   https://arxiv.org/abs/1707.06347
⦁ TRPO paper: https://arxiv.org/abs/1502.05477
⦁ OpenAI PPO blogpost: https://blog.openai.com/openai-baseli...
⦁ Aurelien Geron: KL divergence and entropy in ML:    • A Short Introduction to Entropy, Cros...  
⦁ Deep RL Bootcamp - Lecture 5:    • Deep RL Bootcamp  Lecture 5: Natural ...  
⦁ RL-adventure PyTorch implementation: https://github.com/higgsfield/RL-Adve...
⦁ OpenAI Baselines TensorFlow implementation: https://github.com/openai/baselines},
	urldate = {2024-05-29},
	author = {{Arxiv Insights}},
	month = oct,
	year = {2018},
}

@misc{gordon77_pi_2023,
	type = {Forum},
	title = {Pi camera v3 manual focus and libcamera-apps ? - {Raspberry} {Pi} {Forums}},
	url = {https://forums.raspberrypi.com/viewtopic.php?t=353887},
	urldate = {2024-06-13},
	author = {gordon77},
	month = jul,
	year = {2023},
	file = {Pi camera v3 manual focus and libcamera-apps ? - Raspberry Pi Forums:C\:\\Users\\Guest_\\Zotero\\storage\\LPQXKNUN\\viewtopic.html:text/html},
}

@article{kai_new_2014,
	title = {A new discrete mechanics approach to swing-up control of the cart-pendulum system},
	volume = {19},
	issn = {1007-5704},
	url = {https://www.sciencedirect.com/science/article/pii/S1007570413002232},
	doi = {10.1016/j.cnsns.2013.05.021},
	abstract = {This paper develops a new swing-up control method for the cart-pendulum system via discrete mechanics. The swing-up control law consists of two parts: the swing-up stage and the stabilization one. In the swing-up stage, we use a controller based on a discrete Lyapunov function and it can swing up the pendulum. Then, in the stabilization stage, we utilize a stabilizing controller based on the linearized system and discrete-time optimal regulator theory. In addition, transformation methods from discrete control inputs into continuous zero-order hold inputs are introduced. From some simulation results, we can confirm that the cart-pendulum system is swung up and stabilized by our new method.},
	number = {1},
	urldate = {2024-08-29},
	journal = {Communications in Nonlinear Science and Numerical Simulation},
	author = {Kai, Tatsuya and Bito, Kensuke},
	month = jan,
	year = {2014},
	keywords = {Cart-pendulum system, Discrete mechanics, Solvability analysis, Stabilization, Swing-up control, Zero-order hold input},
	pages = {230--244},
	file = {Kai und Bito - 2014 - A new discrete mechanics approach to swing-up cont.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\LKM3LZ98\\Kai und Bito - 2014 - A new discrete mechanics approach to swing-up cont.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\8XMHF2YJ\\S1007570413002232.html:text/html},
}

@article{mousa_stabilizing_2015,
	title = {Stabilizing and {Swinging}-{Up} the {Inverted} {Pendulum} {Using} {PI} and {PID} {Controllers} {Based} on {Reduced} {Linear} {Quadratic} {Regulator} {Tuned} by {PSO}:},
	volume = {4},
	issn = {2160-9772, 2160-9799},
	shorttitle = {Stabilizing and {Swinging}-{Up} the {Inverted} {Pendulum} {Using} {PI} and {PID} {Controllers} {Based} on {Reduced} {Linear} {Quadratic} {Regulator} {Tuned} by {PSO}},
	url = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJSDA.2015100104},
	doi = {10.4018/IJSDA.2015100104},
	abstract = {The inherited instabilities in the Inverted Pendulum (IP) system make it one of the most difficult nonlinear problems in the control theory. In this research work, Proportional –Integral and Derivative (PID) Controller with a feed forward gain is used with Reduced Linear Quadratic Regulator (RLQR) for stabilizing the Cart Position and Swinging-up the Pendulum angle. Tuning the Controllers' gains is achieved by using Particle Swarm Optimization (PSO) Technique. Obtaining the combined PID controllers' gains with a feed forward gain and RLQR is a multi-dimensions control problem. The Proposed Controllers give minimum Settling Time, Rise Time, Undershoot and Over shoot for both the Cart Position and the Pendulum angle. A disturbance with different amplitudes is applied to the system, and the results showed the robustness of the systems based on the tuned controllers. The overall results are promising.},
	language = {ng},
	number = {4},
	urldate = {2024-08-29},
	journal = {International Journal of System Dynamics Applications},
	author = {Mousa, M. E. and Ebrahim, M. A. and Hassan, M. A. Moustafa},
	month = oct,
	year = {2015},
	pages = {52--69},
	file = {Mousa et al. - 2015 - Stabilizing and Swinging-Up the Inverted Pendulum .pdf:C\:\\Users\\Guest_\\Zotero\\storage\\EYWHR26D\\Mousa et al. - 2015 - Stabilizing and Swinging-Up the Inverted Pendulum .pdf:application/pdf},
}

@article{chang_design_2007,
	title = {Design of nonlinear controller for bi-axial inverted pendulum system},
	volume = {1},
	issn = {1751-8644, 1751-8652},
	url = {https://digital-library.theiet.org/content/journals/10.1049/iet-cta_20060338},
	doi = {10.1049/iet-cta:20060338},
	language = {en},
	number = {4},
	urldate = {2024-08-29},
	journal = {IET Control Theory \& Applications},
	author = {Chang, L.-H. and Lee, A.-C.},
	month = jul,
	year = {2007},
	pages = {979--986},
	file = {Chang und Lee - 2007 - Design of nonlinear controller for bi-axial invert.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\Y8ZFZ2NY\\Chang und Lee - 2007 - Design of nonlinear controller for bi-axial invert.pdf:application/pdf},
}

@inproceedings{kumari_root_2023,
	title = {Root {Locus} {Analysis} for {Swinging} {Up} and {Stabilizing} {Inverted} {Pendulum} {Cart} {System}},
	url = {https://ieeexplore.ieee.org/document/10205880/?arnumber=10205880&tag=1},
	doi = {10.1109/CONIT59222.2023.10205880},
	abstract = {The paper presents swinging up and stabilization of inverted pendulum cart system (IPCS) and analysis of applied controller via root locus method. The IPCS is highly non-linear system which is analysed by the response after application of impulse input to the system. The controller which has combined feature of linear quadratic regulator (LQR) and conventional PID is proposed to handle the system. The root locus are drawn with and without application of the conventional PID and proposed controllers. It reveals that after application of controller, the roots shift towards left hand side (LHS) which are shown via root locus method. Initially, the system is merely controlled with conventional controller and then after LQR is incorporated in the feedback to control the system. The time specifications result are calculated and compared. The results reveal that the controller which is composed with PID and LQR shows better results as compared to the merely application of PID controller. The disturbance effect is also presented to show performance of the proposed controller. A number of simulated waveforms e.g. cart position and pendulum angle response are presented to show controller’s performance.},
	urldate = {2024-08-29},
	booktitle = {2023 3rd {International} {Conference} on {Intelligent} {Technologies} ({CONIT})},
	author = {Kumari, Pallavi and Agarwal, Ruchi},
	month = jun,
	year = {2023},
	keywords = {Optimization, Regulators, inverted pendulum system, IPCS, LQR control, PD control, PI control, PID control, Tuning},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\TXFUTMTA\\10205880.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\AU4MKWP2\\Kumari und Agarwal - 2023 - Root Locus Analysis for Swinging Up and Stabilizin.pdf:application/pdf},
}

@inproceedings{slotine_applied_1991,
	title = {Applied {Nonlinear} {Control}},
	url = {https://www.semanticscholar.org/paper/Applied-Nonlinear-Control-Slotine-Li/1ae0d9625f9f580a3b8d8e92a0edbc2087a1cc0e},
	abstract = {Covers in a progressive fashion a number of analysis tools and design techniques directly applicable to nonlinear control problems in high performance systems (in aerospace, robotics and automotive areas).},
	urldate = {2024-08-29},
	author = {Slotine, J. and Li, Weiping},
	year = {1991},
}

@inproceedings{kalman_contributions_1960,
	title = {Contributions to the {Theory} of {Optimal} {Control}},
	url = {https://www.semanticscholar.org/paper/Contributions-to-the-Theory-of-Optimal-Control-K%C3%A1lm%C3%A1n/4602a97c4965a9f6c41c9a7eeaef5be8333dbaef},
	abstract = {THIS is one of the two ground-breaking papers by Kalman that appeared in 1960—with the other one (discussed next) being the filtering and prediction paper. This first paper, which deals with linear-quadratic feedback control, set the stage for what came to be known as LQR (Linear-Quadratic-Regulator) control, while the combination of the two papers formed the basis for LQG (Linear-Quadratic-Gaussian) control. Both LQR and LQG control had major influence on researchers, teachers, and practitioners of control in the decades that followed. The idea of designing a feedback controller such that the integral of the square of tracking error is minimized was first proposed by Wiener [17] and Hall [8], and further developed in the influential book by Newton, Gould and Kaiser [12]. However, the problem formulation in this book remained unsatisfactory from a mathematical point of view, but, more importantly, the algorithms obtained allowed application only to rather low order systems and were thus of limited value. This is not surprising since it basically took until theH2-interpretation in the 1980s of LQG control before a satisfactory formulation of least squares feedback control design was obtained. Kalman’s formulation in terms of finding the least squares control that evolves from an arbitrary initial state is a precise formulation of the optimal least squares transient control problem. The paper introduced the very important notion of c ntrollability, as the possibility of transfering any initial state to zero by a suitable control action. It includes the necessary and sufficient condition for controllability in terms of the positive definiteness of the Controllability Grammian, and the fact that the linear time-invariant system withn states,},
	urldate = {2024-08-29},
	author = {Kálmán, R.},
	year = {1960},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\PCMFKYPQ\\Kálmán - 1960 - Contributions to the Theory of Optimal Control.pdf:application/pdf},
}

@inproceedings{liu_real-time_2009,
	title = {Real-time controlling of inverted pendulum by fuzzy logic},
	url = {https://ieeexplore.ieee.org/document/5262618/?arnumber=5262618&tag=1},
	doi = {10.1109/ICAL.2009.5262618},
	abstract = {Inverted pendulum is a typical multivariable, nonlinear, strong-coupling, instable system. The basic aim of our work was to balance a real pendulum in the position in center of course. For this purpose we used fuzzy logic controller. The fuzzy logic controller designed in the Matlab-Simulink environment. In this paper, the inverted pendulum mathematical model is built. MATLAB based Hardware in Loop simulation system is designed. A novel expert fuzzy control scheme was proposed. The proposed control scheme was implemented in Matlab and showed good performance in the real-time fuzzy control of the inverted pendulum. The results of simulation and experiment indicated that the control method has good control ability.},
	urldate = {2024-08-29},
	booktitle = {2009 {IEEE} {International} {Conference} on {Automation} and {Logistics}},
	author = {Liu, Yanmei and Chen, Zhen and Xue, Dingyu and Xu, Xinhe},
	month = aug,
	year = {2009},
	note = {ISSN: 2161-816X},
	keywords = {Automation, Control systems, Equations, Fuzzy control, Fuzzy logic, Hardware, inverted pendulum, Mathematical model, mathematical model fuzzy control, Nonlinear control systems, Real time systems, real-time, Software prototyping},
	pages = {1180--1183},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\UJCIES84\\5262618.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\WDCFVMQG\\Liu et al. - 2009 - Real-time controlling of inverted pendulum by fuzz.pdf:application/pdf},
}

@inproceedings{nour_fuzzy_2007,
	title = {Fuzzy logic control vs. conventional {PID} control of an inverted pendulum robot},
	url = {https://ieeexplore.ieee.org/document/4658376},
	doi = {10.1109/ICIAS.2007.4658376},
	abstract = {This paper addresses some of the potential benefits of using fuzzy logic controllers to control an inverted pendulum system. The stages of the development of a fuzzy logic controller using a four input Takagi-Sugeno fuzzy model were presented. The main idea of this paper is to implement and optimize fuzzy logic control algorithms in order to balance the inverted pendulum and at the same time reducing the computational time of the controller. In this work, the inverted pendulum system was modeled and constructed using Simulink and the performance of the proposed fuzzy logic controller is compared to the more commonly used PID controller through simulations using Matlab. Simulation results show that the fuzzy logic controllers are far more superior compared to PID controllers in terms of overshoot, settling time and response to parameter changes.},
	urldate = {2024-08-29},
	booktitle = {2007 {International} {Conference} on {Intelligent} and {Advanced} {Systems}},
	author = {Nour, M. I. H. and Ooi, J. and Chan, K. Y.},
	month = nov,
	year = {2007},
	keywords = {Control systems, Fuzzy logic, Mathematical model, Acceleration, Angular velocity, Computational modeling, Force},
	pages = {209--214},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\5WLX93J9\\Nour et al. - 2007 - Fuzzy logic control vs. conventional PID control o.pdf:application/pdf},
}

@article{maarif_backstepping_2022,
	title = {Backstepping {Sliding} {Mode} {Control} for {Inverted} {Pendulum} {System} with {Disturbance} and {Parameter} {Uncertainty}},
	volume = {3},
	copyright = {Copyright (c) 2021 Alfian Ma'arif},
	issn = {2715-5072},
	url = {https://journal.umy.ac.id/index.php/jrc/article/view/12739},
	doi = {10.18196/jrc.v3i1.12739},
	abstract = {The inverted pendulum system is highly popular in control system applications and has the characteristics of unstable, nonlinear, and fast dynamics. A nonlinear controller is needed to control a system with these characteristics. In addition, there are disturbances and parameter uncertainty issues to be solved in the inverted pendulum system. Therefore, this study uses a nonlinear controller, which is the backstepping sliding mode control. The controller is robust to parameter uncertainty and disturbances so that it is suitable for controlling an inverted pendulum system. Based on testing with step and sine reference signals without interference, the controller can stabilize the system well and has a fast response. In testing with disturbances and mass uncertainty, the backstepping sliding mode controller is robust against these changes and able to make the system reach the reference value. Compared with sliding mode control, backstepping sliding mode control has a better and more robust response to disturbances and parameter uncertainty.},
	language = {en},
	number = {1},
	urldate = {2024-08-29},
	journal = {Journal of Robotics and Control (JRC)},
	author = {Ma'arif, Alfian and Vera, Marco Antonio Márquez and Mahmoud, Magdi Sadek and Ladaci, Samir and Çakan, Abdullah and Parada, Jonattan Niño},
	year = {2022},
	note = {Number: 1},
	keywords = {Backstepping, Disturbance, Inverted Pendulum, Nonlinear, Sliding Mode Control, Uncertainty},
	pages = {86--92},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\5QMWVEWL\\Ma'arif et al. - 2022 - Backstepping Sliding Mode Control for Inverted Pen.pdf:application/pdf},
}

@article{sanjeewa_control_2022,
	title = {Control of rotary double inverted pendulum system using {LQR} sliding surface based sliding mode controller},
	volume = {9},
	issn = {2330-7706},
	url = {https://doi.org/10.1080/23307706.2021.1914758},
	doi = {10.1080/23307706.2021.1914758},
	abstract = {This paper presents LQR sliding surface-based Sliding Mode Controller (LQR–SMC) for balancing control of a Rotary Double Inverted Pendulum (RDIP) system. It is a challenging research topic in control engineering due to its nonlinearity and instability. The RDIP system uses only a motor to control two serially connected pendulums to stand at the upright position. The sliding surface is designed based on the LQR optimal gain. Nonsingular gain matrix is obtained by using the left inverse of the input matrix in the state space form of the system dynamics. The Lyapunov stability theory is used to determine the stability of the controller. To evaluate the performance of LQR–SMC, some performance indices, including the Integral Absolute Error (IAE), Integral Time Absolute Error (ITAE), and the Integrated Square Error (ISE), are used. System stability can be maintained by LQR–SMC under external disturbances as well as model and parameter uncertainties.},
	number = {1},
	urldate = {2024-08-29},
	journal = {Journal of Control and Decision},
	author = {Sanjeewa, Sondarangallage D.A. and Parnichkun, Manukid},
	month = jan,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/23307706.2021.1914758},
	keywords = {linear quadratic regulator sliding surface based sliding mode control, LQR, Rotary double inverted pendulum, SMC},
	pages = {89--101},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\T262R7TW\\Sanjeewa und Parnichkun - 2022 - Control of rotary double inverted pendulum system .pdf:application/pdf},
}

@article{ibanez_lyapunov-based_2005,
	title = {Lyapunov-{Based} {Controller} for the {Inverted} {Pendulum} {Cart} {System}},
	volume = {40},
	issn = {1573-269X},
	url = {https://doi.org/10.1007/s11071-005-7290-y},
	doi = {10.1007/s11071-005-7290-y},
	abstract = {A nonlinear control force is presented to stabilize the under-actuated inverted pendulum mounted on a cart. The control strategy is based on partial feedback linearization, in a first stage, to linearize only the actuated coordinate of the inverted pendulum, and then, a suitable Lyapunov function is formed to obtain a stabilizing feedback controller. The obtained closed-loop system is locally asymptotically stable around its unstable equilibrium point. Additionally, it has a very large attraction domain.},
	language = {en},
	number = {4},
	urldate = {2024-08-29},
	journal = {Nonlinear Dynamics},
	author = {Ibañez, Carlos Aguilar and Frias, O. Gutiérrez and Castañón, M. Suárez},
	month = jun,
	year = {2005},
	keywords = {Automotive Engineering, Lyapunov-based control, nonlinear systems, under-actuated system},
	pages = {367--374},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\LQC4SMB5\\Ibañez et al. - 2005 - Lyapunov-Based Controller for the Inverted Pendulu.pdf:application/pdf},
}

@article{abido_optimal_2002,
	title = {Optimal design of power-system stabilizers using particle swarm optimization},
	volume = {17},
	issn = {1558-0059},
	url = {https://ieeexplore.ieee.org/document/1033970},
	doi = {10.1109/TEC.2002.801992},
	abstract = {In this paper, a novel evolutionary algorithm-based approach to optimal design of multimachine power-system stabilizers (PSSs) is proposed. The proposed approach employs a particle-swarm-optimization (PSO) technique to search for optimal settings of PSS parameters. Two eigenvalue-based objective functions to enhance system damping of electromechanical modes are considered. The robustness of the proposed approach to the initial guess is demonstrated. The performance of the proposed PSO-based PSS (PSOPSS) under different disturbances, loading conditions, and system configurations is tested and examined for different multimachine power systems. Eigenvalue analysis and nonlinear simulation results show the effectiveness of the proposed PSOPSSs to damp out the local and interarea modes of oscillations and work effectively over a wide range of loading conditions and system configurations. In addition, the potential and superiority of the proposed approach over the conventional approaches is demonstrated.},
	number = {3},
	urldate = {2024-08-29},
	journal = {IEEE Transactions on Energy Conversion},
	author = {Abido, M.A.},
	month = sep,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Energy Conversion},
	keywords = {Robustness, Algorithm design and analysis, Analytical models, Damping, Eigenvalues and eigenfunctions, Evolutionary computation, Particle swarm optimization, Power system analysis computing, Power system simulation, System testing},
	pages = {406--413},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\ITDD7MXL\\1033970.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\HG36GI4T\\Abido - 2002 - Optimal design of power-system stabilizers using p.pdf:application/pdf},
}

@article{dahiya_introduction_2016,
	title = {Introduction to fuzzy {Logic}},
	volume = {2},
	issn = {2309-5504},
	url = {https://typeset.io/papers/introduction-to-fuzzy-logic-25lekr11ns},
	abstract = {The paper presents a brief introduction to fuzzy logic concepts. The concepts of fuzzy logic can be applied to real world problems to get required solutions. The concept of fuzziness resembles human thought process and is therefore popular in practical life to give optimistic solutions to real world problems.},
	language = {en},
	number = {2},
	urldate = {2024-10-14},
	journal = {International journal of applied research},
	author = {Dahiya, Naveen},
	month = feb,
	year = {2016},
	pages = {345--347},
}

@article{rehman_introduction_2017,
	title = {An {Introduction} to {Fuzzy} {Logic} {Controller} and its {Applications}},
	volume = {3},
	issn = {2395-6011},
	url = {https://typeset.io/papers/an-introduction-to-fuzzy-logic-controller-and-its-52hhuungdc},
	doi = {10.32628/IJSRST173787},
	abstract = {This paper presents the nature of fuzziness and how the fuzzy operations are performed and how fuzzy rules can incorporate the underlying knowledge to develop a fuzzy logic controller or simply a fuzzy controller. Fuzzy logic is a way to make machines more intelligent to deal with uncertain, imprecise or qualitative decision making problems like humans. This paper also provides some applications of fuzzy controller in a simple and easy to understand manner.},
	language = {en},
	number = {7},
	urldate = {2024-10-14},
	journal = {International Journal of Scientific Research in Science and Technology},
	author = {Rehman, Tohida},
	month = oct,
	year = {2017},
	pages = {540--545},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\MR7GVN4L\\2017 - An Introduction to Fuzzy Logic Controller and its .pdf:application/pdf},
}

@incollection{mahmoud_basics_2018,
	address = {Cham},
	title = {Basics of {Fuzzy} {Control}},
	isbn = {978-3-319-54954-5},
	url = {https://doi.org/10.1007/978-3-319-54954-5_2},
	abstract = {This chapter investigates the problem of fuzzy control design methods. We keep in mind the simple statement, ‘Fuzzy logic’ means approximate reasoning, information granulation, computing with words, and so on. First we provide a tutorial introduction to the ingredients of fuzzy logic-based systems. We show the properties and features of classical versus fuzzy sets and demonstrate the process of fuzzification by several examples. Next we consider fuzzy rule-based systems and pay particular attention to comparative analysis with conventional approaches. After illustrating the different methods of defuzzification, we shed lights on fuzzy inference systems of Mamdani, Tsukamoto, and Sugeno. Finally, we show how to deal with a variety of basic fuzzy control systems.},
	language = {en},
	urldate = {2024-10-14},
	booktitle = {Fuzzy {Control}, {Estimation} and {Diagnosis}: {Single} and {Interconnected} {Systems}},
	publisher = {Springer International Publishing},
	author = {Mahmoud, Magdi S.},
	editor = {Mahmoud, Magdi S.},
	year = {2018},
	doi = {10.1007/978-3-319-54954-5_2},
	pages = {15--44},
}

@book{ozcan_analysis_1998,
	title = {Analysis of a {Simple} {Particle} {Swarm} {Optimization} {System}},
	volume = {1998},
	abstract = {Particle Swarm Optimization (PSO) is a recently proposed approach, based on social behavior of organisms such as birds and fishes (Kennedy and Eberhart, 1995). Search is conducted by "flying" particles in the space. In research results reported so far, the trajectories of even a simple particle swarm optimization system have not been formally obtained. This paper provides the trajectory equations for various parameter values in closed form.},
	author = {Özcan, Ender and Mohan, Chilukuri},
	month = jan,
	year = {1998},
	note = {Journal Abbreviation: Intelligent Engineering Systems Through Artificial Neural Networks
Publication Title: Intelligent Engineering Systems Through Artificial Neural Networks},
}

@inproceedings{shi_parameter_1998,
	address = {Berlin, Heidelberg},
	title = {Parameter selection in particle swarm optimization},
	isbn = {978-3-540-68515-9},
	doi = {10.1007/BFb0040810},
	abstract = {This paper first analyzes the impact that inertia weight and maximum velocity have on the performance of the particle swarm optimizer, and then provides guidelines for selecting these two parameters. Analysis of experiments demonstrates the validity of these guidelines.},
	language = {en},
	booktitle = {Evolutionary {Programming} {VII}},
	publisher = {Springer},
	author = {Shi, Yuhui and Eberhart, Russell C.},
	editor = {Porto, V. W. and Saravanan, N. and Waagen, D. and Eiben, A. E.},
	year = {1998},
	pages = {591--600},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\NSUFDL9U\\Shi und Eberhart - 1998 - Parameter selection in particle swarm optimization.pdf:application/pdf},
}

@inproceedings{angeline_evolutionary_1998,
	address = {Berlin, Heidelberg},
	title = {Evolutionary optimization versus particle swarm optimization: {Philosophy} and performance differences},
	isbn = {978-3-540-68515-9},
	shorttitle = {Evolutionary optimization versus particle swarm optimization},
	doi = {10.1007/BFb0040811},
	abstract = {This paper investigates the philosophical and performance differences of particle swarm and evolutionary optimization. The method of processing employed in each technique are first reviewed followed by a summary of their philosophical differences. Comparison experiments involving four non-linear functions well studied in the evolutionary optimization literature are used to highlight some performance differences between the techniques.},
	language = {en},
	booktitle = {Evolutionary {Programming} {VII}},
	publisher = {Springer},
	author = {Angeline, Peter J.},
	editor = {Porto, V. W. and Saravanan, N. and Waagen, D. and Eiben, A. E.},
	year = {1998},
	keywords = {Evolutionary Computation, Evolutionary Optimization, Particle Swarm, Particle Swarm Optimization, Strategy Parameter},
	pages = {601--610},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\UF4IALHJ\\Angeline - 1998 - Evolutionary optimization versus particle swarm op.pdf:application/pdf},
}

@inproceedings{kennedy_discrete_1997,
	title = {A discrete binary version of the particle swarm algorithm},
	volume = {5},
	url = {https://ieeexplore.ieee.org/abstract/document/637339},
	doi = {10.1109/ICSMC.1997.637339},
	abstract = {The particle swarm algorithm adjusts the trajectories of a population of "particles" through a problem space on the basis of information about each particle's previous best performance and the best previous performance of its neighbors. Previous versions of the particle swarm have operated in continuous space, where trajectories are defined as changes in position on some number of dimensions. The paper reports a reworking of the algorithm to operate on discrete binary variables. In the binary version, trajectories are changes in the probability that a coordinate will take on a zero or one value. Examples, applications, and issues are discussed.},
	urldate = {2024-10-14},
	booktitle = {Computational {Cybernetics} and {Simulation} 1997 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics}},
	author = {Kennedy, J. and Eberhart, R.C.},
	month = oct,
	year = {1997},
	note = {ISSN: 1062-922X},
	keywords = {Space technology, Stochastic processes, Robustness, Particle swarm optimization, Hypercubes, Random number generation, Routing, State-space methods, Statistics},
	pages = {4104--4108 vol.5},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\75JGJB67\\Kennedy und Eberhart - 1997 - A discrete binary version of the particle swarm al.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\PMP4BBSE\\637339.html:text/html},
}

@inproceedings{eberhart_new_1995,
	title = {A new optimizer using particle swarm theory},
	url = {https://ieeexplore.ieee.org/abstract/document/494215},
	doi = {10.1109/MHS.1995.494215},
	abstract = {The optimization of nonlinear functions using particle swarm methodology is described. Implementations of two paradigms are discussed and compared, including a recently developed locally oriented paradigm. Benchmark testing of both paradigms is described, and applications, including neural network training and robot task learning, are proposed. Relationships between particle swarm optimization and both artificial life and evolutionary computation are reviewed.},
	urldate = {2024-10-14},
	booktitle = {{MHS}'95. {Proceedings} of the {Sixth} {International} {Symposium} on {Micro} {Machine} and {Human} {Science}},
	author = {Eberhart, R. and Kennedy, J.},
	month = oct,
	year = {1995},
	keywords = {Testing, Artificial neural networks, Acceleration, Evolutionary computation, Particle swarm optimization, Statistics, Genetic algorithms, Optimization methods, Particle tracking, Performance evaluation},
	pages = {39--43},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\XNBNH9F9\\Eberhart und Kennedy - 1995 - A new optimizer using particle swarm theory.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\SS5W253R\\494215.html:text/html},
}

@article{varghese_optimal_2017,
	title = {Optimal control of inverted pendulum system using {PID} controller, {LQR} and {MPC}},
	volume = {263},
	issn = {1757-899X},
	url = {https://dx.doi.org/10.1088/1757-899X/263/5/052007},
	doi = {10.1088/1757-899X/263/5/052007},
	abstract = {Inverted pendulum is a highly nonlinear system. Here we propose an optimal control technique for the control of an inverted Pendulum - cart system. The system is modeled, linearized and controlled. Here, the control objective is to control the system such that when the cart reaches a desired position the inverted pendulum stabilizes in the upright position. Initially PID controller is used to control the system. Later, Linear Quadratic Regulator (LQR) a well-known optimal control technique which makes use of the states of the dynamical system and control input to frame the optimal control decision is used. Various combinations of both PID and LQR controllers are implemented. To validate the robustness of the controller, the system is simulated with and without disturbance. Finally the system is also controlled using Model Predictive controller (MPC). MPC has well predictive ability to calculate future events and implement necessary control actions. The performance of the system is compared and analyzed.},
	language = {en},
	number = {5},
	urldate = {2024-10-14},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Varghese, Elisa Sara and Vincent, Anju K. and Bagyaveereswaran, V.},
	month = nov,
	year = {2017},
	note = {Publisher: IOP Publishing},
	pages = {052007},
	file = {IOP Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\IDPKT8FK\\Varghese et al. - 2017 - Optimal control of inverted pendulum system using .pdf:application/pdf},
}

@inproceedings{prasad_optimal_2011,
	title = {Optimal control of nonlinear inverted pendulum dynamical system with disturbance input using {PID} controller \& {LQR}},
	url = {https://ieeexplore.ieee.org/abstract/document/6190585},
	doi = {10.1109/ICCSCE.2011.6190585},
	abstract = {Optimal response of the controlled dynamical systems is desired hence for that is the optimal control. Linear quadratic regulator (LQR), an optimal control method, and PID control which are generally used for control of the linear dynamical systems have been used in this paper to control the nonlinear dynamical system. The inverted pendulum, a highly nonlinear unstable system is used as a benchmark for implementing the control methods. In this paper the modeling and control design of nonlinear inverted pendulum-cart dynamic system with disturbance input using PID control \& LQR have been presented. The nonlinear system states are fed to LQR which is designed using linear state-space model. Here PID \& LQR control methods have been implemented to control the cart position and stabilize the inverted pendulum in vertically upright position. The MATLAB-SIMULINK models have been developed for simulation of the control schemes. The simulation results justify the comparative advantages of LQR control methods.},
	urldate = {2024-10-14},
	booktitle = {2011 {IEEE} {International} {Conference} on {Control} {System}, {Computing} and {Engineering}},
	author = {Prasad, Lal Bahadur and Tyagi, Barjeev and Gupta, Hari Om},
	month = nov,
	year = {2011},
	keywords = {Simulation, Inverted pendulum, Optimal control, PID control, Equations, Mathematical model, Force, LQR, disturbance input, Nonlinear dynamical systems, nonlinear system, optimal control},
	pages = {540--545},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\BCDN4QWS\\Prasad et al. - 2011 - Optimal control of nonlinear inverted pendulum dyn.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\2NRGZIXK\\6190585.html:text/html},
}

@article{lim_stabilising_2018,
	title = {Stabilising an {Inverted} {Pendulum} with {PID} {Controller}},
	volume = {152},
	issn = {2261-236X},
	url = {http://www.scopus.com/inward/record.url?scp=85042922366&partnerID=8YFLogxK},
	doi = {10.1051/matecconf/201815202009},
	abstract = {Inverted pendulum is a system in which the centre of the mass is above the pivot point, where the mass can freely rotate. The inverted pendulum has a unique trait; it is unpredictable, non-linear and consists of multiple variables. Balancing by PID controller is a continuous process where it corrects the feedback system error from the difference between the measured value and the desired value. This research mainly focusses on balancing an inverted pendulum with reaction wheel. The research objectives are to construct a self-balanced inverted pendulum and using PID controller to control the stability of the pendulum. The PID configuration is then evaluated based on the response of the system. The idea is to use the reaction torque generated by the motor to counter balance the inverted pendulum. The factor which governs the amount of torque generated is the height of the pendulum and the mass of the wheel. To balance the pendulum, tuning the PID gain is essential. Proportional gain is tuned first to get oscillation, next is to tune the integral and derivative gain to get a smoother and quicker response. Idea is to get short settling time, and minimum overshoot percentage. Hypothesis is that higher proportional gain will give a faster response rate and the acceleration of the motor is the key on generating torque. A simulation of the pendulum falling is simulated and the results are recorded in term of the response of the pendulum against time. At initial point, proportional gain, integral gain and derivative gain are set to zero to validate the simulation. The finding in this research is that torque is generated by the acceleration of the reaction wheel. Higher acceleration gives a high torque. Others findings is the PID parameter; Proportional gain increases the response rate; Integral gain is used to eliminate steady state error; Derivative gain is used to lessen the overshoot.},
	number = {02009},
	urldate = {2024-10-14},
	journal = {MATEC Web of Conferences},
	author = {Lim, Yon Yaw and Hoo, Choon Lih and Felicia Wong, Yen Myan},
	month = feb,
	year = {2018},
	file = {Volltext:C\:\\Users\\Guest_\\Zotero\\storage\\CYM6TJSD\\Lim et al. - 2018 - Stabilising an Inverted Pendulum with PID Controll.pdf:application/pdf},
}

@article{vinodh_kumar_robust_2013,
	series = {International {Conference} on {Design} and {Manufacturing} ({IConDM2013})},
	title = {Robust {LQR} {Controller} {Design} for {Stabilizing} and {Trajectory} {Tracking} of {Inverted} {Pendulum}},
	volume = {64},
	issn = {1877-7058},
	url = {https://www.sciencedirect.com/science/article/pii/S1877705813016020},
	doi = {10.1016/j.proeng.2013.09.088},
	abstract = {This paper describes the method for stabilizing and trajectory tracking of Self Erecting Single Inverted Pendulum (SESIP) using Linear Quadratic Regulator (LQR). A robust LQR is proposed in this paper not only to stabilize the pendulum in upright position but also to make the cart system to track the given reference signal even in the presence of disturbance. The control scheme of pendulum system consists of two controllers such as swing up controller and stabilizing controller. The main focus of this work is on the design of stabilizing controller which can accommodate the disturbance present in the system in the form of wind force. An optimal LQR controller with well tuned weighting matrices is implemented to stabilize the pendulum in the vertical position. The steady state and dynamic characteristics of the proposed controller are investigated by conducting experiments on benchmark linear inverted pendulum system. Experimental results prove that the proposed LQR controller can guarantee the inverted pendulum a faster and smoother stabilizing process with less oscillation and better robustness than a Full State Feedback (FSF) controller by pole placement approach.},
	urldate = {2024-10-14},
	journal = {Procedia Engineering},
	author = {Vinodh Kumar, E. and Jerome, Jovitha},
	month = jan,
	year = {2013},
	keywords = {Inverted Pendulum, Full State Feedback Controller, LQR Controller, Pole placement approach, PV Controller, Riccatti Equation},
	pages = {169--178},
	file = {ScienceDirect Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\EMH36S7B\\S1877705813016020.html:text/html;Vinodh Kumar und Jerome - 2013 - Robust LQR Controller Design for Stabilizing and T.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\EAEDDFAD\\Vinodh Kumar und Jerome - 2013 - Robust LQR Controller Design for Stabilizing and T.pdf:application/pdf},
}

@article{marsden_discrete_2001,
	title = {Discrete mechanics and variational integrators},
	volume = {10},
	issn = {1474-0508, 0962-4929},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/discrete-mechanics-and-variational-integrators/C8F45478A9290DEC24E63BB7FBE3CEB5},
	doi = {10.1017/S096249290100006X},
	abstract = {This paper gives a review of integration algorithms for finite dimensional 
mechanical systems that are based on discrete variational principles. The 
variational technique gives a unified treatment of many symplectic schemes, 
including those of higher order, as well as a natural treatment of the discrete 
Noether theorem. The approach also allows us to include forces, dissipation 
and constraints in a natural way. Amongst the many specific schemes treated 
as examples, the Verlet, SHAKE, RATTLE, Newmark, and the symplectic 
partitioned Runge–Kutta schemes are presented.},
	language = {en},
	urldate = {2024-10-14},
	journal = {Acta Numerica},
	author = {Marsden, J. E. and West, M.},
	month = may,
	year = {2001},
	pages = {357--514},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\W3A5S7AN\\Marsden und West - 2001 - Discrete mechanics and variational integrators.pdf:application/pdf},
}

@book{edwards_sliding_1998,
	address = {London},
	title = {Sliding {Mode} {Control}: {Theory} {And} {Applications}},
	isbn = {978-0-429-07593-3},
	shorttitle = {Sliding {Mode} {Control}},
	abstract = {In the formation of any control problem there will be discrepancies between the actual plant and the mathematical model for controller design. Sliding mode control theory seeks to produce controllers to over some such mismatches. This text provides the reader with a grounding in sliding mode control and is appropriate for the graduate with a basic knowledge of classical control theory and some knowledge of state-space methods.},
	publisher = {CRC Press},
	author = {Edwards, Christopher and Spurgeon, Sarah K.},
	month = aug,
	year = {1998},
	doi = {10.1201/9781498701822},
}

@article{chalanga_output_2019,
	title = {Output regulation using new sliding surface with an implementation on inverted pendulum system},
	volume = {45},
	issn = {0947-3580},
	url = {https://www.sciencedirect.com/science/article/pii/S0947358018301249},
	doi = {10.1016/j.ejcon.2018.09.011},
	abstract = {In this paper constant reference output tracking is achieved using second order sliding mode (SOSM) control. To achieve tracking a new sliding surface is proposed and to ensure sliding motion super-twisting control (STC) is employed. The proposed sliding surface is more general, and it can be used for constant reference tracking in both minimum phase and non-minimum phase systems. The proposed method is validated on the unstable non-minimum phase inverted pendulum system and also the results are compared with the first order sliding mode control (SMC) in simulation and experimentally both.},
	urldate = {2024-10-15},
	journal = {European Journal of Control},
	author = {Chalanga, Asif and Patil, Machhindranath and Bandyopadhyay, Bijnan and Arya, Hemendra},
	month = jan,
	year = {2019},
	keywords = {Non-minimum phase system, Output regulation, Second order sliding mode, Super-twisting control},
	pages = {85--91},
	file = {Eingereichte Version:C\:\\Users\\Guest_\\Zotero\\storage\\9LXLPF3J\\Chalanga et al. - 2019 - Output regulation using new sliding surface with a.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\JDFRRWKW\\S0947358018301249.html:text/html},
}

@article{maarif_sliding_2022,
	title = {Sliding {Mode} {Control} {Design} for {Magnetic} {Levitation} {System}},
	volume = {3},
	copyright = {Copyright (c) 2023 Alfian Ma'arif},
	issn = {2715-5072},
	url = {https://journal.umy.ac.id/index.php/jrc/article/view/12389},
	doi = {10.18196/jrc.v3i6.12389},
	abstract = {This paper presents a control system design for a magnetic levitation system (Maglev) or MLS using sliding mode control (SMC). The MLS problem of levitating the object in the air will be solved using the controller. Inductors used in MLS make the system have nonlinear characteristics. Thus, a nonlinear controller is the most suitable control design for MLS. SMC is one of the nonlinear controllers with good robustness and can handle any model mismatch. Based on simulation results with a step as input reference, MLS provided good system performances: 0.0991s rise time, 0.1712s settling time, and 0.0159 overshoot. Moreover, a prominent tracking control for sine wave reference was also shown. Although the augmented system had a chattering effect on the control signal, the chattering control signal did not affect MLS performances.},
	language = {en},
	number = {6},
	urldate = {2024-10-15},
	journal = {Journal of Robotics and Control (JRC)},
	author = {Ma'arif, Alfian and Vera, Marco Antonio Marquez and Mahmoud, Magdi Sadek and Umoh, Edwin and Abougarair, Ahmed Jaber and Rahmadhia, Safinta Nurindra},
	year = {2022},
	note = {Number: 6},
	keywords = {Magnetic levitation system, Nonlinear control, Nonlinear system, Sliding mode control},
	pages = {848--853},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\Q5Y42RWY\\Ma'arif et al. - 2022 - Sliding Mode Control Design for Magnetic Levitatio.pdf:application/pdf},
}

@article{uswarman_robust_2019,
	title = {Robust {Control} of a {Quadcopter} {Flying} via {Sliding} {Mode}},
	volume = {2},
	copyright = {Copyright (c)},
	issn = {2581-0545},
	url = {https://journal.itera.ac.id/index.php/jsat/article/view/168},
	doi = {10.35472/281446},
	abstract = {Sliding Mode Control (SMC) used to control the stability of a quadcopter from disturbances and uncertainties. This technique has two main advantages: the nonlinear dynamics and modelling errors of the quadcopter can be eliminated by switching function and the uncertainty problem can be overcome with a closed-loop response. The controller of the sliding mode technique consists of two components. The first is design equivalent control law to maintain the system state trajectory on the sliding surface. The second is design switching control law to reach the sliding surface. The Lyapunov theorem is used to ensure the stability of the system. Simulation results verify the robustness of the controller.},
	language = {en},
	number = {1},
	urldate = {2024-10-15},
	journal = {Journal of Science and Applicative Technology},
	author = {Uswarman, Rudi and Istiqphara, Swadexi and Yunmar, Rajif Agung and Rakhman, Arkham Zahri},
	month = may,
	year = {2019},
	note = {Number: 1},
	pages = {135--143},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\LASI5ESY\\Uswarman et al. - 2019 - Robust Control of a Quadcopter Flying via Sliding .pdf:application/pdf},
}

@article{mahmoud_optimizing_2021,
	title = {Optimizing the {Parameters} of {Sliding} {Mode} {Controllers} for {Stepper} {Motor} through {Simulink} {Response} {Optimizer} {Application}},
	volume = {1},
	copyright = {Copyright (c) 2021 Magdi Sadek Mahmoud},
	issn = {2775-2658},
	url = {https://pubs2.ascee.org/index.php/IJRCS/article/view/345},
	doi = {10.31763/ijrcs.v1i2.345},
	abstract = {This paper will focus on optimizing parameters of sliding mode controllers (SMC) for hybrid stepper motor models simulated in Matlab/Simulink. The main objective is to achieve a smooth transient and robust, steady-state to track reference rotor position when the stepper motor is subjected to load disturbances. Two different structures of SMC controllers will be studied, which are based on the flat system concept that is applicable to the stepper motor model. The hassle to determine controller parameters will be optimized using the Simulink Response Optimizer application.  The performance of the controllers will be evaluated by considering load torque and variation in the model parameters. Although the results showed that an open-loop controller could move the rotor to the desired position, however, the transient response had undesired oscillations before the output settled at the steady state. The response was improved by optimizing SMC controllers’ parameters to meet the desire step response requirement. Despite both SMC methods have successfully tracked the reference, there are some challenges to deal with each method in regard to the state measurements, the number of optimized controllers’ parameters, and the scattering of control inputs.},
	language = {en},
	number = {2},
	urldate = {2024-10-15},
	journal = {International Journal of Robotics and Control Systems},
	author = {Mahmoud, Magdi Sadek and AlRamadhan, Ali H.},
	month = jul,
	year = {2021},
	note = {Number: 2},
	keywords = {Flat Output, Input-Output, Parameters Optimizations, Sliding Mode Controller (SMC), State Feedback, Stepper Motor},
	pages = {209--225},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\97T8FKLW\\Mahmoud und AlRamadhan - 2021 - Optimizing the Parameters of Sliding Mode Controll.pdf:application/pdf},
}

@article{valluru_stabilization_2017,
	title = {Stabilization of nonlinear inverted pendulum system using {MOGA} and {APSO} tuned nonlinear {PID} controller},
	volume = {4},
	issn = {null},
	url = {https://doi.org/10.1080/23311916.2017.1357314},
	doi = {10.1080/23311916.2017.1357314},
	abstract = {An inverted pendulum system (IPS) is a highly nonlinear dynamical open loop unstable system, typically used as a benchmark to verify the performance of controllers. The IPS emulates the behaviour of an altitude control of a space booster or rocket on take-off. The problem is to develop suitable controllers to maintain the stabilization and swing up of an inverted pendulum on a cart. This paper presents the evolutionary tuning methods of nonlinear PID (NL-PID) controller for IPS with the multi-objective genetic algorithm (MOGA) and adaptive particle swarm optimization (APSO) algorithm. The function of NL-PID controllers is to keep the pendulum in an upright position by maintaining the pendulum at same state and angle at zero degrees. The comparison of responses and performance of MOGA tuned NL-PID and APSO tuned NL-PID controllers for an IPS are described. The mathematical modeling and simulation analysis of the IPS is presented in detail to test the effectiveness of controller tuning algorithm. The APSO based tuning of the NL-PID controller has lesser chattering, noise and fast settling time than MOGA based tuning of the controller.},
	number = {1},
	urldate = {2024-10-15},
	journal = {Cogent Engineering},
	author = {Valluru, Sudarshan K. and Singh, Madhusudan},
	editor = {Chadli, Mohammed},
	month = jan,
	year = {2017},
	note = {Publisher: Cogent OA
\_eprint: https://doi.org/10.1080/23311916.2017.1357314},
	keywords = {adaptive particle swarm optimization algorithm, inverted pendulum system, multi-objective genetic algorithm, NL-PID controller, tuning},
	pages = {1357314},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\5BJBV2ZN\\Valluru und Singh - 2017 - Stabilization of nonlinear inverted pendulum syste.pdf:application/pdf},
}
