
@misc{opencv_team_opencv-python_2023,
	title = {opencv-python: {Wrapper} package for {OpenCV} python bindings.},
	copyright = {Apache Software License},
	shorttitle = {opencv-python},
	url = {https://github.com/opencv/opencv-python},
	urldate = {2024-04-11},
	author = {OpenCV Team},
	month = dec,
	year = {2023},
	keywords = {Scientific/Engineering, Software Development, Scientific/Engineering - Image Recognition},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\ETF42V9M\\opencv-python.html:text/html},
}

@misc{huaman_opencv_nodate,
	title = {{OpenCV}: {Image} {Moments}},
	url = {https://docs.opencv.org/4.x/d0/d49/tutorial_moments.html},
	urldate = {2024-04-11},
	author = {Huamán, Ana},
	file = {OpenCV\: Image Moments:C\:\\Users\\Guest_\\Zotero\\storage\\QSFDTJEF\\tutorial_moments.html:text/html},
}

@misc{newton_shape_2023,
	title = {Shape {Based} {Object} {Tracking} with {OpenCV} on {Raspberry} {Pi}},
	url = {https://how2electronics.com/shape-based-object-tracking-with-opencv-on-raspberry-pi/},
	abstract = {Overview: This project is about Shape Based Object Detection \& Tracking with OpenCV on Raspberry Pi 4 Computer. In the world of computer vision,},
	language = {en-US},
	urldate = {2024-04-11},
	journal = {How To Electronics},
	author = {Newton, Alex},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\AR4MKA6E\\shape-based-object-tracking-with-opencv-on-raspberry-pi.html:text/html},
}

@misc{noauthor_hsl_2024,
	title = {{HSL} and {HSV}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=HSL_and_HSV&oldid=1218017971},
	abstract = {HSL and HSV are the two most common cylindrical-coordinate representations of points in an RGB color model.  The two representations rearrange the geometry of RGB in an attempt to be more intuitive and perceptually relevant than the cartesian (cube) representation.  Developed in the 1970s for computer graphics applications, HSL and HSV are used today in color pickers, in image editing software, and less commonly in image analysis and computer vision.
HSL stands for hue, saturation, and lightness, and is often also called HLS. HSV stands for hue, saturation, and value, and is also often called HSB (B for brightness). A third model, common in computer vision applications, is HSI, for hue, saturation, and intensity. However, while typically consistent, these definitions are not standardized, and any of these abbreviations might be used for any of these three or several other related cylindrical models. (For technical definitions of these terms, see below.)
In each cylinder, the angle around the central vertical axis corresponds to "hue", the distance from the axis corresponds to "saturation", and the distance along the axis corresponds to "lightness", "value" or "brightness". Note that while "hue" in HSL and HSV refers to the same attribute, their definitions of "saturation" differ dramatically. Because HSL and HSV are simple transformations of device-dependent RGB models, the physical colors they define depend on the colors of the red, green, and blue primaries of the device or of the particular RGB space, and on the gamma correction used to represent the amounts of those primaries. Each unique RGB device therefore has unique HSL and HSV spaces to accompany it, and numerical HSL or HSV values describe a different color for each basis RGB space.Both of these representations are used widely in computer graphics, and one or the other of them is often more convenient than RGB, but both are also criticized for not adequately separating color-making attributes, or for their lack of perceptual uniformity. Other more computationally intensive models, such as CIELAB or CIECAM02 are said to better achieve these goals.},
	language = {en},
	urldate = {2024-04-11},
	journal = {Wikipedia},
	month = apr,
	year = {2024},
	note = {Page Version ID: 1218017971},
}

@misc{arducam_picamera2_nodate,
	title = {Picamera2 - {Arducam} {Wiki}},
	url = {https://docs.arducam.com/Raspberry-Pi-Camera/Native-camera/PiCamera2-User-Guide/},
	urldate = {2024-04-11},
	author = {Arducam},
	file = {Picamera2 - Arducam Wiki:C\:\\Users\\Guest_\\Zotero\\storage\\QZA873KZ\\PiCamera2-User-Guide.html:text/html},
}

@misc{lotfi_serial_2017,
	type = {Forum},
	title = {Serial {Port} {Communication} over the {USB} in {Raspberry} {Pi} 3 - {Raspberry} {Pi} {Forums}},
	url = {https://forums.raspberrypi.com/viewtopic.php?t=195963},
	urldate = {2024-04-11},
	author = {Lotfi},
	month = oct,
	year = {2017},
}

@misc{the_robotics_back-end_raspberry_2019,
	title = {Raspberry {Pi} {Arduino} {Serial} {Communication} - {Everything} {You} {Need} {To} {Know}},
	url = {https://roboticsbackend.com/raspberry-pi-arduino-serial-communication/},
	abstract = {Raspberry Pi Arduino Serial communication - with complete Python code example. Learn how to connect your boards together, setup software, and write code.},
	language = {en-US},
	urldate = {2024-04-11},
	journal = {The Robotics Back-End},
	author = {The Robotics Back-End},
	month = nov,
	year = {2019},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\364QJ87E\\raspberry-pi-arduino-serial-communication.html:text/html},
}

@misc{openai_openaigym_2023,
	title = {openai/gym},
	copyright = {MIT License},
	url = {https://github.com/openai/gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms.},
	urldate = {2024-04-11},
	publisher = {OpenAI},
	author = {OpenAI},
	month = jan,
	year = {2023},
	note = {original-date: 2016-04-27T14:59:16Z},
}

@misc{andriy_answer_2011,
	title = {Answer to "fastest (low latency) method for {Inter} {Process} {Communication} between {Java} and {C}/{C}++"},
	url = {https://stackoverflow.com/a/6412333},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Andriy},
	month = jun,
	year = {2011},
}

@misc{vsekhar_answer_2011,
	title = {Answer to "{Interprocess} communication in {Python}"},
	url = {https://stackoverflow.com/a/6921402},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {vsekhar},
	month = aug,
	year = {2011},
}

@misc{kelling_answer_2011,
	title = {Answer to "{Interprocess} communication in {Python}"},
	url = {https://stackoverflow.com/a/6921340},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Kelling, Zach},
	month = aug,
	year = {2011},
}

@misc{basj_answer_2020,
	title = {Answer to "{Interprocess} communication in {Python}"},
	url = {https://stackoverflow.com/a/61771563},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Basj},
	month = may,
	year = {2020},
}

@misc{westphal_answer_2014,
	title = {Answer to "{Lazy} pub/sub in zeromq, only get last message"},
	url = {https://stackoverflow.com/a/26383697},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {Westphal, Mathieu},
	month = oct,
	year = {2014},
}

@misc{rmunn_answer_2016,
	title = {Answer to "'{Last} message only' option in {ZMQ} {Subscribe} socket"},
	url = {https://stackoverflow.com/a/38257854},
	urldate = {2024-04-11},
	journal = {Stack Overflow},
	author = {rmunn},
	month = jul,
	year = {2016},
}

@misc{pilcolearner_cart-pole_2011,
	title = {Cart-{Pole} {Swing}-up},
	url = {https://www.youtube.com/watch?v=XiigTGKZfks},
	abstract = {Learning to control a real cart-pole system from scratch in only 7 trials. The whole learning process is shown. The learning progress can easily be seen after each trial.},
	urldate = {2024-04-11},
	author = {{PilcoLearner}},
	month = may,
	year = {2011},
}

@article{deisenroth_pilco_nodate,
	title = {{PILCO}: {A} {Model}-{Based} and {Data}-{Efficient} {Approach} to {Policy} {Search}},
	abstract = {In this paper, we introduce pilco, a practical, data-eﬃcient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning eﬃciency on challenging and high-dimensional control tasks.},
	language = {en},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	file = {Deisenroth und Rasmussen - PILCO A Model-Based and Data-Efficient Approach t.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\88IP78HP\\Deisenroth und Rasmussen - PILCO A Model-Based and Data-Efficient Approach t.pdf:application/pdf},
}

@article{gal_improving_nodate,
	title = {Improving {PILCO} with {Bayesian} {Neural} {Network} {Dynamics} {Models}},
	abstract = {Model-based reinforcement learning (RL) allows an agent to discover good policies with a small number of trials by generalising observed transitions. Data efﬁciency can be further improved with a probabilistic model of the agent’s ignorance about the world, allowing it to choose actions under uncertainty. Bayesian modelling offers tools for this task, with PILCO [1] being a prominent example, achieving state-of-theart data efﬁciency on low dimensional RL benchmarks. But PILCO relies on Gaussian processes (GPs), which prohibits its applicability to problems that require a larger number of trials to be solved. Further, PILCO does not consider temporal correlation in model uncertainty between successive state transitions, which results in PILCO underestimating state uncertainty at future time steps [2]. In this paper we extend PILCO’s framework to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality. Using particle methods we sample dynamics function realisations, and obtain lower cumulative cost than PILCO. We give insights into the modelling assumptions made in PILCO, and show that moment matching is a crucial simplifying assumption made by the model. Our implementation can leverage GPU architectures, offering faster running time than PILCO, and will allow structured observation spaces to be modelled (images or higher dimensional inputs) in the future.},
	language = {en},
	author = {Gal, Yarin and McAllister, Rowan Thomas and Rasmussen, Carl Edward},
	file = {Gal et al. - Improving PILCO with Bayesian Neural Network Dynam.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\TT7FCIN4\\Gal et al. - Improving PILCO with Bayesian Neural Network Dynam.pdf:application/pdf},
}

@misc{raffin_stable-baselines3_2024,
	title = {stable-baselines3: {Pytorch} version of {Stable} {Baselines}, implementations of reinforcement learning algorithms.},
	copyright = {MIT},
	shorttitle = {stable-baselines3},
	url = {https://github.com/DLR-RM/stable-baselines3},
	urldate = {2024-04-11},
	author = {Raffin, Antonin},
	month = mar,
	year = {2024},
	keywords = {baselines,, data-science, gym,, gymnasium,, machine-learning,, openai,, python,, reinforcement-learning-algorithms,, reinforcement-learning,, stable,, toolbox,},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\MCXY8GGU\\stable-baselines3.html:text/html},
}

@misc{nikhilaggarwal3_queue_2019,
	title = {Queue in {Python}},
	url = {https://www.geeksforgeeks.org/queue-in-python/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-US},
	urldate = {2024-04-16},
	journal = {GeeksforGeeks},
	author = {nikhilaggarwal3},
	month = oct,
	year = {2019},
	note = {Section: Python},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\K3HXUSM5\\queue-in-python.html:text/html},
}

@misc{tensorboard_team_tensorflowtensorboard_2024,
	title = {tensorflow/tensorboard},
	copyright = {Apache-2.0},
	url = {https://github.com/tensorflow/tensorboard},
	abstract = {TensorFlow's Visualization Toolkit},
	urldate = {2024-04-16},
	publisher = {tensorflow},
	author = {Tensorboard Team},
	month = apr,
	year = {2024},
	note = {original-date: 2017-05-15T20:08:07Z},
}

@article{manrique_escobar_parametric_2020,
	title = {A {Parametric} {Study} of a {Deep} {Reinforcement} {Learning} {Control} {System} {Applied} to the {Swing}-{Up} {Problem} of the {Cart}-{Pole}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/24/9013},
	doi = {10.3390/app10249013},
	abstract = {In this investigation, the nonlinear swing-up problem associated with the cart-pole system modeled as a multibody dynamical system is solved by developing a deep Reinforcement Learning (RL) controller. Furthermore, the sensitivity analysis of the deep RL controller applied to the cart-pole swing-up problem is carried out. To this end, the influence of modifying the physical properties of the system and the presence of dry friction forces are analyzed employing the cumulative reward during the task. Extreme limits for the modifications of the parameters are determined to prove that the neural network architecture employed in this work features enough learning capability to handle the task under modifications as high as 90\% on the pendulum mass, as well as a 100\% increment on the cart mass. As expected, the presence of dry friction greatly affects the performance of the controller. However, a post-training of the agent in the modified environment takes only thirty-nine episodes to find the optimal control policy, resulting in a promising path for further developments of robust controllers.},
	language = {en},
	number = {24},
	urldate = {2024-04-17},
	journal = {Applied Sciences},
	author = {Manrique Escobar, Camilo Andrés and Pappalardo, Carmine Maria and Guida, Domenico},
	month = jan,
	year = {2020},
	note = {Number: 24
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, artificial intelligence, cart-pole swing-up, deep reinforcement learning, dry friction, multibody system dynamics, nonlinear control, robustness},
	pages = {9013},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\6I7XCKQM\\Manrique Escobar et al. - 2020 - A Parametric Study of a Deep Reinforcement Learnin.pdf:application/pdf},
}

@article{doya_reinforcement_2000,
	title = {Reinforcement {Learning} in {Continuous} {Time} and {Space}},
	volume = {12},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976600300015961},
	doi = {10.1162/089976600300015961},
	abstract = {This article presents a reinforcement learning framework for continuous-time dynamical systems without a priori discretization of time, state, and action. Basedonthe Hamilton-Jacobi-Bellman (HJB) equation for infinite-horizon, discounted reward problems, we derive algorithms for estimating value functions and improving policies with the use of function approximators. The process of value function estimation is formulated as the minimization of a continuous-time form of the temporal difference (TD) error. Update methods based on backward Euler approximation and exponential eligibility traces are derived, and their correspondences with the conventional residual gradient, TD (0), and TD (λ) algorithms are shown. For policy improvement, two methods—a continuous actor-critic method and a value-gradient-based greedy policy—are formulated. As a special case of the latter, a nonlinear feedback control law using the value gradient and the model of the input gain is derived. The advantage updating, a model-free algorithm derived previously, is also formulated in the HJB-based framework.The performance of the proposed algorithms is first tested in a nonlinear control task of swinging a pendulum up with limited torque. It is shown in the simulations that (1) the task is accomplished by the continuous actor-critic method in a number of trials several times fewer than by the conventional discrete actor-critic method; (2) among the continuous policy update methods, the value-gradient-based policy with a known or learned dynamic model performs several times better than the actor-critic method; and (3) a value function update using exponential eligibility traces is more efficient and stable than that based on Euler approximation. The algorithms are then tested in a higher-dimensional task: cart-pole swing-up. This task is accomplished in several hundred trials using the value-gradient-based policy with a learned dynamic model.},
	number = {1},
	urldate = {2024-04-17},
	journal = {Neural Computation},
	author = {Doya, Kenji},
	month = jan,
	year = {2000},
	pages = {219--245},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\5DPK38RW\\Doya - 2000 - Reinforcement Learning in Continuous Time and Spac.pdf:application/pdf;Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\U2CTTD69\\Reinforcement-Learning-in-Continuous-Time-and.html:text/html},
}

@inproceedings{wawrzynski_model-free_2004,
	title = {Model-free off-policy reinforcement learning in continuous environment},
	volume = {2},
	url = {https://ieeexplore.ieee.org/abstract/document/1380086},
	doi = {10.1109/IJCNN.2004.1380086},
	abstract = {We introduce an algorithm of reinforcement learning in continuous state and action spaces. In order to construct a control policy, the algorithm utilizes the entire history of agent-environment interaction. The policy is a result of an estimation process based on all available information rather than the result of stochastic convergence as in classical reinforcement learning approaches. The policy is derived from the history directly, not through any kind of a model of the environment. We test our algorithm in the cart-pole swing-up simulated environment. The algorithm learns to control this plant in about 100 trials, which corresponds to 15 minutes of plant's real time. This is several times shorter than the one required by other algorithms.},
	urldate = {2024-04-17},
	booktitle = {2004 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks} ({IEEE} {Cat}. {No}.{04CH37541})},
	author = {Wawrzynski, P. and Pacut, A.},
	month = jul,
	year = {2004},
	note = {ISSN: 1098-7576},
	keywords = {Artificial intelligence, Control engineering computing, Convergence, Dynamic programming, History, Learning, Monte Carlo methods, Space technology, Stochastic processes, Testing},
	pages = {1091--1096 vol.2},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\D7CP4JH2\\1380086.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\9ZXKMPWR\\Wawrzynski und Pacut - 2004 - Model-free off-policy reinforcement learning in co.pdf:application/pdf},
}

@inproceedings{kimura_stochastic_1999,
	address = {Tokyo, Japan},
	title = {Stochastic real-valued reinforcement learning to solve a nonlinear control problem},
	volume = {5},
	isbn = {978-0-7803-5731-0},
	url = {http://ieeexplore.ieee.org/document/815604/},
	doi = {10.1109/ICSMC.1999.815604},
	abstract = {This paper presents a new approach to reinforcement learning (RL) to solve a non-linear control problem efficiently in which state and action spaces are continuous, Many DP-based reinforcement learning (RL) algorithms approximate the value function and give a greedy policy with respect to the learned value function. However, it is too expensive to fit highly accurate value functions, particularly in continuous state-action spaces. We provide a hierarchical RL algorithm composed of local linear controllers and TD-learning, which are both very simple. The continuous state space is discretized into an array of coarse boxes, and each box has its own local linear controller for choosing primitive continuous actions. The higher-level of the hierarchy accumulates state-values using tables with one entry for each box. Each linear controller improves the local control policy by using an actor-critic method. The algorithm was applied to a simulation of a cart-pole swing-up problem, and feasible solutions are found in less time than those of conventional discrete RL methods.},
	language = {en},
	urldate = {2024-04-17},
	booktitle = {{IEEE} {SMC}'99 {Conference} {Proceedings}. 1999 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({Cat}. {No}.{99CH37028})},
	publisher = {IEEE},
	author = {Kimura, H. and Kobayashi, S.},
	year = {1999},
	pages = {510--515},
	file = {Kimura und Kobayashi - 1999 - Stochastic real-valued reinforcement learning to s.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\66I7J378\\Kimura und Kobayashi - 1999 - Stochastic real-valued reinforcement learning to s.pdf:application/pdf},
}

@misc{igus_schrittmotoren_nodate,
	title = {Schrittmotoren},
	url = {https://docs.rs-online.com/17c8/0900766b81643255.pdf},
	urldate = {2024-04-23},
	author = {igus},
	file = {0900766b81643255.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\QSJQFLSK\\0900766b81643255.pdf:application/pdf},
}

@misc{stepperonline_nema_nodate,
	title = {Nema 17 {Bipolar} {59Ncm}(83.55oz.in) {2A} 42x48mm 4 {Wires} w/ 1m {Cable} \& {Connector} - {17HS19}-{2004S1}{\textbar}{STEPPERONLINE}},
	url = {https://www.omc-stepperonline.com/nema-17-bipolar-59ncm-84oz-in-2a-42x48mm-4-wires-w-1m-cable-connector-17hs19-2004s1},
	urldate = {2024-04-23},
	author = {stepperonline},
	file = {Nema 17 Bipolar 59Ncm(83.55oz.in) 2A 42x48mm 4 Wires w/ 1m Cable & Connector - 17HS19-2004S1|STEPPERONLINE:C\:\\Users\\Guest_\\Zotero\\storage\\SV9TW285\\nema-17-bipolar-59ncm-84oz-in-2a-42x48mm-4-wires-w-1m-cable-connector-17hs19-2004s1.html:text/html},
}

@misc{ma_eureka_2023,
	title = {Eureka: {Human}-{Level} {Reward} {Design} via {Coding} {Large} {Language} {Models}},
	shorttitle = {Eureka},
	url = {http://arxiv.org/abs/2310.12931},
	abstract = {Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex lowlevel manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present EUREKA, a human-level reward design algorithm powered by LLMs. EUREKA exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-theart LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, EUREKA generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, EUREKA outperforms human experts on 83\% of the tasks, leading to an average normalized improvement of 52\%. The generality of EUREKA also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using EUREKA rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.},
	language = {en},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12931 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Ma et al. - 2023 - Eureka Human-Level Reward Design via Coding Large.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\IWLWZP9A\\Ma et al. - 2023 - Eureka Human-Level Reward Design via Coding Large.pdf:application/pdf},
}

@inproceedings{mahmood_benchmarking_2018,
	title = {Benchmarking {Reinforcement} {Learning} {Algorithms} on {Real}-{World} {Robots}},
	url = {https://proceedings.mlr.press/v87/mahmood18a.html},
	abstract = {Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of {The} 2nd {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Mahmood, A. Rupam and Korenkevych, Dmytro and Vasan, Gautham and Ma, William and Bergstra, James},
	month = oct,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {561--591},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\7Z8MZRIR\\Mahmood et al. - 2018 - Benchmarking Reinforcement Learning Algorithms on .pdf:application/pdf},
}

@inproceedings{liu_swing-up_2023,
	address = {Singapore},
	title = {Swing-{Up} and {Balance} {Control} of {Cart}-{Pole} {Based} on {Reinforcement} {Learning} {DDPG}},
	isbn = {978-981-9915-49-1},
	doi = {10.1007/978-981-99-1549-1_33},
	abstract = {As a typical strong artificial intelligence method, reinforcement learning has been applied to real control tasks. Cart-pole system is an ideal controlled object, which is often used to verify the feasibility of control theory. In order to explore the effect of continuous reinforcement learning in the control of physical systems, this paper proposes a full control method for the swing up and stabilization of the cart-pole based on reinforcement learning DDPG. By interacting the cart-pole model with the main body of reinforcement learning, the Actor-Critic framework and the deterministic gradient algorithm DDPG are used to complete the learning of the cart-pole and realize the whole process of swing-up and balance control. Finally, the stability and effectiveness of reinforcement learning for the control of cart-pole are verified by simulation and experiment.},
	language = {en},
	booktitle = {Bio-{Inspired} {Computing}: {Theories} and {Applications}},
	publisher = {Springer Nature},
	author = {Liu, Jie and Zhuan, Xiangtao and Lu, Chuang},
	editor = {Pan, Linqiang and Zhao, Dongming and Li, Lianghao and Lin, Jianqing},
	year = {2023},
	keywords = {Cart-pole system, DDPG, Reinforcement learning},
	pages = {419--429},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\ZX5AVRQ9\\Liu et al. - 2023 - Swing-Up and Balance Control of Cart-Pole Based on.pdf:application/pdf},
}

@misc{pyzmq_team_zeromqpyzmq_2024,
	title = {zeromq/pyzmq},
	copyright = {BSD-3-Clause},
	url = {https://github.com/zeromq/pyzmq},
	abstract = {PyZMQ:  Python bindings for zeromq},
	urldate = {2024-05-09},
	publisher = {The ZeroMQ project},
	author = {pyzmq Team},
	month = may,
	year = {2024},
	note = {original-date: 2010-07-21T07:20:37Z},
	keywords = {cython, python, zeromq},
}

@misc{opencv_team_opencv_nodate,
	title = {{OpenCV}: {Hough} {Circle} {Transform}},
	url = {https://docs.opencv.org/3.4/d4/d70/tutorial_hough_circle.html},
	urldate = {2024-05-09},
	author = {OpenCV Team},
	file = {OpenCV\: Hough Circle Transform:C\:\\Users\\Guest_\\Zotero\\storage\\QHF7V96F\\tutorial_hough_circle.html:text/html},
}

@misc{opencv_team_opencv_nodate-1,
	title = {{OpenCV}: {Structural} {Analysis} and {Shape} {Descriptors}},
	url = {https://docs.opencv.org/4.x/d3/dc0/group__imgproc__shape.html#ga0012a5fdaea70b8a9970165d98722b4c},
	urldate = {2024-05-09},
	author = {OpenCV Team},
	file = {OpenCV\: Structural Analysis and Shape Descriptors:C\:\\Users\\Guest_\\Zotero\\storage\\DCGN5NRG\\group__imgproc__shape.html:text/html},
}

@misc{opencv_team_opencv_nodate-2,
	title = {{OpenCV}: cv::{Moments} {Class} {Reference}},
	url = {https://docs.opencv.org/4.x/d8/d23/classcv_1_1Moments.html},
	urldate = {2024-05-09},
	author = {OpenCV Team},
	file = {OpenCV\: cv\:\:Moments Class Reference:C\:\\Users\\Guest_\\Zotero\\storage\\4DMGJUNY\\classcv_1_1Moments.html:text/html},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}, second edition: {An} {Introduction}},
	isbn = {978-0-262-35270-3},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	note = {Google-Books-ID: uWV0DwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Data Science / Neural Networks, Computers / Programming / Algorithms},
}

@book{szepesvari_algorithms_2022,
	title = {Algorithms for {Reinforcement} {Learning}},
	isbn = {978-3-031-01551-9},
	abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations. Table of Contents: Markov Decision Processes / Value Prediction Problems / Control / For Further Exploration},
	language = {en},
	publisher = {Springer Nature},
	author = {Szepesvári, Csaba},
	month = may,
	year = {2022},
	note = {Google-Books-ID: g4RyEAAAQBAJ},
	keywords = {Computers / Information Technology, Computers / Artificial Intelligence / General, Mathematics / Applied},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	language = {en},
	number = {3},
	urldate = {2024-05-16},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\55YPL2DA\\Watkins und Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{rummery_-line_1994,
	title = {On-{Line} {Q}-{Learning} {Using} {Connectionist} {Systems}},
	abstract = {Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete finite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of different algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Difference algorithm (Sutton 1988), including a new algorithm (Modified Connectionist Q-Learning), and Q() (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each t...},
	journal = {Technical Report CUED/F-INFENG/TR 166},
	author = {Rummery, G. and Niranjan, Mahesan},
	month = nov,
	year = {1994},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\FN8QY9M3\\Rummery und Niranjan - 1994 - On-Line Q-Learning Using Connectionist Systems.pdf:application/pdf},
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement {Learning}: {A} {Survey}},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Reinforcement {Learning}},
	url = {https://www.jair.org/index.php/jair/article/view/10166},
	doi = {10.1613/jair.301},
	abstract = {This paper surveys the field of reinforcement learning from    a computer-science perspective. It is written to be accessible to    researchers familiar with machine learning.  Both the historical basis    of the field and a broad selection of current work are summarized.    Reinforcement learning is the problem faced by an agent that learns    behavior through trial-and-error interactions with a dynamic    environment.  The work described here has a resemblance to work in    psychology, but differs considerably in the details and in the use of    the word ``reinforcement.''  The paper discusses central issues of    reinforcement learning, including trading off exploration and    exploitation, establishing the foundations of the field via Markov    decision theory, learning from delayed reinforcement, constructing    empirical models to accelerate learning, making use of generalization    and hierarchy, and coping with hidden state.  It concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning.},
	language = {en},
	urldate = {2024-05-16},
	journal = {Journal of Artificial Intelligence Research},
	author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
	month = may,
	year = {1996},
	pages = {237--285},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\W47TKRT4\\Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf:application/pdf},
}

@article{bellman_dynamic_1957,
	title = {Dynamic programming and statistical communication theory},
	volume = {43},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.43.8.749},
	doi = {10.1073/pnas.43.8.749},
	number = {8},
	urldate = {2024-05-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bellman, Richard and Kalaba, Robert},
	month = aug,
	year = {1957},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {749--751},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\3RQ3XYER\\Bellman und Kalaba - 1957 - Dynamic programming and statistical communication .pdf:application/pdf},
}

@article{auer_finite-time_2002,
	title = {Finite-time {Analysis} of the {Multiarmed} {Bandit} {Problem}},
	volume = {47},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1013689704352},
	doi = {10.1023/A:1013689704352},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
	language = {en},
	number = {2},
	urldate = {2024-05-16},
	journal = {Machine Learning},
	author = {Auer, Peter and Cesa-Bianchi, Nicolò and Fischer, Paul},
	month = may,
	year = {2002},
	keywords = {adaptive allocation rules, bandit problems, finite horizon regret},
	pages = {235--256},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\3BIUPXTS\\Auer et al. - 2002 - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:application/pdf},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2024-05-16},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science},
	pages = {529--533},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\2GNQUNSH\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@inproceedings{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v48/mniha16.html},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1928--1937},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\KWLKAKC5\\Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\ZJNKPFH7\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\QE7Z7RIZ\\1707.html:text/html},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2024-05-16},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
	file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\CGYX95XH\\Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@article{kober_reinforcement_2013,
	title = {Reinforcement learning in robotics: {A} survey},
	volume = {32},
	issn = {0278-3649},
	shorttitle = {Reinforcement learning in robotics},
	url = {https://doi.org/10.1177/0278364913495721},
	doi = {10.1177/0278364913495721},
	abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
	language = {en},
	number = {11},
	urldate = {2024-05-16},
	journal = {The International Journal of Robotics Research},
	author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
	month = sep,
	year = {2013},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {1238--1274},
	file = {SAGE PDF Full Text:C\:\\Users\\Guest_\\Zotero\\storage\\W6PNMJRY\\Kober et al. - 2013 - Reinforcement learning in robotics A survey.pdf:application/pdf},
}

@article{deng_deep_2017,
	title = {Deep {Direct} {Reinforcement} {Learning} for {Financial} {Signal} {Representation} and {Trading}},
	volume = {28},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/7407387},
	doi = {10.1109/TNNLS.2016.2522401},
	abstract = {Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.},
	number = {3},
	urldate = {2024-05-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Deng, Yue and Bao, Feng and Kong, Youyong and Ren, Zhiquan and Dai, Qionghai},
	month = mar,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Feature extraction, Artificial neural networks, Deep learning (DL), financial signal processing, Learning (artificial intelligence), neural network (NN) for finance, Optimization, reinforcement learning (RL), Robustness, Signal representation, Training},
	pages = {653--664},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\FF6TABE4\\Deng et al. - 2017 - Deep Direct Reinforcement Learning for Financial S.pdf:application/pdf},
}

@misc{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2005.01643},
	doi = {10.48550/arXiv.2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01643 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\N7T965N3\\Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\SZGM2P3W\\2005.html:text/html},
}

@inproceedings{dai_sbeed_2018,
	title = {{SBEED}: {Convergent} {Reinforcement} {Learning} with {Nonlinear} {Function} {Approximation}},
	shorttitle = {{SBEED}},
	url = {https://proceedings.mlr.press/v80/dai18c.html},
	abstract = {When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov’s smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm’s sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1125--1134},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\GSN2L5IT\\Dai et al. - 2018 - SBEED Convergent Reinforcement Learning with Nonl.pdf:application/pdf},
}

@inproceedings{cobbe_quantifying_2019,
	title = {Quantifying {Generalization} in {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v97/cobbe19a.html},
	abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent’s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1282--1289},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\DPZXEUWB\\Cobbe et al. - 2019 - Quantifying Generalization in Reinforcement Learni.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Guest_\\Zotero\\storage\\52RRBFND\\Cobbe et al. - 2019 - Quantifying Generalization in Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{bellemare_distributional_2017,
	title = {A {Distributional} {Perspective} on {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v70/bellemare17a.html},
	abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {449--458},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\JUHUNYZV\\Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Guest_\\Zotero\\storage\\LMCIER3Z\\Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:application/pdf},
}

@misc{pan_policy_2018,
	title = {Policy {Optimization} with {Model}-based {Explorations}},
	url = {http://arxiv.org/abs/1811.07350},
	doi = {10.48550/arXiv.1811.07350},
	abstract = {Model-free reinforcement learning methods such as the Proximal Policy Optimization algorithm (PPO) have successfully applied in complex decision-making problems such as Atari games. However, these methods suffer from high variances and high sample complexity. On the other hand, model-based reinforcement learning methods that learn the transition dynamics are more sample efficient, but they often suffer from the bias of the transition estimation. How to make use of both model-based and model-free learning is a central problem in reinforcement learning. In this paper, we present a new technique to address the trade-off between exploration and exploitation, which regards the difference between model-free and model-based estimations as a measure of exploration value. We apply this new technique to the PPO algorithm and arrive at a new policy optimization method, named Policy Optimization with Model-based Explorations (POME). POME uses two components to predict the actions' target values: a model-free one estimated by Monte-Carlo sampling and a model-based one which learns a transition model and predicts the value of the next state. POME adds the error of these two target estimations as the additional exploration value for each state-action pair, i.e, encourages the algorithm to explore the states with larger target errors which are hard to estimate. We compare POME with PPO on Atari 2600 games, and it shows that POME outperforms PPO on 33 games out of 49 games.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Pan, Feiyang and Cai, Qingpeng and Zeng, An-Xiang and Pan, Chun-Xiang and Da, Qing and He, Hualin and He, Qing and Tang, Pingzhong},
	month = nov,
	year = {2018},
	note = {arXiv:1811.07350 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\E547VX42\\Pan et al. - 2018 - Policy Optimization with Model-based Explorations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\EYEXKNRA\\1811.html:text/html},
}

@misc{wouter_van_heeswijk_proximal_2023,
	title = {Proximal {Policy} {Optimization} ({PPO}) {Explained}},
	url = {https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b},
	abstract = {The journey from REINFORCE to the go-to algorithm in continuous control},
	language = {en},
	urldate = {2024-05-21},
	journal = {Medium},
	author = {Wouter van Heeswijk},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\2UKQZKMW\\proximal-policy-optimization-ppo-explained-abed1952457b.html:text/html},
}

@misc{openai_proximal_2017,
	title = {Proximal {Policy} {Optimization}},
	url = {https://openai.com/index/openai-baselines-ppo/},
	abstract = {We’re releasing a new class of reinforcement learning algorithms, Proximal Policy Optimization (PPO), which perform comparably or better than state-of-the-art approaches while being much simpler to implement and tune. PPO has become the default reinforcement learning algorithm at OpenAI because of its ease of use and good performance.},
	language = {en-US},
	urldate = {2024-05-21},
	author = {OpenAI},
	month = jul,
	year = {2017},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\XGR5IUPE\\openai-baselines-ppo.html:text/html},
}

@article{astrom_swinging_2000,
	title = {Swinging up a pendulum by energy control},
	volume = {36},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109899001405},
	doi = {10.1016/S0005-1098(99)00140-5},
	abstract = {Properties of simple strategies for swinging up an inverted pendulum are discussed. It is shown that the behavior critically depends on the ratio of the maximum acceleration of the pivot to the acceleration of gravity. A comparison of energy-based strategies with minimum time strategy gives interesting insights into the robustness of minimum time solutions.},
	number = {2},
	urldate = {2024-05-21},
	journal = {Automatica},
	author = {Åström, K. J. and Furuta, K.},
	month = feb,
	year = {2000},
	keywords = {Energy control, Inverted pendulum, Minimum time control, Swing-up},
	pages = {287--295},
	file = {Eingereichte Version:C\:\\Users\\Guest_\\Zotero\\storage\\CLVNJB6Z\\Åström und Furuta - 2000 - Swinging up a pendulum by energy control.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\KXL9A9BJ\\S0005109899001405.html:text/html},
}

@misc{kumar_balancing_2020,
	title = {Balancing a {CartPole} {System} with {Reinforcement} {Learning} -- {A} {Tutorial}},
	url = {http://arxiv.org/abs/2006.04938},
	doi = {10.48550/arXiv.2006.04938},
	abstract = {In this paper, we provide the details of implementing various reinforcement learning (RL) algorithms for controlling a Cart-Pole system. In particular, we describe various RL concepts such as Q-learning, Deep Q Networks (DQN), Double DQN, Dueling networks, (prioritized) experience replay and show their effect on the learning performance. In the process, the readers will be introduced to OpenAI/Gym and Keras utilities used for implementing the above concepts. It is observed that DQN with PER provides best performance among all other architectures being able to solve the problem within 150 episodes.},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Kumar, Swagat},
	month = jun,
	year = {2020},
	note = {arXiv:2006.04938 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guest_\\Zotero\\storage\\GJVIBK93\\Kumar - 2020 - Balancing a CartPole System with Reinforcement Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\Y54A2BVD\\2006.html:text/html},
}

@inproceedings{nagendra_comparison_2017,
	title = {Comparison of reinforcement learning algorithms applied to the cart-pole problem},
	url = {https://ieeexplore.ieee.org/abstract/document/8125811},
	doi = {10.1109/ICACCI.2017.8125811},
	abstract = {Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide an optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cart-pole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy-gradient actorcritic, and value-function approximation are compared in this context with the standard linear quadratic regulator solution. Further, we propose a novel approach for integrating RL and swing-up controllers.},
	urldate = {2024-05-21},
	booktitle = {2017 {International} {Conference} on {Advances} in {Computing}, {Communications} and {Informatics} ({ICACCI})},
	author = {Nagendra, Savinay and Podila, Nikhil and Ugarakhod, Rashmi and George, Koshy},
	month = sep,
	year = {2017},
	keywords = {Learning (artificial intelligence), Approximation algorithms, Benchmark testing, Heuristic algorithms, Intelligent systems, learning systems, nonlinear control systems, Optimal control, Regulators, Space exploration},
	pages = {26--32},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\LVVE35XG\\8125811.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\VTU9HS96\\Nagendra et al. - 2017 - Comparison of reinforcement learning algorithms ap.pdf:application/pdf},
}

@inproceedings{kumar_empirical_2024,
	title = {Empirical study of deep reinforcement learning algorithms for {CartPole} problem},
	url = {https://ieeexplore.ieee.org/abstract/document/10512107},
	doi = {10.1109/SPIN60856.2024.10512107},
	abstract = {CartPole problem is a classical control problem, which is important in many fields, including robotics and self-driving cars. Deep reinforcement learning is applied for controlling the CartPole balancing problem. This study compares the performance of four deep reinforcement learning algorithms: Deep Q-Network (DQN), Proximal Policy Optimization (PPO), QR (Quantile Regression) DQN, and Advantage Actor-Critic (A2C). The CartPole balance problem in the OpenAIGym environment is taken into consideration to execute the deep reinforcement learning algorithms. Many deep Reinforcement algorithms exist, and selecting the right one might be difficult depending on the problem’s characteristics. Our findings can help in the ranking of the four methods based on how well they fit the requirements of a given application. The performance in terms of training time, training timesteps, maximum value of episode-reward-mean, and average frames per second for each method are evaluated. The results demonstrate that the best approach for resolving this problem is to use PPO method with a set of prespecified hyperparameters.},
	urldate = {2024-05-21},
	booktitle = {2024 11th {International} {Conference} on {Signal} {Processing} and {Integrated} {Networks} ({SPIN})},
	author = {Kumar, Yogesh and Kumar, Pankaj},
	month = mar,
	year = {2024},
	note = {ISSN: 2688-769X},
	keywords = {Artificial neural networks, Training, Advantage Actor-Critic, Aerospace electronics, Autonomous automobiles, CartPole, Classical Control Problem, Deep Q-Network, Deep reinforcement learning, Proximal Policy Optimization, QR (Quantile Regression) DQN, Signal processing, Signal processing algorithms, Stable Baselines3},
	pages = {78--83},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\43DW5JJW\\10512107.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\TTGMCC47\\Kumar und Kumar - 2024 - Empirical study of deep reinforcement learning alg.pdf:application/pdf},
}

@article{florian_correct_2007,
	title = {Correct equations for the dynamics of the cart-pole system},
	abstract = {The problem of balancing a pole on a moving cart is a widely used benchmark problem for testing reinforcement learning algorithms. The classic papers that introduced this problem contain mistakes in the equations that govern the dynamics of the cart-pole system, and these mistakes propagated in other studies that used the same problem as a benchmark. Here we provide the equations that describe correctly the dynamics of the system.},
	language = {en},
	author = {Florian, Razvan V},
	month = feb,
	year = {2007},
	file = {Florian - Correct equations for the dynamics of the cart-pol.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\GYQS475Q\\Florian - Correct equations for the dynamics of the cart-pol.pdf:application/pdf},
}

@phdthesis{nayante_reinforcement_2021,
	address = {Gummersbach},
	type = {Bachelor {Thesis}},
	title = {Reinforcement {Learning} am realen und simulierten {Cart}-{Pole}-{Swing}-{Up} {Pendulum} im {Vergleich}},
	url = {https://www.gm.fh-koeln.de/~konen/research/PaperPDF/BA_Nayante2021_final.pdf},
	language = {de},
	urldate = {2024-05-21},
	school = {TH Köln},
	author = {Nayante, Yendoukon},
	month = sep,
	year = {2021},
	file = {BA_Nayante2021_final.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\344ZKJLT\\BA_Nayante2021_final.pdf:application/pdf},
}

@inproceedings{rasmussen_probabilistic_2008,
	address = {Berlin, Heidelberg},
	title = {Probabilistic {Inference} for {Fast} {Learning} in {Control}},
	isbn = {978-3-540-89722-4},
	doi = {10.1007/978-3-540-89722-4_18},
	abstract = {We provide a novel framework for very fast model-based reinforcement learning in continuous state and action spaces. The framework requires probabilistic models that explicitly characterize their levels of confidence. Within this framework, we use flexible, non-parametric models to describe the world based on previously collected experience. We demonstrate learning on the cart-pole problem in a setting where we provide very limited prior knowledge about the task. Learning progresses rapidly, and a good policy is found after only a hand-full of iterations.},
	language = {en},
	booktitle = {Recent {Advances} in {Reinforcement} {Learning}},
	publisher = {Springer},
	author = {Rasmussen, Carl Edward and Deisenroth, Marc Peter},
	editor = {Girgin, Sertan and Loth, Manuel and Munos, Rémi and Preux, Philippe and Ryabko, Daniil},
	year = {2008},
	keywords = {Gaussian Process, Goal State, Neural Information Processing System, Reinforcement Learning, Successor State},
	pages = {229--242},
	file = {Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\U2ZDU4NP\\Rasmussen und Deisenroth - 2008 - Probabilistic Inference for Fast Learning in Contr.pdf:application/pdf},
}

@misc{fattnerd_how-_2022,
	title = {[{HOW}-{TO}] camera stuck waiting for image · {Issue} \#336 · raspberrypi/picamera2},
	url = {https://github.com/raspberrypi/picamera2/issues/336},
	abstract = {Please only ask one question per issue! Describe what it is that you want to accomplish I use the camera in a python function to capture QRCODES. first time the function is called, it works fine an...},
	language = {en},
	urldate = {2024-05-24},
	journal = {GitHub},
	author = {FattNerd},
	month = oct,
	year = {2022},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\6VFIBK2J\\336.html:text/html},
}

@article{rowley_neural_1998,
	title = {Neural network-based face detection},
	volume = {20},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/655647},
	doi = {10.1109/34.655647},
	abstract = {We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.},
	number = {1},
	urldate = {2024-05-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rowley, H.A. and Baluja, S. and Kanade, T.},
	month = jan,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Machine learning, Artificial neural networks, Computer vision, Detectors, Face detection, Filters, Machine learning algorithms, Neural networks, Pattern recognition, Pixel},
	pages = {23--38},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\2X8DT64R\\655647.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\TGMSPV6I\\Rowley et al. - 1998 - Neural network-based face detection.pdf:application/pdf},
}

@inproceedings{deng_new_2013,
	title = {New types of deep neural network learning for speech recognition and related applications: an overview},
	shorttitle = {New types of deep neural network learning for speech recognition and related applications},
	url = {https://ieeexplore.ieee.org/abstract/document/6639344},
	doi = {10.1109/ICASSP.2013.6639344},
	abstract = {In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications,” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.},
	urldate = {2024-05-28},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
	month = may,
	year = {2013},
	note = {ISSN: 2379-190X},
	keywords = {Optimization, Training, Neural networks, Acoustics, convolutional neural network, deep neural network, Hidden Markov models, multilingual, multitask, music processing, optimization, recurrent neural network, spectrogram features, Speech, speech recognition, Speech recognition},
	pages = {8599--8603},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guest_\\Zotero\\storage\\558EJNDX\\6639344.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Guest_\\Zotero\\storage\\F95Q3AXB\\Deng et al. - 2013 - New types of deep neural network learning for spee.pdf:application/pdf},
}

@article{zhang_deep_2015,
	title = {Deep {Neural} {Networks} in {Machine} {Translation}: {An} {Overview}},
	volume = {30},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1541-1672},
	shorttitle = {Deep {Neural} {Networks} in {Machine} {Translation}},
	url = {http://ieeexplore.ieee.org/document/7243232/},
	doi = {10.1109/MIS.2015.69},
	language = {en},
	number = {5},
	urldate = {2024-05-28},
	journal = {IEEE Intelligent Systems},
	author = {Zhang, Jiajun and Zong, Chengqing},
	month = sep,
	year = {2015},
	pages = {16--25},
	file = {Zhang und Zong - 2015 - Deep Neural Networks in Machine Translation An Ov.pdf:C\:\\Users\\Guest_\\Zotero\\storage\\2FZ49LCD\\Zhang und Zong - 2015 - Deep Neural Networks in Machine Translation An Ov.pdf:application/pdf},
}

@incollection{bengio_practical_2012,
	address = {Berlin, Heidelberg},
	title = {Practical {Recommendations} for {Gradient}-{Based} {Training} of {Deep} {Architectures}},
	isbn = {978-3-642-35289-8},
	url = {https://doi.org/10.1007/978-3-642-35289-8_26},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	language = {en},
	urldate = {2024-05-29},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
	publisher = {Springer},
	author = {Bengio, Yoshua},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_26},
	pages = {437--478},
	file = {Eingereichte Version:C\:\\Users\\Guest_\\Zotero\\storage\\YDN2A7RB\\Bengio - 2012 - Practical Recommendations for Gradient-Based Train.pdf:application/pdf},
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	shorttitle = {Machine learning},
	url = {https://www.science.org/doi/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
	number = {6245},
	urldate = {2024-05-29},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	month = jul,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {255--260},
}

@misc{google_deepmind_alphago_2020,
	title = {{AlphaGo}},
	url = {https://deepmind.google/technologies/alphago/},
	abstract = {Novel AI system mastered the ancient game of Go, defeated a Go world champion, and inspired a new era of AI.},
	language = {en},
	urldate = {2024-05-29},
	journal = {Google DeepMind},
	author = {Google DeepMind},
	month = dec,
	year = {2020},
	file = {Snapshot:C\:\\Users\\Guest_\\Zotero\\storage\\EFKE3IU6\\alphago.html:text/html},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2024-05-29},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science, Reward},
	pages = {354--359},
	file = {Eingereichte Version:C\:\\Users\\Guest_\\Zotero\\storage\\QZSR9ZYW\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@misc{nicholas_renotte_reinforcement_2021,
	title = {Reinforcement {Learning} in 3 {Hours} {\textbar} {Full} {Course} using {Python}},
	url = {https://www.youtube.com/watch?v=Mut_u40Sqz4},
	abstract = {Want to get started with Reinforcement Learning?

This is the course for you!

This course will take you through all of the fundamentals required to get started with reinforcement learning with Python, OpenAI Gym and Stable Baselines. You'll be able to build deep learning powered agents to solve a varying number of RL problems including CartPole, Breakout and CarRacing as well as learning how to build your very own environment!

In this video you'll learn: 
1. All the basics to get up and started with Reinforcement Learning
2. How to build custom environments using OpenAI Gym
3. About working on custom projects for Reinforcement Learning

Get the code for this tutorial: https://github.com/nicknochnack/Reinf...

Links Mentioned
Stable Baselines 3: https://stable-baselines3.readthedocs...
OpenAI Gym: https://gym.openai.com/
PyTorch: https://pytorch.org/
Atarimania ROMs: http://www.atarimania.com/roms/Roms.rar
Swig: http://www.swig.org/Doc1.3/Windows.html

Chapters
0:00 - Start
0:23 - Introduction
1:15 - Gameplan
4:24 - RL in a Nutshell
13:30 - 1. Setup Stable Baselines
21:45 - 2. Environments
30:10 - Loading OpenAI Gym Environments
40:00 - Understanding OpenAI Gym Environments
42:58 - 3. Training
51:32 - Train a Reinforcement Learning Model
1:00:00 - Saving and Reloading Environments
1:04:23 - 4. Testing and Evaluation
1:06:35 - Evaluating RL Models
1:09:34 - Testing the Agent
1:15:56 - Viewing Logs in Tensorboard
1:24:50 - Performance Tuning
1:26:31 - 5. Callbacks, Alternate Algorithms, Neural Networks
1:27:39 - Adding Training Callbacks
1:34:44 - Changing Policies
1:38:27 - Changing Algorithms
1:40:29 - 6. Projects
1:41:31 - Project 1 Atari
1:41:51 - Importing Dependencies
1:44:16 - Applying GPU Acceleration with PyTorch
1:45:11 - Testing Atari Environments
1:51:35 - Vectorizing Environments
1:56:48 - Save and Reload Atari Model
1:57:45 - Evaluate and Test Atari RL Model
2:02:16 - Updated Performance
2:06:34 - Project 2 Autonomous Driving
2:06:56 - Installing Dependencies
2:09:27 - Test CarRacing-v0 Environment
2:12:23 - Train Autonomous Driving Agent
2:17:16 - Save and Reload Self Driving model
2:18:20 - Updated Self Driving Performance
2:28:56 - Project 3 Custom Open AI Gym Environments
2:29:35 - Import Dependencies for Custom Environment
2:32:00 - Types of OpenAI Gym Spaces
2:38:47 - Building a Custom Open AI Environment
2:51:49 - Testing a Custom Environment
2:52:49 - Train a RL Model for a Custom Environment
2:56:22 - Save a Custom Environment Model
2:58:49 - 7. Wrap Up

Oh, and don't forget to connect with me!
LinkedIn: https://bit.ly/324Epgo
Facebook: https://bit.ly/3mB1sZD
GitHub: https://bit.ly/3mDJllD
Patreon: https://bit.ly/2OCn3UW
Join the Discussion on Discord: https://bit.ly/3dQiZsV

Happy coding!
Nick

P.s. Let me know how you go and drop a comment if you need a hand!},
	urldate = {2024-05-29},
	author = {{Nicholas Renotte}},
	month = jun,
	year = {2021},
}

@misc{sentdex_q_2019,
	title = {Q {Learning} {Intro}/{Table} - {Reinforcement} {Learning} p.1},
	url = {https://www.youtube.com/watch?v=yMk_XtIEzH8},
	abstract = {Welcome to a reinforcement learning tutorial. In this part, we're going to focus on Q-Learning.

Q-Learning is a model-free form of machine learning, in the sense that the AI "agent" does not need to know or have a model of the environment that it will be in. The same algorithm can be used across a variety of environments.

For a given environment, everything is broken down into "states" and "actions." The states are observations and samplings that we pull from the environment, and the actions are the choices the agent has made based on the observation. For the purposes of the rest of this tutorial, we'll use the context of our environment to exemplify how this works.

Text-based tutorial and sample code: https://pythonprogramming.net/q-learn...

Channel membership:    / @sentdex  
Discord:   / discord  
Support the content: https://pythonprogramming.net/support...
Twitter:   / sentdex  
Instagram:   / sentdex  
Facebook:   / pythonprogramming.net  
Twitch:   / sentdex  

\#reinforcementlearning \#machinelearning \#python},
	urldate = {2024-05-29},
	author = {{sentdex}},
	month = may,
	year = {2019},
}

@misc{arxiv_insights_introduction_2018,
	title = {An introduction to {Policy} {Gradient} methods - {Deep} {Reinforcement} {Learning}},
	url = {https://www.youtube.com/watch?v=5P7I-xPq8u8},
	abstract = {In this episode I introduce Policy Gradient methods for Deep Reinforcement Learning.

After a general overview, I dive into Proximal Policy Optimization: an algorithm designed at OpenAI that tries to find a balance between sample efficiency and code complexity. PPO is the algorithm used to train the OpenAI Five system and is also used in a wide range of other challenges like Atari and robotic control tasks.

If you want to support this channel, here is my patreon link:
  / arxivinsights   --- You are amazing!! ;)

If you have questions you would like to discuss with me personally, you can book a 1-on-1 video call through Pensight: https://pensight.com/x/xander-steenbr... 

Links mentioned in the video:
⦁ PPO paper:   https://arxiv.org/abs/1707.06347
⦁ TRPO paper: https://arxiv.org/abs/1502.05477
⦁ OpenAI PPO blogpost: https://blog.openai.com/openai-baseli...
⦁ Aurelien Geron: KL divergence and entropy in ML:    • A Short Introduction to Entropy, Cros...  
⦁ Deep RL Bootcamp - Lecture 5:    • Deep RL Bootcamp  Lecture 5: Natural ...  
⦁ RL-adventure PyTorch implementation: https://github.com/higgsfield/RL-Adve...
⦁ OpenAI Baselines TensorFlow implementation: https://github.com/openai/baselines},
	urldate = {2024-05-29},
	author = {{Arxiv Insights}},
	month = oct,
	year = {2018},
}

@misc{gordon77_pi_2023,
	type = {Forum},
	title = {Pi camera v3 manual focus and libcamera-apps ? - {Raspberry} {Pi} {Forums}},
	url = {https://forums.raspberrypi.com/viewtopic.php?t=353887},
	urldate = {2024-06-13},
	author = {gordon77},
	month = jul,
	year = {2023},
	file = {Pi camera v3 manual focus and libcamera-apps ? - Raspberry Pi Forums:C\:\\Users\\Guest_\\Zotero\\storage\\LPQXKNUN\\viewtopic.html:text/html},
}
