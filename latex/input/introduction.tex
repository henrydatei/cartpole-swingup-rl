\chapter{Introduction}

- Artificial neural networks sind in den letzten Jahren immer wichtiger geworden, viele populäre Anwendungen wie Spracherkennung (Deng et al. 2013), Bilderkennung (Rowley et al. 1998) und maschinelles Übersetzen (Zhang & Zong, 2015) basieren auf ihnen %todo: quellen
- Artifical neural networks sind ein Teilgebiet des maschinellen Lernens, ein anderes Teilgebiet, welches weit weniger populär ist, ist das Reinforcement Learning %todo: quelle
- dabei sind auch im reinforcement learning beeindruckende Fortschritte erzielt worden, wie z.B. AlphaGo, das 2016 den Weltmeister im Go besiegte %todo: quelle
- der einstieg in reinforcement learning ist dank zahlreicher Videos und Tutorials im Internet relativ einfach, zudem gibt es viele Bibliotheken (wie z.B. OpenAI Gym), die die Implementierung von reinforcement learning Algorithmen erleichtern %todo: quellen
- Das Buch "Reinforcement Learning: An Introduction" von Sutton und Barto ist ein Standardwerk in diesem Bereich und bietet eine umfassende Einführung in die Thematik und ist zudem unter creative commons lizensiert, sodass es kostenlos verfügbar ist (Sutton & Barto, 2018)

- Eines von den standard Beispielen in der Literatur ist das Cart-Pole-Problem, bei dem ein Wagen einen Stab balancieren muss, der auf ihm befestigt ist. dieses problem wird auch in vielen Videos und Tutorials im Internet behandelt
- dieses problem ist auch in OpenAI Gym implementiert, sodass es einfach ist, es zu verwenden und zu experimentieren und so in wenigen Zeilen Code ein reinforcement learning agent zu trainieren, der das Cart-Pole-Problem löst. Die Verwendung der Bibliothek Stable Baselines 3 macht es noch einfacher, da sie viele vorgefertigte reinforcement learning Algorithmen enthält, die einfach zu verwenden sind, so unter anderem auch Proximal Policy Optimization (PPO), das in vielen reinforcement learning Anwendungen erfolgreich eingesetzt wird %todo: quellen
- das von OpenAI implementierte Environment ist aber nur simuliert, sodass der Ausgangszustand, das Pendel in der oberen Position, leicht wieder herzustellen ist. Wenn man das Cart-Pole-Problem in der Realität lösen möchte, so muss man manuell das Pendel nach jedem erfolglosen Versuch wieder in die obere Position bringen, was das Training deutlich verlängert
- wenn man also den trainingsprozess beschleunigen möchte, so muss einen reinforcement learning agent entwickeln, der das Pendel von seiner unteren Position in die obere Position bringt, um so das Cart-Pole-Swing-Up-Problem oder inverted-pendulum-problem in der Realität zu lösen
- in der Literatur gibt es nur wenige Arbeiten, die das Problem in der Realität lösen, und die meisten davon verwenden einfache reward Funktionen und simulieren das Environment, sodass es keine realen Herausforderungen gibt, die das Problem in der Realität lösen
- diese Arbeit beschäftigt sich mit der Entwicklung eines reinforcement learning agents, der das Swing-Up-Cart-Pole-Problem in der Realität löst, und dabei ein realistisches Environment verwendet, das auf einem Raspberry Pi basiert und ein echtes Cart und Pendel verwendet. Es beschreibt die Herausforderungen, die bei der Entwicklung eines solchen Systems auftreten, und die Lösungen, die gefunden wurden, um diese Herausforderungen zu bewältigen, sowie zukünftige Forschungsansätze, um das System weiter zu verbessern