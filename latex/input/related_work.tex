\chapter{Background and Literature Review}

\section{Fundamentals of Reinforcement Learning}

Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. This approach is inspired by behaviorist psychology, where learning is achieved through interactions with the environment and is reinforced by rewards and punishments. RL has gained significant attention due to its successful applications in various domains such as robotics, game playing, and autonomous systems (\cite{sutton_reinforcement_2018}).

The fundamental concepts of reinforcement learning are as follows:
\begin{itemize}
    \item Agent and Environment: In RL, the learning process involves an agent and an environment. The agent interacts with the environment by performing actions, and the environment responds by providing feedback in the form of rewards or penalties. The objective of the agent is to learn a policy that maximizes the total reward over time (\cite{sutton_reinforcement_2018}).
    \item State ($S$): Represents the current situation or configuration of the environment.
    \item Action ($A$): The set of all possible moves the agent can make.
    \item Reward ($R$): The feedback from the environment based on the action taken by the agent.
    \item Policy ($\pi$): A policy is a strategy used by the agent to decide the next action based on the current state.
    \item Value Function ($V$): The value function estimates the expected return (total reward) starting from a state and following a particular policy. It helps in evaluating the goodness of states (\cite{szepesvari_algorithms_2022}).
    \item Model-Free RL: The agent learns a policy or value function without understanding the underlying model of the environment. Examples include Q-Learning (\cite{watkins_q-learning_1992}) and PPO (\cite{pan_policy_2018}).
    \item Model-Based RL: The agent learns a model of the environment (transition probabilities and reward function) and uses it to plan actions (\cite{kaelbling_reinforcement_1996}).
\end{itemize}

The key algorithms in reinforcement learning include:
\begin{itemize}
    \item Dynamic Programming: Dynamic programming (DP) methods require a complete model of the environment. They are used to compute optimal policies by iteratively improving value functions. Examples include Policy Iteration and Value Iteration.
    \item Monte Carlo (MC) methods learn directly from episodes of experience. They do not require knowledge of the environment's model and estimate value functions based on sample returns.
    \item Temporal-Difference (TD) learning combines ideas from DP and MC methods. It updates value estimates based on the difference between consecutive estimates (\cite{sutton_reinforcement_2018}).
\end{itemize}

One of the central challenges in RL is the trade-off between exploration (trying new actions to discover their effects) and exploitation (choosing actions that are known to yield high rewards). Various strategies such as $\varepsilon$-greedy, softmax, and Upper Confidence Bound (UCB) are employed to balance this trade-off (\cite{auer_finite-time_2002}).

With the advent of deep learning, RL has evolved into deep reinforcement learning (DRL), where neural networks are used to approximate value functions or policies. Notable advancements include:
\begin{itemize}
    \item Deep Q-Network (DQN): Combines Q-Learning with deep neural networks to handle high-dimensional state spaces, such as those in Atari games (\cite{mnih_human-level_2015}).
    \item Actor-Critic Methods: Use two neural networks, one for the policy (actor) and one for the value function (critic). Examples include A3C (Asynchronous Advantage Actor-Critic) and PPO (Proximal Policy Optimization) (\cite{mnih_asynchronous_2016,schulman_proximal_2017}).
\end{itemize}

The state-of-the-art algorithm is Proximal Policy Optimization (PPO) which is a model-free, on-policy algorithm that optimizes the policy by maximizing the expected return. It uses a clipped surrogate objective to prevent large policy updates and ensure stable learning. PPO has been successfully applied to a wide range of tasks, including robotic manipulation, locomotion, and game playing. It combines the benefits of policy gradient methods with the stability of trust region methods and is implemented in the OpenAI Baselines library (\cite{schulman_proximal_2017,openai_proximal_2017,wouter_van_heeswijk_proximal_2023}).

\section{The Cart-Pole Problem and Swing-Up Dynamics}
The cart-pole problem, often referred to as the inverted pendulum, is a classic benchmark in control theory and reinforcement learning. It involves a pole attached to a cart that moves along a frictionless track. The goal is to balance the pole upright by applying forces to the cart. This problem has been extensively studied because it embodies the fundamental challenges of dynamic stability and control like nonlinearity and instability (). % TODO: cite kumari & Agarwal (2023)

The cart-pole system consists of a cart of mass $m_c$ and a pole of length $l$ and mass $m_p$. The cart can move horizontally, and the pole is free to swing in the vertical plane. The system's state can be described by four variables: the cart's position $x$, the cart's velocity $\dot{x}$, the pole's angle $\theta$ from the vertical, and the pole's angular velocity $\dot{\theta}$.

The equations of motion for the cart-pole system are derived from Newton's laws of motion and can be expressed as follows (assuming no friction):
\begin{align}
\ddot{\theta} &= \frac{g\sin\theta + \cos\theta\left(\frac{-F-m_p l \dot{\theta}^2 \sin\theta}{m_c+m_p}\right)}{l\left(\frac{4}{3}-\frac{m_p \cos^2 \theta}{m_c+m_p}\right)} \notag \\
\ddot{x} &= \frac{F+m_p l (\dot{\theta}^2 \sin\theta - \ddot{\theta} \cos\theta)}{m_c+m_p} \notag
\end{align}
where $F$ is the force applied to the cart, and $g$ is the acceleration due to gravity. These equations are coupled and nonlinear, posing a significant challenge for control (\cite{florian_correct_2007}).

While balancing the pole is a standard control problem, the swing-up task involves bringing the pole from a hanging downward position to an upright position and then stabilizing it. The swing-up problem is more complex because it requires controlling the system to pass through a large range of states.

One popular method for solving the swing-up problem is based on energy control. The idea is to apply forces to the cart to increase the total energy of the system to the desired level that allows the pole to be balanced upright. The total energy $E$ of the system is the sum of the kinetic and potential energies:
\begin{align}
E = \frac{1}{2} m_c \dot{x}^2 + \frac{1}{2} m_p (\dot{x}^2 + l^2 \dot{\theta}^2 + 2 l \dot{x} \dot{\theta} \cos(\theta)) + m_p g l \cos(\theta) \notag
\end{align}
The control strategy involves applying a force $F$ to the cart that drives the system's energy towards the desired energy level for the upright position. When the pole's energy is close to the required level, a switching strategy can be employed to transition from swing-up control to balance control (\cite{astrom_swinging_2000}).

\section{Solution Approaches for the Cart-Pole Problem}

- notable approaches with the following control algorithms (kumari & Agarwal (2023))
- PID controller
- reinforcement learning
- LQR controller
- fuzzy controller
- sliding mode controller
- LQG controller
- Lyapunov-based controller
- particle swarm optimized controller
- energy control
- Diese Ansätze werden gerne in Kombination angewendet, um die besten ergebnisse zu erzielen
- Weiterhin wird exemplarisch ein Paper vorgestellt, welches diesen Ansatz nutzt

PID controller
- PID = Proportional-Integral-Derivative, Steuerungsbefehl zum Zeitpunkt $t$ ist die Summe aus Proportional-, Integral- und Derivative-Komponenten
- alle 3 Komponenten reagieren auf Fehler $e_t$ (Differenz zwischen gewünschtem und aktuellem Zustand)
- P-Komponente: reagiert proportional zum Fehler
- I-Komponente: reagiert auf die Summe der bisherigen Fehler
- D-Komponente: reagiert auf die Änderungsrate des Fehlers
- Steuerungsbefehl $u_t = K_P\cdot e_t + K_I\cdot\sum_{i=0}^t e_i + K_D\cdot\frac{e_t - e_{t-1}}{\Delta t}$ für diskrete Zeitpunkt. Bei kontinuierlicher Zeit wird der Differenzialterm durch die Ableitung ersetzt und der Summenterm durch das Integral
- Werte für $K_P$, $K_I$ und $K_D$ werden durch manuelle Einstellung oder Optimierung bestimmt und als gain bezeichnet
- Chang & Lee (2007) nutzen 3 PID controller:  einen für das Swing-up, einen zum Stabilisieren des Winkels, wenn das Pendel bei $\pm$ 14 Grad ist, und einen zum Balacieren des Pendels, wenn es bei $\pm$ 3 Grad ist. Die zweite Phase erhöht die Robustheit des Systems und nutzt Feedback Linearization Control um das nichtlineare System zu linearisieren. Die Grundidee von Feedback Linearization Control ist ein nichtlineares System $\dot{x} = f(x) + g(x)\cdot u$ ($x$ Zustandsvektor, $u$ Steuerungsbefehl, $f$ und $g$ nichtlineare Funktionen, die die Dynamik des Systems beschreiben) durch eine Transformation des Steuerungsbefehls und Einführung eines neuen Steuerungsbefehls $v$ zu linearisieren: $u = \frac{1}{g(x)}(v - f(x))$. Dann wird das System zu $\dot{x}=v$, damit linear und kann mit linearen Reglern (PID oder LQR) kontrolliert werden. (J. Slotine, Weiping Li 1991, p. 208ff)
- Vergleichende Studien, wie z.B. von Prasad et al 2011 oder Varghese et al 2017, vergleichen den PID controller mit anderen Controllern, meistens PID und LQR zusammen, und zeigen, dass der PID controller zwar gut ist, aber andere Controller besser. Beide Studien nutzen MATLAB Simulink für ihre Simulationen mit einem Zeitschritt von 0.01 Sekunden und einem maximalen Simulationszeitraum von 10 Sekunden. Nach etwa 7 Sekunden erreicht das System das Gleichgewicht. Die state space equations mit disturbance input sind
\begin{align}
    \dot{x} &= \frac{\mathrm{d}}{\mathrm{d}t}\begin{pmatrix}
        \theta \\
        \dot{\theta} \\
        z \\
        \dot{z}
    \end{pmatrix} = \begin{pmatrix}
        \dot{\theta} \\
        f_1 \\
        \dot{z} \\
        f_2
    \end{pmatrix} \\
    y &= Cx \\
    f_1 &= \frac{u\cos(\theta) - (M+m)g\sin(\theta) + ml(\cos(\theta)\sin(\theta))\dot{\theta}^2 - \frac{M}{m}F_w\cos(\theta)}{ml\cos^2(\theta) - (M+m)l} \\
    f_2 &= \frac{u + ml(\sin(\theta))\dot{\theta}^2 - mg\cos(\theta)\sin(\theta) + F_w\sin^2(\theta)}{M+m-m\cos^2(\theta)}
\end{align}
mit $z$ Position des Carts, $\theta$ Winkel des Pendels, $M$ Masse des Carts, $m$ Masse des Pendels, $l$ Länge des Pendels, $g$ Erdbeschleunigung, $F_w$ Störung durch Wind, $u$ Steuerungsbefehl in Form als ausgeübte Kraft auf das Cart. Beide Studien verwenden 2 PID controller, deren Steuerungsbefehle sie addieren. Der erste PID controller versucht das Cart in die Mitte zu bekommen, der andere Controller das Pendel aufrecht zu balancieren. Der Wind wird durch ein White Noise Signal simuliert.

Reinforcement Learning
- Zur Theorie von RL siehe oben
- RL ist ein Ansatz, bei dem ein Agent durch Interaktion mit der Umgebung lernt, wie er Aktionen auswählen soll, um eine Belohnung zu maximieren. Durch die Interaktion mit der Umgebung sammelt der Agent Erfahrung und passt seine Strategie an, um die Belohnung zu maximieren. Damit weniger Aufwand für die Modellierung der Umgebung benötigt wird, wird Model-Free RL verwendet, bei dem der Agent direkt aus Erfahrungen lernt, ohne die Umgebung zu modellieren.
- There are few papers that use a real cart-pole system for the swing-up task (i.e. \cite{nayante_reinforcement_2021,pilcolearner_cart-pole_2011,deisenroth_pilco_2011}); many use simulated environments (often without the swing-up task) and similar reward functions (i.e. \cite{kumar_balancing_2020,liu_swing-up_2023,kimura_stochastic_1999}). Different approaches for reward functions are listed in the Experimental Setup section. Only one paper was found that uses a real cart-pole system for the swing-up task and was published recently, but it employs a simple reward function (\cite{nayante_reinforcement_2021}).

LQR controller
- Idea is from Kalman (1960)
- LQR = Linear Quadratic Regulator und ist für lineare Systeme mit quadratischen Kosten optimal
- lineares System: $\dot{x} = Ax + Bu$, $x$ Zustandsvektor, $u$ Steuerungsbefehl, $A$ und $B$ Systemmatrizen
- Kostenfunktion: $J = \int_0^\infty (x^T Q x + u^T R u) dt$, $Q$ und $R$ sind Gewichtungsmatrizen
- Ziel: Minimierung der Kostenfunktion durch Wahl von $K$ (Reglermatrix) in $u = -Kx$ und System bleibt stabil
- Finden von $K$ durch Lösung der Riccati-Gleichung $A^T P + PA - PBR^{-1}B^T P + Q = 0$ (finden von $P$) und dann einsetzen von $P$ um $K$ zu finden $K = R^{-1}B^T P$
- Lösung der Riccati-Gleichung garantiert, dass der Regler dass System stabilisert (solange System $(A,B)$ stabilisierbar ist) und Kostenfunktion minimiert
- Kumar & Jerome 2013 nutzen diesen Ansatz und nach einer Analyse des Systems erhalten sie folgende Systemmatrizen:
\begin{align}
    A &= \begin{pmatrix}
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 \\
        0 & \frac{(M_pl)^2}{q} & \frac{-B_{eq}(M_pl^2 + I)}{q} & \frac{M_plB_p}{q} \\
        0 & \frac{(M+M_p)M_pgl}{q} & \frac{M_plB_{eq}}{q} & \frac{(M+M_p)B_p}{q}
    \end{pmatrix} \\
    B &= \begin{pmatrix}
        0 \\
        0 \\
        \frac{M_pl^2+I}{q} \\
        \frac{M_pl}{q}
    \end{pmatrix} \\
    x &= \begin{pmatrix}
        z \\
        \theta \\
        \dot{z} \\
        \dot{\theta}
    \end{pmatrix}
\end{align}
mit $M_p$ und $l$ Masse und Länge des Pendels, $M$ Masse des Carts, $I$ Trägheitsmoment des Pendels, $B_{eq}$ Äquivalente Dämpfungskonstante, $B_p$ Dämpfungskonstante des Pendels, $g$ Erdbeschleunigung, $z$ Position des Carts, $\theta$ Winkel des Pendels, $q = (M+M_p)(I+M_pl^2) - (M_pl)^2$. Mit den folgenden Gewichtungsmatrizen
\begin{align}
    Q &= \begin{pmatrix}
        0.5 & 0 & 0 & 0 \\
        0 & 5.5 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0
    \end{pmatrix} \\
    R &= \begin{pmatrix}
        0.0003
    \end{pmatrix}
\end{align}
ist $K$
\begin{align}
    K = \begin{pmatrix}
        -44.72 & 200.8 & -49.77 & 27.38
    \end{pmatrix}
\end{align}
für das System, welches Kumar und Jerome in ihrem Labor aufgebaut haben. Nach etwa 7 Sekunden erreicht das System das Gleichgewicht.

Lyapunov-based controller
- Lyapunov-based control is a method for designing controllers that guarantee the stability of a system using Lyapunov functions. A Lyapunov function is a scalar function that measures the energy of the system and is used to analyze the system's stability. The Lyapunov function is chosen such that its derivative along the system's trajectories is negative definite, ensuring that the system converges to a stable equilibrium point. Lyapunov-based control is widely used in control theory to design controllers for nonlinear systems and has been applied to a variety of problems, including robotic manipulation, aerospace systems, and power systems (\cite{slotine_applied_1991}).
- Für verschiedene Phasen des Cart-Pole-Systems können unterschiedliche Lyapunov-Funktionen verwendet werden, um die Stabilität zu gewährleisten. Für das Swing-up-Problem kann eine Lyapunov-Funktion verwendet werden, die die Energie des Systems misst und sicherstellt, dass die Energie des Pendels auf das erforderliche Niveau gebracht wird. Für das Balancieren des Pendels kann eine andere Lyapunov-Funktion verwendet werden, die die Stabilität des aufrechten Gleichgewichts sicherstellt.
- Kai & Bito 2014 nutzen eine Kombination aus Lyapunov-Funktion und LQR controller um das Cartpole-Problem per Simulation zu lösen. Weiterhin nutzen sie discrete mechanics, da diese weniger numerische Fehler aufweisen, trotzdem einige Gesetze der Physik erhalten bleiben und sich größere sampling times realisieren lassen (Marsden & West 2003). Mit ihrer Methode konnten sie den Pole nach 3.2 Sekunden aufrecht Balacieren. Die genutzte Lyapunov-Funktion ist
\begin{align}
    V_k = w_1E_k^2 + w_2\left((\cos\theta_k-1)^2 + \sin^2\theta_k\right) + w_3(\theta_{k-1} - \theta_k)^2 + w_4z_k^2
\end{align}
wobei $w_1, w_2, w_3, w_4>0$ Gewichte sind, $\theta_k$ der Winkel des Poles zum Zeitpunkt $k$, $z_k$ die Position des Carts zum Zeitpunkt $k$ und $E_k$ eine discrete energy-like Funktion ist mit
\begin{align}
    E_k = \frac{1}{2}ml^2\left(\frac{\theta_k - \theta_{k-1}}{h}\right)^2 + mgl[\cos((1-\alpha)\theta_{k-1} + \alpha\theta_k) - 1]
\end{align}
wobei $m$ die Masse und $l$ die Länge des Pendels ist, $h$ ist das sampling interval. Einführung einer Fehler-Funktion $\Gamma(\theta_{k+1}, u_k)$, die beim Einsetzen des optimalen Steuerungsbefehls $u_k$ und des nächsten Zustands $\theta_{k+1}$ den Wert 0 hat:
\begin{align}
    \Gamma(\theta_{k+1}, u_k) = V_{k+1} - V_k + p(\theta_k - \theta_{k-1})^2 = 0
\end{align}
$p$ ist hier ein Gewicht, die Lyapunov-Funktion $V_k$ hängt von dem Zustand des Systems ab und daher auch indirekt von $u_k$. Mit Hilfe des Newton-Verfahrens und der update rule
\begin{align}
    u_{k}^{l+1} = u_k^l - \frac{\partial\Gamma}{\partial u_k}^{-1}\Gamma
\end{align}
wird iterativ der optimale Steuerungsbefehl $u_k$ gefunden. Für die Stabilisierung wird dann ein LQR controller benutzt, der bei einem Winkel von $\frac{\pi}{4}$ aktiviert wird.

PSO controller
- Finden der Parameter eines Reglers häufig durch ausprobieren oder Optimierungsalgorithmen
- Ein solcher Optimierungsalgorithmus ist Particle Swarm Optimization (PSO)
- Idee von Eberhard & Kennedy 1995
- Particle Swarm Optimization (PSO), introduced by James Kennedy and Russell Eberhart in 1995, is an optimization algorithm inspired by the social behavior of birds flocking or fish schooling. The basic concept of PSO is to simulate the social dynamics of a group of individuals, known as particles, that collectively search for the optimal solution to a problem. In this algorithm, each particle represents a potential solution. These particles move through the solution space and adjust their positions based on their own experiences as well as the experiences of neighboring particles (Kennedy & Eberhard 1997). Each particle has a position and a velocity, which are updated as the algorithm progresses, relying on three key influences: the particle's current position, its personal best position found so far, and the global best position discovered by the entire swarm.
- Initially, a swarm of particles is randomly positioned in the search space, and their velocities are also set randomly (Shi & Eberhard 1998). The fitness of each particle, which determines how close it is to the optimal solution, is evaluated based on an objective function. To improve their positions, particles update their velocities considering their current direction (inertia), the direction toward their personal best position (cognitive influence), and the direction toward the global best position (social influence). These velocity updates guide the particles' movement through the solution space, and over time, they gravitate toward better solutions. The particle's personal best is updated whenever it discovers a better position, and the global best is updated if any particle finds a new overall best position. This process repeats until the algorithm meets a stopping criterion, such as reaching a maximum number of iterations or finding a sufficiently good solution (Angeline 1998).
- Mathematically, the velocity update for each particle is influenced by its inertia, the difference between its personal best and current position, and the difference between the global best and current position. Random factors are introduced to ensure variability in the search, and certain parameters control how strongly the particles are pulled toward their personal best or the global best (Özcan & Mohan 1998).
- Mousa et al 2015 vergleicht den Ansatz PI controller mit LQR und PI controller mit PSO Optimierung und findet heraus, dass der PSO optimierte PI controller besser ist als der PI+LQR controller. In seiner Simulation ist das Pendel mit dem PI+LQR controller nach 6 Sekunden in Balance, bei dem mit PSO optimierten PI controller nach 4 Sekunden.

Fuzzy controller
- A fuzzy logic controller is a type of control system that uses fuzzy logic, rather than traditional binary logic, to handle uncertain, imprecise, or ambiguous data. Unlike conventional controllers, which rely on precise inputs and mathematical models, a fuzzy logic controller mimics human decision-making by interpreting inputs in a way that resembles natural language reasoning (Rehman 2017). This makes it particularly useful for complex systems where exact mathematical models are difficult to derive or where control needs to handle vagueness.
- The basic operation of a fuzzy logic controller involves three main stages: fuzzification, inference, and defuzzification. In the fuzzification stage, the crisp (precise) input values are converted into fuzzy sets. These sets represent linguistic variables like low, medium, or high using membership functions, which define the degree to which a particular input belongs to each category (Liu et al 2009).
- The next stage is the inference process, where the fuzzy sets are used in a set of if-then-rules to determine the control action. These rules are based on expert knowledge and are expressed in simple terms (Dahiya 2016). Multiple rules can be applied simultaneously, and the system combines them to determine the most appropriate output.
- In the final stage, defuzzification, the fuzzy results from the inference process are converted back into crisp values to produce a specific control output (Mahmoud 2018, pp 15ff).
- Nour et al 2007 nutzen einen Fuzzy Logic Controller um ein simuliertes Cart-Pole-System zu balancieren. Allerdings startet der Pole bereits in der oberen Position und wird nur durch Störungen aus dem Gleichgewicht gebracht. Durch Bewegen des Carts soll dann der Pole wieder in die obere Position gebracht werden. Der Fuzzy Logic Controller wird mit einem PID controller verglichen und zeigt bessere Ergebnisse. Insbesondere werden die Regelparameter für den PID controller einmal für ein bestimmtes Gewicht (1 kilogramm cart, 100 gramm pole) für das System optimiert und dann für ein anderes Gewicht übernommen. Der Fuzzy Logic Controller zeigt eine bessere Robustheit gegenüber Änderungen in den Systemparametern. Es gibt 5 fuzzy sets für den Winkel, 5 fuzzy sets für die Winkelgeschwindigkeit, 2 fuzzy sets für die Position und Geschwindigkeit des Carts und 13 fuzzy sets für die Kraft, die auf das Cart ausgeübt werden soll. Es werden 225 Regeln aufgestellt, die die Kraft bestimmen, anhand folgender Formel und den fuzzy sets, in die die Werte fallen:
\begin{align}
    F = \theta + (\dot{\theta}-1) + (-x+3) + (-\dot{x}+3)
\end{align}
In einem zweiten Schritt werden die Anzahl an Regeln reduziert, auf 16: für jeden der 4 Inputs gibt es jetzt nur 2 fuzzy sets (positiv oder negativ). Die Rechenzeit wurde dadurch reduziert und die Ergebnisse sind immer noch zufriedenstellend.

Sliding mode controller
- A sliding mode controller (SMC) is a type of nonlinear control method designed to drive the system's state to a predefined surface, called the sliding surface, and keep it on that surface for all future time. The control strategy is particularly useful for systems with uncertainties or disturbances, offering robust performance by switching the control law based on the system's state (Edwards & Spurgeon 1998).
- The working principle of an SMC is divided into two phases: the reaching phase and the sliding phase (Uswarman et al 2019). In the reaching phase, the controller drives the system's state to reach the sliding surface. Once the state reaches this surface, the sliding phase begins, where the system state slides along the surface to the desired equilibrium point. The sliding surface is chosen such that the system's behavior on the surface satisfies the control objective (Ma'arif et al 2022, Sliding Mode Control Design for Magnetic Levitation System).
- An essential feature of SMC is its robustness to disturbances and model uncertainties. This robustness arises because the control action switches in such a way that once the system enters the sliding mode, the dynamics of the system are governed by the sliding surface and are independent of certain types of disturbances or parameter variations. However, this switching behavior can cause a phenomenon called chattering, which is rapid switching of the control signal that can be undesirable in practical implementations. To mitigate chattering, various methods, such as introducing a boundary layer around the sliding surface, are employed to smooth the control actions (Chalanga et al 2019).
- Mathematically, the control law in SMC typically involves a discontinuous component that switches the control action based on the system's position relative to the sliding surface (Mahmoud & AlRamadhan 2021). The design of the sliding surface is crucial, as it determines the system's performance in the sliding phase.
- (Ma'arif et al 2022, Backstepping) erweiteren diesen Controller um Backstepping, ein Verfahren um ein komplexes System in Subsysteme zu unterteilen. Sie simulieren nicht den swing-up, sondern nur das Balancieren des Pendels, dafür nutzen sie das folgende nichtlineare Modell:
\begin{align}
    \dot{\theta} &= f(x,t) + g(x,t)u + d(x,t)
\end{align}
wobei $\theta$ der Winkel des Pendels ist und $x$ ein Vektor aus $\theta$ und $\dot{\theta}$. $u$ ist die Kraft, die auf das Cart ausgeübt wird und $d(x,t)$ ist eine Störung. $f(x,t)$ und $g(x,t)$ sind nichtlineare Funktionen, die die Dynamik des Systems beschreiben.
\begin{align}
    f(x,t) &= \frac{g\sin(\theta) - \frac{ml\dot{\theta}^2\cos(\theta)\sin(\theta)}{m_c+m}}{l\left(\frac{4}{3}-\frac{m\cos^2(\theta)}{m_c+m}\right)} \\
    g(x,t) &= \frac{\frac{\cos(\theta)}{m_c+m}}{l\left(\frac{4}{3}-\frac{m\cos^2(\theta)}{m_c+m}\right)}
\end{align}
wobei $m_c$ die Masse des Carts, $m$ die Masse des Pendels, $l$ die Länge des Pendels, $g$ die Erdbeschleunigung ist (Valluru & Singh 2017). (Ma'arif et al 2022, Backstepping) define an error function $e = \theta_d - \theta$ with $\theta_d$ the desired angle and design a sliding surface $s = \dot{e} + c e$ with $c$ a positive constant. Using the Lyapunov function $V=\frac{1}{2}s$ they conclude that $\dot{V}<0$ and therefore the system is stable. They then design a control law $u = g(x,t)^{-1}(-f(x,t) +\ddot{\theta_d} + c\dot{e} + \eta\text(sgn)(s))$ to stabilize the system where $\text{sgn}(\cdot)$ is the signum function and $\eta$ a variable to elimate small oscillations in the response. The authors set the value of $\eta$ to 0.03 and later to 0.05 and archive a stabilisation in less than 1 second after the system is disturbed. This also works then the parameters (like mass or length) of the system are changed.
