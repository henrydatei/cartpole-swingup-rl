\chapter{Conclusion and Future Work}

\section{Summary of Findings}
Although it was not possible to balance the pendulum after it swung up, several improvements were made to the system that will enable better training of the RL agent in the future. The modular design of the system allows for the replacement and enhancement of individual components, such as angle detection, the RL agent, or motor control via Arduino. This modularity also reduces complexity in both setup and maintenance. The motor now has sufficient power to move the pendulum upwards without overheating, and the motor control is precise. The processing time for angle detection runs asynchronously to all other components, allowing multiple cores of the Raspberry Pi to be utilized. Consequently, the RL agent does not have to wait for an image and angle, reducing the delay between the latest observation and reality.

Unfortunately, none of the reward functions proposed in the literature could solve the task, including the one proposed in this work. Specifically, with the reward function from Kimura and Kobayashi (\citeyear{kimura_stochastic_1999}), the pendulum could not swing up, likely due to the high penalty for angular velocity. High angular velocity, however, is necessary during the swinging phase to move the pendulum upwards.

\section{Directions for Future Research}
Is angle detection via camera a good idea? Other methods of angle measurement, such as gyroscopes or magnetometers, should be explored. These methods might achieve lower latency, as image processing is computationally intensive, especially when no graphics unit is present, such as in the Raspberry Pi.

Making the pole longer could also be beneficial. It is easier for a human to balance a longer pole, and our current pole is relatively short. The pendulum in Nayante (\citeyear{nayante_reinforcement_2021}) is 940 mm long, whereas our pendulum is only 350 mm long.

Another direction is to make the motor faster. In one YouTube video, the motor is much faster and seems to have more power, enabling the pendulum to be moved upwards. Speed could also be increased with the current motor through a gearbox, but it is crucial to ensure that the power remains sufficient. If not, increasing the current and enhancing cooling might be necessary. Reducing the required power could also be achieved by reconsidering the cart's attachment on the rods to reduce friction, possibly through the use of lubricants.

Training for a longer duration could also improve results. In this work, communication errors between the Arduino and Raspberry Pi interrupted the training. These errors, although rare, made it difficult to train for more than 1 million steps. Future work could address this issue by using more robust implementations or different communication methods.

Ma et al. (\citeyear{ma_eureka_2023}) have presented a method to find an effective reward function using LLM. This method could be applied in future research to develop a better reward function for this task.