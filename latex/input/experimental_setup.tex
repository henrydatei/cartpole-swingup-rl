\chapter{Experimental Setup}

In the search for a system that solves the Inverse Cartpole Problem, an iterative approach was taken. This means that an experiment was conducted, the results were analyzed, and the experiment was adjusted to improve the outcomes. The focus of the initial experiments was on improving angle determination using a camera. The focus of the later experiments shifted towards improving the remaining hardware, particularly the stepper motor. The final experiments concentrated on enhancing the software, specifically the reward function of the reinforcement learning agent.

\section{Determining the angle} \label{sec:determining_the_angle}
In experiments prior to mine, the angle of the pendulum was determined by detecting colored rectangles on the pendulum and on a piece of plastic attached to the pendulum such that it is on the opposite side of the rotation point of the pendulum. The colors used were yellow and violet, and by finding the center of the bounding box around the detected colors, the angle could then be determined by connecting the midpoint and the rotation point of the pendulum, which was also visible in the image but fixed by hardcoded pixel coordinates. Thus, the camera could not be moved, as any movement would result in incorrect angle determination.

Further experiments involved attaching symbols, specifically a circle and a triangle, to the pendulum and the piece of plastic, so that color detection was no longer necessary, and instead, the symbols could be recognized. The approach was to detect the different geometric shapes based on their contours and approximate them using the Douglas-Peucker algorithm, implemented in OpenCV's \texttt{approxPolyDP()} (\cite{opencv_team_opencv_nodate-1}). The triangle and circle were chosen due to their differing number of vertices and very different contours. Filtering was done based on the moments of a contour using OpenCV's \texttt{moments()} function, for instance, to detect only shapes of a certain size (\cite{opencv_team_opencv_nodate-2}).

For circle detection, another method was used: detecting circles using OpenCV's Hough Circle Transform, which first determines potential centers and then matches radii to recognize circles (\cite{opencv_team_opencv_nodate}).

Subsequent experiments aimed to improve the color detection method. The image was transformed from the RGB color space to the HSV color space, which is less sensitive to changes in lighting. Searching for complementary colors in the HSV color space yielded colors such as blue and yellow. Additionally, sunlight from outside was reduced by lowering the blinds, and constant lighting of the room was attempted using artificial light. Since the pendulum was already printed in blue plastic, only a yellow piece of tape was attached to the piece of plastic. The pendulum was not always fully visible, which could lead to incorrect angles if using the center of a bounding box. Instead, the moment \texttt{m00} from OpenCV's \texttt{moments()} function, which gives the coordinates of the centroid, was used. This method also prevents problems with angle determination if contours with uneven edges are detected, for example, when shadows are cast unfavorably; the centroid remains relatively in the same position (\cite{opencv_team_opencv_nodate-2}).

\section{Optimizing Hardware}
During experiments, it became clear that the setup, in the configuration used in previous experiments, was not optimal and was unable to lift the pendulum upwards. Various optimization possibilities were identified: making the pendulum lighter so that less force was needed to lift it, increasing the current of the stepper motor to generate more force, or increasing the speed at which the stepper motor moves the cart.

Many of these experiments lasted only a few minutes, as it was already apparent that they would not be successful in lifting the pendulum.

The pendulum was made lighter by removing the weight at the end of the pendulum, leaving only the blue rod and the two connecting pieces. This reduced the weight of the pendulum from 128 g to 79 g.

In its original configuration, the stepper motor was operated by the stepper motor driver with a current of 0.61 A (RMS, Root Mean Square), which means a peak current of 0.86 A. Various stepper motors were tested: 1x MOT-AN-S-060-005-042-L-A-AAAA from igus and 2x 17HS19-2004S1 from stepperonline at different currents. The installed motor MOT-AN-S-060-005-042-L-A-AAAA from igus can withstand a current of 1.8 A, the stepper motor driver can deliver a maximum RMS current of 2 A (peak 2.83 A), and the stepper motor 17HS19-2004S1 from stepperonline can withstand a current of 2 A.

Experiments were conducted with different speeds and accelerations of the stepper motor to achieve the best performance. The cart was always tested with maximum speed and random actions (left or right) to see if the pendulum could be lifted. Tested speeds ranged from 50,000 steps/second to 70,000 steps/second in increments of 5,000, and tested accelerations ranged from 1,000,000 steps/second$^2$ to 10,000,000 steps/second$^2$ in increments of 1,000,000.

\section{Optimizing Software}

The software consists of two parts: handling the determined angles of the pendulum and the reinforcement learning (RL) agent, particularly the reward function.

The information extracted from the camera images largely depended on the reward function, but fundamentally, the angle and angular velocity were always determined. Various methods for angle determination were described in Section \ref{sec:determining_the_angle}. 

For the angular velocity, the difference in angles from the last two frames and their time difference were used to calculate the angular velocity. Additionally, angular velocity was also determined using a FIFO queue containing the last 3 or 5 angles with timestamps, to smooth the angular velocity calculation over a longer period (\cite{nikhilaggarwal3_queue_2019}).

All relevant information (e.g., current time, last 1-5 angles, angular velocity) was then placed into a queue of length 1 via ZeroMQ, which the RL agent could query to obtain the latest observation. It can happen that the RL agent retrieves information from the ZeroMQ queue faster than new observations can be generated, resulting in the RL agent receiving a message with only zeros. To differentiate whether a message contains only zeros because the pole is upright and stationary or because the RL agent is too fast, a flag was introduced. This flag is set to 1 when the pole is upright. Thus, a message consisting only of zeros can never be valid. In such cases, the RL agent stores the last message and uses it as the current observation.

The observation space includes the last 1-5 angles, angular velocity, the current position and velocity of the cart, and whether the pendulum is upright or not. The pendulum is considered upright if the absolute angle is less than 12 degrees, as suggested by Nagendra (\citeyear{nagendra_comparison_2017}). Otherwise, the pendulum is considered down.

Various action spaces were tested with the simple reward function. Inspired by the Cartpole environment of OpenAI Gym, an action space of 2 actions was tested, moving the cart either left or right. This was extended to more actions, allowing the cart to move in 10-degree increments from fully left to fully right (a total of 648 possible actions).

Actions represent velocities that move the cart in steps per second, with the velocity and sign chosen in increments of 1000 from 0 to 60,000 steps/second. This was later reduced to 2000 and 5000 steps increments to decrease the number of actions, as a larger action space increases training time.

The action space was further reduced to 2 actions: velocities of -60,000 and 60,000 steps/second. However, while the pole is down, each action is executed for at least 0.1 seconds to build up angular velocity. This 0.1-second delay was implemented using \texttt{time.sleep(0.1)}. When the pole is upright, the delay is removed, allowing for many movements to balance the pole.

If the cart reaches the edge, it automatically stops, and only actions that move the cart back towards the center are executed. In preliminary tests, the RL agent received a penalty that increased exponentially the further the cart moved from the center. This led to the cart moving slightly in the center over time to minimize the penalty, which prevented the pole from being swung up due to insufficient momentum. Therefore, the penalty was removed.

Various reward functions were tested to train the RL agent to lift the pendulum. The length of an episode is 2048 steps, which is also the default value of the PPO implementation in Stable Baselines 3.

A simple reward function, where the reward $r_{simple}=\cos(\theta)$, was used, where $\theta$ is the angle of the pendulum. By definition, an angle of 0 corresponds to the upright position, yielding a reward of 1, while angles of $\pm 180$ degrees (the downward position) result in a reward of -1. This reward function is mentioned by Doya (\citeyear{doya_reinforcement_2000}) and by Wawrzynski \& Pacut (\citeyear{wawrzynski_model-free_2004}). If the cart reached the end of its travel range, any action that did not move the cart back towards the center was ignored.

H. Kimura \& S. Kobayashi (\citeyear{kimura_stochastic_1999}) proposed the following reward function:
\begin{align}
    r_{Kimura,Kobayashi} = \begin{cases}
        -1 & \vert\theta\vert \ge 0.8\pi \\
        -3 & \vert\dot{\theta}\vert \ge 10 \\
        1 & \vert\theta\vert < 0.133\pi \land \vert\dot{\theta}\vert < 2 \\ 
        0 & \text{else}
    \end{cases}\notag
\end{align} 
Here, $\theta$ is the angle in radians and $\dot{\theta}$is the angular velocity in radians per second.

Escobar et al. (\citeyear{manrique_escobar_parametric_2020}) proposed the following reward function, which was slightly modified to fit the setup. In the original paper, the applied force is considered to encourage the RL agent to achieve the goal with minimal force. A second part of the reward function provides a negative reward if the cart is outside the allowable range. This is not applicable in this setup, so this part of the reward function was omitted. A similar reward function can be found in Liu et al. (\citeyear{liu_swing-up_2023}).
\begin{align}
    r_{Escobar} = -0.01\left(0.01\cdot\vert x\vert^2 + 0.1\cdot\vert\theta\vert^2 + 5\cdot\vert \dot{x}\vert^2\right) \notag
\end{align} 
where $x$ is the position and $\dot{x}$ is the velocity of the cart.

Due to the high angular velocities necessary to lift the pole but detrimental during the balancing phase when the pole is upright, another reward function was tested that encourages high angular velocities only up to a certain point.
\begin{align}
    r_{highVelocity} = \begin{cases}
        \cos(\theta) & \vert\theta\vert \le 12^\circ \\
        \left|\frac{\dot{\theta}}{100}\right| & \vert\theta\vert > 12^\circ
    \end{cases} \notag
\end{align}
From previous experiments, a reward function emerged that uses the last 5 angles to determine an angle change. This reward function consists of several parts: rewarding the angle, penalizing high angular velocities, penalizing the RL agent if the cart moves too far from the center position, and penalizing the RL agent if the angle remains down with advanced training.
\begin{align}
    \text{angle reward} &= \exp\left(\frac{\cos(\theta_1) + \cos(\theta_2) + \cos(\theta_3) + \cos(\theta_4) + \cos(\theta_5)}{\alpha}\right) \notag \\
    \text{position penalty} &= \exp\left(\frac{\vert\text{current position}\vert}{\vert\text{max position}\vert}\right)-1 \notag \\
    \text{angular velocity penalty} &= \begin{cases}
        \text{angle reward}\cdot\frac{\Delta\theta_1 + \Delta\theta_2 + \Delta\theta_3 + \Delta\theta_4}{4\cdot 2\pi} & \vert\theta_5\vert \le 12^\circ \\
        0 & \text{else}
    \end{cases} \notag \\
    \text{no swing up penalty} &= \begin{cases}
        \frac{\text{time steps since training start}}{50000} & \vert\theta_5\vert \ge 168^\circ \\
        0 & \text{else}
    \end{cases} \notag \\
    r_{complex} &= \text{angle reward} - \beta\cdot \text{position penalty} \notag \\
    &- \gamma\cdot \text{angular velocity penalty} - \delta\cdot \text{no swing up penalty} \notag
\end{align}
where $\theta_5$ is the most recent angle and $\Delta\theta_i$ for $i=1,...,4$ is the absolute change from $\theta_i$ to $\theta_{i+1}$. $\alpha$ to $\delta$ allow for different weightings of the individual parts of the reward function. The current position can be obtained by sending a p-message to the Arduino, the maximum position by counting the number of revolutions of the stepper motor required to move the cart from the center to the edge. The counted revolutions multiplied by 12800 steps/revolution then give the maximum position.