\chapter{Experimental Setup}

- auf der Suche nach einem System, was das Inverse Cartpole Problem löst, wurde iterativ vorgegangen, das heißt es wurde ein Experiment durchgeführt, die Ergebnisse wurden analysiert und das Experiment wurde angepasst, um die Ergebnisse zu verbessern
- Fokus der ersten Experimente lag darauf, die Winkelbestimmung mittels Kamera zu verbessern
- Fokus der späteren Experimente lag darauf, die übrige Hardware, insbesondere den Stepper Motor, zu verbessern
- Fokus der letzten Experimente lag darauf, die Software zu verbessern, insbesondere die Reward-Funktion des Reinforcement Learning Agents

\section{Determining the angle} \label{sec:determining_the_angle}
- in Experimenten vor mir wurde der Winkel des Pendels durch die Erkennung von farbigen Rechtecken auf dem Pendel und auf dem Stück Plastik, welches so am Pendel befestigt ist, dass es auf der anderen Seite des Rotationspunktes des Pendels ist, bestimmt
- Die Farben waren Gelb und Violett und mithilfe der Mitte der Bounding Box um die erkannten Farben konnte dann der Winkel bestimmt werden, durch eine Verbindung des Mittelpunktes und des Rotationspunktes des Pendels, welcher auch im Bild erkennbar war, aber durch hartcodierte Pixelkoordinaten festgelegt war. Die Kamera durfte sich also nicht bewegen, da sonst wie Winkelbestimmung nicht mehr korrekt war.

- weitere Experimente: Anbringen von Symbolen, hier Kreis und Dreieck, auf dem Pendel und auf dem Stück Plastik, sodass die Farberkennung nicht mehr notwendig war, sondern die Symbole erkannt werden konnten.
- Ansatz: Erkennung der verschiedenen geometrischen Figuren anhand ihrer Konturen und Approximation mittels des Douglas-Peucker Algorithmus, Implementation in OpenCV's approxPolyDP(). Wahl von Dreieck und Kreis aufgrund ihrer unterschiedlichen Anzahl von Ecken und sehr unterschiedlichen Konturen.
- Filterung über Momente einer Kontur mittels OpenCV's moments() Funktion, z.B. um nur Formen einer bestimmten Größe zu erkennen
- Erkennung der Kreise auf anderem Weg: Erkennung des Kreises mittels OpenCV's Hough Circle Transform, die zuerst mögliche Mittelpunkte und dann passende Radien bestimmt, um Kreise zu erkennen. 

- nächste Experimente versuchen den Weg der Farberkennung zu verbessern
- Transformation des Bildes vom RGB-Farbraum in den HSV-Farbraum, der weniger empfindlich für Beleuchtungsänderungen ist
- Suchen von komplementären Farben im HSV Farbraum liefert zum Beispiel die Farben blau und gelb
- Zusätzlich wurde Sonneneinstrahlung von außen durch Herunterlassen des Sonnenschutzes versucht zu verringern und dauerhafte Beleuchtung des Zimmers mittels künstlichem Licht 
- Da das Pendel bereits in blauem Plastik gedruckt wurde, wird nur auf dem Stück Plastik ein gelbes Stück Klebeband angebracht
- das Pendel ist nicht immer komplett zu sehen, weswegen der Einsatz einer Bounding-Box und dann nehmen des Mittelpunktes zu falschen Winkeln führen würde
- stattdessen Bestimmen des Moments m00 von OpenCV's moments() Funktion, welcher die Koordinates des Schwerpunktes angibt. Dieses Vorgehen verhindert auch Probleme mit der Winkelbestimmung, falls Konturen nicht glatte Kanten erkannt werden, z.B. wenn Schatten ungünstig geworfen werden; der Schwerpunkt bleibt relativ an der selben Stelle

\section{Optimizing Hardware}
- Bei Experimenten wurde klar, dass das Setup in der Konfiguration, in der es sich bei vorgegangen Experimenten befand, nicht optimal war und nicht in der Lage war, das Pendel nach oben zu bekommen
- Verschiedene Möglichkeiten der Optimierung wurden identifiziert: das Pendel leichter machen, sodass weniger Kraft nötig ist, um es nach oben zu bekommen, den Strom des Stepper Motors erhöhen, um mehr Kraft zu erzeugen, oder die Geschwindigkeit mit der der Stepper Motor das Cart bewegt, erhöhen
- Viele dieser Experimente dauerten nur wenige Minuten, da bereits abzusehen war, dass sie nicht erfolgreich sein würden und das Pendel nicht nach oben gebracht werden würde

- Das Pendel wurde leichter gemacht, indem das Gewicht am Ende des Pendels entfernt wurde, sodass das Pendel nur noch aus dem blauen Stab und den 2 Verbindungsstücken besteht. Das Gewicht des Pendels wurde dadurch von 128 g auf 79 g reduziert

- In seiner urprünglichen Kofiguration wurde der Stepper Motor vom Stepper Motor Driver mit einem Strom von 0.61 A (RMS, Root Mean Square) betrieben, was einen Peakstrom von 0.86 A bedeutet
- Testen verschiedener Stepper Motoren: 1x MOT-AN-S-060-005-042-L-A-AAAA von igus und 2x 17HS19-2004S1 von stepperonline bei verschiedenen Strömen
- Der verbautete Motor MOT-AN-S-060-005-042-L-A-AAAA von igus verträgt eine Stromstärke von 1.8 A, der Stepper Motor Driver kann maximal einen RMS Strom von 2 A liefern (Peak 2.83 A), der Stepper Motor 17HS19-2004S1 von stepperonline verträgt eine Stromstärke von 2 A 

- Experimente mit verschiedenen Geschwindigkeiten und Beschleunigungen des Stepper Motors, um die beste Leistung zu erzielen. Das Cart wurde immer mit maximaler Geschwindigkeit und zufälligen Actions (links oder rechts) getestet, um zu schauen, ob das Pendel nach oben gebracht werden kann. Geteste Geschwindigkeiten 50000 Steps/Sekunde - 70000 Steps/Sekunde in 5000er Schritten, geteste Beschleunigungen 1000000 Steps/Sekunde^2 - 10000000 Steps/Sekunde^2 in 1000000er Schritten

\section{Optimizing Software}
- Zur Software gehören 2 Teile: zum einen der Umgang mit den bestimmten Winkeln des Pendels, zum anderen der RL Agent und dort insbesondere die Reward-Funktion

- welche Informationen aus den Kamerabildern genutzt werden, hing immer größtenteils von der Reward-Funktion ab, aber grundlegend wurde immer der Winkel und die Winkelgeschwidigkeit ermittelt.
- verschiedene Methoden zur Winkelbestimmung wurden in der Section \ref{sec:determining_the_angle} beschrieben, für die Winkelgeschwindigkeit wurde die Differenz der Winkel in den letzten 2 Frames genommen und deren Zeitdifferenz genutzt, um die Winkelgeschwindigkeit zu bestimmen
- es wurde auch versucht die Winkelgeschwidigkeit über eine FIFO-Queue zu bestimmen, die entweder die letzten 3 oder 5 Winkel mit Zeitstempel enthält, um so die Winkelgeschwidigkeit über einen größeren Zeitraum zu bestimmen und so zu Glätten
- alle relevanten Informationen (z.B. aktuelle Zeit, 1-5 letzten Winkel, Winkelgeschwidigkeit) wurden dann über ZeroMQ in eine Queue der Länge 1 gelegt, die der RL Agent abfragen konnte, um die aktuellste Beobachtung zu erhalten
- es kann passieren, dass der RL Agent schneller Infors aus der ZeroMQ-Queue abruft, als neue Beobachtungen erzeugt werden können und der RL Agent in diesem Fall eine Nachricht mit nur Nullen enthält. Um zu unterscheiden, ob die Nachricht nur Nullen enthält weil der Pole gerade oben ist und sich nicht bewegt oder weil der RL Agent zu schnell ist, wurde ein Flag eingeführt, welches 1 ist, wenn der Pole oben ist. Eine Nachricht, die nur aus Nullen besteht, kann dann nie gültig sein. In diesem Fall hat der RL Agent die letzte Nachricht gespeichert und nutzt diese als aktuellste Beobachtung.
- Observation space enthält die 1-5 Winkel, die Winkelgeschwindigkeit, die aktuelle Position und Geschwindigkeit des Carts und ob das Pendel oben oder unten ist. Das Pendel ist dann oben, wenn der Betrag des Winkels kleiner als 12 Grad ist, wie von Nagendra (2017) vorgeschlagen. Ansonsten ist das Pendel unten.

- verschiedene action spaces wurden mit der simplen Reward-Funktion getestet
- in Anlehnung an Cartpole Umgebung von OpenAI Gym wurde ein action space von 2 Aktionen getestet, die das Cart entweder nach links oder nach rechts bewegen
- Erweiterung auf mehr Aktionen, die das Cart in 10$^\circ$ Abstufungen mit einer Aktion von ganz links nach ganz rechts bewegen können (insgesamt 648 Aktionen möglich)
- Aktionen sind Geschwindigkeiten, die das Cart in Steps/Sekunde bewegen, wobei die Geschwindigkeit und Vorzeichen in 1000er Schritten von 0 bis 60000 Steps/Sekunde gewählt werden kann
- Reduktion auf 2000er Schritte und 5000er Schritte, um die Anzahl der Aktionen zu reduzieren, da die Anzahl der Aktionen die Trainingszeit erhöht
- Reduktion auf 2 Aktionen, Geschwindigkeit von -60000 und 60000, aber solange der Pole unten ist, wird jede Aktion für mindestens 0.1 Sekunden ausgeführt, um Winkelgeschwindigkeit aufzubauen. Die 0.1 Sekunden wurden per time.sleep(0.1) realisiert. Wenn der Pole oben ist, entfällt die Zeitverzögerung, der Pole kann durch viele Bewegungen balanciert werden.
- Wenn das Cart den Rand erreicht, so stoppt es automatisch und nur Aktionen, die das Cart wieder in die Mitte bringen, werden ausgeführt. In Vortests bekam der RL Agent eine Strafe, die exponentiell größer geworden ist, je weiter sich das Cart von der Mitte entfernt hat. Das hat dazu geführt, dass sich das Cart nach einiger Zeit nur leicht in der Mitte bewegt hat, um die Strafe so klein wie möglich zu halten. Hochgeschwungen ist der Pole dann nicht mehr, da das Cart nicht genug Moment aufbauen kann. Deswegen wurde die Strafe entfernt.

- verschiedene Reward-Funktionen wurden getestet, um den RL Agent zu trainieren, das Pendel nach oben zu bekommen
- die Länge einer Episode ist 2048 Steps, was auch der Standardwert der Implementierung von PPO in Stable Baselines 3 ist
- simple Reward-Funktion, in der der Reward $r_{simple}=\cos(\theta)$ ist, wobei $\theta$ der Winkel des Pendels ist. Durch die Defition der Winkel ist in der oberen Position der Winkel 0, welches einen Reward von 1 ergibt, in der unteren Position $\pm$ 180 Grad, was einen Reward von -1 ergibt. Diese Reward Funktion findet sich z.B. bei Doya (2000) oder bei Wawrzynski & Pacut (2004). Erreichte das Cart das Ende des Fahrbereiches, so wurde jede Action, die das Cart nicht wieder zurück in Richtung Mitte bringt, ignoriert.
- H. Kimura & S. Kobayashi (1999) haben folgende Reward-Funktion vorgeschlagen: \begin{align}
    r_{Kimura,Kobayashi} = \begin{cases}
        -1 & \vert\theta\vert \ge 0.8\pi \\
        -3 & \vert\dot{\theta}\vert \ge 10 \\
        1 & \vert\theta\vert < 0.133\pi \land \vert\dot{\theta}\vert < 2 \\ 
        0 & \text{sonst}
    \end{cases}
\end{align} hier mit $\theta$ als Winkel in Radiant und $\dot{\theta}$ als Winkelgeschwindigkeit in Radiant pro Sekunde.
- Escobar et al. (2020) haben folgende Reward-Funktion vorgeschlagen, die leicht modifiziert wurde, um sie an das Setup anzupassen. Im originalen Paper fließt die verwendete Kraft ein, um den RL Agent zu motivieren, mit möglichst wenig Kraft das Ziel zu erreichen. Ein zweiter Teil der Reward-Funktion liefert einen negativen Reward, wenn das Cart sich außerhalb des zulässigen Bereiches befindet. Das ist bei diesem Setup nicht möglich, weswegen dieser Teil der Reward-Funktion weggelassen wurde. A similar reward function can be found in Liu et al. (2023). \begin{align}
    r_{Escobar} = -0.01\left(0.01\cdot\vert x\vert^2 + 0.1\cdot\vert\theta\vert^2 + 5\cdot\vert \dot{x}\vert^2\right)
\end{align} wobei $x$ die Position und $\dot{x}$ die Geschwindigkeit des Carts ist. 
- Aufgrund von hohen Winkelgeschwindigkeiten, die zwar nötig sind, um den Pole nach oben zu bekommen, aber in der Balancierungsphase, wenn der Pole oben ist, hinderlich sind, wurde noch eine Reward-Funktionen getestet, die hohe Winkelgeschwindigkeiten nur bis einem bestimmten Punkt fördern. \begin{align}
    r_{highVelocity} = \begin{cases}
        \cos(\theta) & \vert\theta\vert \le 12^\circ \\
        \left|\frac{\dot{\theta}}{100}\right| & \vert\theta\vert > 12^\circ
    \end{cases}
\end{align}
- Aus vorgegangen Experimenten hatte sich auch eine Reward-Funktion ergeben, diese nutzt die letzten 5 Winkel und bestimmt eine Winkeländerung. Zugleich besteht diese Reward-Funktion aus mehreren Teilen, die den Winkel belohnen, zu hohe Winkelgeschwindigkeiten bestrafen, den RL Agent bestrafen, wenn sich das Cart zu weit aus der mittleren Position bewegt und den RL Agent bestraft, wenn der Winkel mit fortgeschrittenem Training unten bleibt. \begin{align}
    angle\_reward &= \exp\left(\frac{\cos(\theta_1) + \cos(\theta_2) + \cos(\theta_3) + \cos(\theta_4) + \cos(\theta_5)}{\alpha}\right) \\
    position\_penalty &= \exp\left(\frac{\vert\text{current position}\vert}{\vert\text{max position}\vert}\right)-1 \\
    angular\_velocity\_penalty &= \begin{cases}
        angle\_reward\cdot\frac{\Delta\theta_1 + \Delta\theta_2 + \Delta\theta_3 + \Delta\theta_4}{4\cdot 2\pi} & \vert\theta_5\vert \le 12^\circ \\
        0 & \text{sonst}
    \end{cases} \\
    no\_swing\_up\_penalty &= \begin{cases}
        \frac{\text{time steps since training start}}{50000} & \vert\theta_5\vert \ge 168^\circ \\
        0 & \text{sonst}
    \end{cases} \\
    r_{complex} &= angle\_reward - \beta\cdot position\_penalty - \gamma\cdot angular\_velocity\_penalty - \delta\cdot no\_swing\_up\_penalty
\end{align}
wobei $\theta_5$ der aktuellste Winkel ist und $\Delta\theta_i$ für $i=1,...,4$ die absolute Änderung von $\theta_i$ zu $\theta_{i+1}$ ist. $\alpha$ bis $\delta$ erlauben eine unterschiedliche Gewichtung der einzelnen Teile der Reward-Funktion.